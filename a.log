nohup: 忽略输入
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12:25:32 - INFO: Epoch 9 average loss: 0.167648
12:25:33 - INFO: Epoch 19 average loss: 0.170917
12:25:35 - INFO: Epoch 29 average loss: 0.107676
12:25:36 - INFO: Epoch 39 average loss: 0.111848
12:25:38 - INFO: Epoch 49 average loss: 0.150170
12:25:39 - INFO: Epoch 59 average loss: 0.095816
12:25:40 - INFO: Epoch 69 average loss: 0.124586
12:25:41 - INFO: Epoch 79 average loss: 0.085586
12:25:43 - INFO: Epoch 89 average loss: 0.076794
12:25:44 - INFO: Starting epoch 99:
/home/zhangxiaohong/yangjunzhe/temp_F/Code_FL
test: Lang-1-fs_ddpm
fault line:  [200468, 200471, 200467]
feature shape before [Data Augmentation]:  (88, 1731)
[2, 5, 1413, 7, 8, 9, 1415, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1427, 1429, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1508, 1510]
entropy 5.92943289215312 0.8946391829225355
req_shape:  16
statements selected: 16
statements selected: Index([100181, 100257, 100260, 100261, 100262, 200034, 200036, 200038, 200040,
       200042, 200044, 200046, 200048, 200050, 200052, 200054],
      dtype='int64')
Dataset shape: torch.Size([88, 1, 1, 16]) torch.Size([88])
Batch Dataloader shape: torch.Size([16, 1, 1, 16]) torch.Size([16])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.172]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0654]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0944]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0689]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.131]  83%|████████▎ | 5/6 [00:00<00:00, 49.36it/s, MSE=0.131] 83%|████████▎ | 5/6 [00:00<00:00, 49.36it/s, MSE=0.0863]100%|██████████| 6/6 [00:00<00:00, 47.58it/s, MSE=0.0863]
12:25:44 - INFO: Epoch 99 average loss: 0.102978
12:25:45 - INFO: Epoch 109 average loss: 0.106655
12:25:47 - INFO: Epoch 119 average loss: 0.088291
12:25:48 - INFO: Epoch 129 average loss: 0.089629
12:25:49 - INFO: Epoch 139 average loss: 0.071319
12:25:51 - INFO: Epoch 149 average loss: 0.062817
12:25:52 - INFO: Epoch 159 average loss: 0.070851
12:25:53 - INFO: Epoch 169 average loss: 0.045987
12:25:54 - INFO: Epoch 179 average loss: 0.054689
12:25:56 - INFO: Epoch 189 average loss: 0.072065
12:25:57 - INFO: Starting epoch 199:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0428]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0611]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.102]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0638]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0727] 83%|████████▎ | 5/6 [00:00<00:00, 48.74it/s, MSE=0.0727] 83%|████████▎ | 5/6 [00:00<00:00, 48.74it/s, MSE=0.0639]100%|██████████| 6/6 [00:00<00:00, 47.50it/s, MSE=0.0639]
12:25:57 - INFO: Epoch 199 average loss: 0.067752
12:25:58 - INFO: Epoch 209 average loss: 0.055231
12:26:00 - INFO: Epoch 219 average loss: 0.080892
12:26:01 - INFO: Epoch 229 average loss: 0.075482
12:26:02 - INFO: Epoch 239 average loss: 0.082540
12:26:03 - INFO: Epoch 249 average loss: 0.079159
12:26:05 - INFO: Epoch 259 average loss: 0.062512
12:26:06 - INFO: Epoch 269 average loss: 0.072994
12:26:07 - INFO: Epoch 279 average loss: 0.047775
12:26:08 - INFO: Epoch 289 average loss: 0.048178
12:26:09 - INFO: Starting epoch 299:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0434]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0326]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0342]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0621]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0582]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.085] 100%|██████████| 6/6 [00:00<00:00, 62.18it/s, MSE=0.085]
12:26:09 - INFO: Epoch 299 average loss: 0.052573
12:26:11 - INFO: Epoch 309 average loss: 0.070056
12:26:12 - INFO: Epoch 319 average loss: 0.073824
12:26:13 - INFO: Epoch 329 average loss: 0.069085
12:26:15 - INFO: Epoch 339 average loss: 0.065578
12:26:16 - INFO: Epoch 349 average loss: 0.079519
12:26:17 - INFO: Epoch 359 average loss: 0.052866
12:26:18 - INFO: Epoch 369 average loss: 0.066929
12:26:20 - INFO: Epoch 379 average loss: 0.065505
12:26:21 - INFO: Epoch 389 average loss: 0.048972
12:26:22 - INFO: Starting epoch 399:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0558]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0373]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0686]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0382]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0583] 83%|████████▎ | 5/6 [00:00<00:00, 45.22it/s, MSE=0.0583] 83%|████████▎ | 5/6 [00:00<00:00, 45.22it/s, MSE=0.0636]100%|██████████| 6/6 [00:00<00:00, 46.03it/s, MSE=0.0636]
12:26:22 - INFO: Epoch 399 average loss: 0.053628
12:26:24 - INFO: Epoch 409 average loss: 0.052510
12:26:25 - INFO: Epoch 419 average loss: 0.068509
12:26:26 - INFO: Epoch 429 average loss: 0.054784
12:26:28 - INFO: Epoch 439 average loss: 0.055736
12:26:29 - INFO: Epoch 449 average loss: 0.055539
12:26:30 - INFO: Epoch 459 average loss: 0.059743
12:26:32 - INFO: Epoch 469 average loss: 0.053701
12:26:33 - INFO: Epoch 479 average loss: 0.051067
12:26:34 - INFO: Epoch 489 average loss: 0.055198
12:26:35 - INFO: Starting epoch 499:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0779]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0512]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0462]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0574]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0526]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0717]100%|██████████| 6/6 [00:00<00:00, 53.10it/s, MSE=0.0717]100%|██████████| 6/6 [00:00<00:00, 52.92it/s, MSE=0.0717]
12:26:35 - INFO: Epoch 499 average loss: 0.059505
12:26:37 - INFO: Epoch 509 average loss: 0.068051
12:26:38 - INFO: Epoch 519 average loss: 0.058882
12:26:39 - INFO: Epoch 529 average loss: 0.101634
12:26:40 - INFO: Epoch 539 average loss: 0.058481
12:26:41 - INFO: Epoch 549 average loss: 0.066436
12:26:43 - INFO: Epoch 559 average loss: 0.057798
12:26:44 - INFO: Epoch 569 average loss: 0.081401
12:26:45 - INFO: Epoch 579 average loss: 0.057095
12:26:47 - INFO: Epoch 589 average loss: 0.052353
12:26:48 - INFO: Starting epoch 599:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0579]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.124]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0448]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0442] 67%|██████▋   | 4/6 [00:00<00:00, 38.95it/s, MSE=0.0442] 67%|██████▋   | 4/6 [00:00<00:00, 38.95it/s, MSE=0.0774] 67%|██████▋   | 4/6 [00:00<00:00, 38.95it/s, MSE=0.0401]100%|██████████| 6/6 [00:00<00:00, 42.20it/s, MSE=0.0401]
12:26:48 - INFO: Epoch 599 average loss: 0.064686
12:26:49 - INFO: Epoch 609 average loss: 0.053611
12:26:51 - INFO: Epoch 619 average loss: 0.057955
12:26:52 - INFO: Epoch 629 average loss: 0.057259
12:26:53 - INFO: Epoch 639 average loss: 0.050583
12:26:55 - INFO: Epoch 649 average loss: 0.054153
12:26:56 - INFO: Epoch 659 average loss: 0.063861
12:26:57 - INFO: Epoch 669 average loss: 0.054450
12:26:59 - INFO: Epoch 679 average loss: 0.060267
12:27:00 - INFO: Epoch 689 average loss: 0.052404
12:27:01 - INFO: Starting epoch 699:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0584]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0335]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0751]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0629]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0651] 83%|████████▎ | 5/6 [00:00<00:00, 42.71it/s, MSE=0.0651] 83%|████████▎ | 5/6 [00:00<00:00, 42.71it/s, MSE=0.0472]100%|██████████| 6/6 [00:00<00:00, 46.56it/s, MSE=0.0472]
12:27:01 - INFO: Epoch 699 average loss: 0.057047
12:27:03 - INFO: Epoch 709 average loss: 0.057061
12:27:04 - INFO: Epoch 719 average loss: 0.046825
12:27:05 - INFO: Epoch 729 average loss: 0.039241
12:27:07 - INFO: Epoch 739 average loss: 0.044382
12:27:08 - INFO: Epoch 749 average loss: 0.062376
12:27:09 - INFO: Epoch 759 average loss: 0.064589
12:27:11 - INFO: Epoch 769 average loss: 0.048942
12:27:12 - INFO: Epoch 779 average loss: 0.046705
12:27:13 - INFO: Epoch 789 average loss: 0.070031
12:27:14 - INFO: Starting epoch 799:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0472]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.106]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0776]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0417]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0483] 83%|████████▎ | 5/6 [00:00<00:00, 49.52it/s, MSE=0.0483] 83%|████████▎ | 5/6 [00:00<00:00, 49.52it/s, MSE=0.0298]100%|██████████| 6/6 [00:00<00:00, 52.58it/s, MSE=0.0298]
12:27:14 - INFO: Epoch 799 average loss: 0.058460
12:27:16 - INFO: Epoch 809 average loss: 0.055658
12:27:17 - INFO: Epoch 819 average loss: 0.061774
12:27:18 - INFO: Epoch 829 average loss: 0.048269
12:27:19 - INFO: Epoch 839 average loss: 0.058139
12:27:21 - INFO: Epoch 849 average loss: 0.050591
12:27:22 - INFO: Epoch 859 average loss: 0.046021
12:27:23 - INFO: Epoch 869 average loss: 0.060149
12:27:25 - INFO: Epoch 879 average loss: 0.054576
12:27:26 - INFO: Epoch 889 average loss: 0.048522
12:27:27 - INFO: Starting epoch 899:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0429]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0483]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0375]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0286]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0881] 83%|████████▎ | 5/6 [00:00<00:00, 49.33it/s, MSE=0.0881] 83%|████████▎ | 5/6 [00:00<00:00, 49.33it/s, MSE=0.0356]100%|██████████| 6/6 [00:00<00:00, 48.17it/s, MSE=0.0356]
12:27:27 - INFO: Epoch 899 average loss: 0.046853
12:27:28 - INFO: Epoch 909 average loss: 0.057434
12:27:30 - INFO: Epoch 919 average loss: 0.044412
12:27:31 - INFO: Epoch 929 average loss: 0.045609
12:27:33 - INFO: Epoch 939 average loss: 0.070596
12:27:34 - INFO: Epoch 949 average loss: 0.043728
12:27:35 - INFO: Epoch 959 average loss: 0.043717
12:27:36 - INFO: Epoch 969 average loss: 0.066860
12:27:37 - INFO: Epoch 979 average loss: 0.056413
12:27:39 - INFO: Epoch 989 average loss: 0.046830
12:27:40 - INFO: Starting epoch 999:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0461]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0405]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0564]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0298]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0617] 83%|████████▎ | 5/6 [00:00<00:00, 46.99it/s, MSE=0.0617] 83%|████████▎ | 5/6 [00:00<00:00, 46.99it/s, MSE=0.0403]100%|██████████| 6/6 [00:00<00:00, 43.18it/s, MSE=0.0403]
12:27:40 - INFO: Epoch 999 average loss: 0.045782
12:27:41 - INFO: Total sampling 86 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12:30:11 - INFO: Epoch 9 average loss: 0.204414
12:30:12 - INFO: Epoch 19 average loss: 0.240273
12:30:12 - INFO: Epoch 29 average loss: 0.172605
12:30:13 - INFO: Epoch 39 average loss: 0.211092
12:30:13 - INFO: Epoch 49 average loss: 0.242732
12:30:13 - INFO: Epoch 59 average loss: 0.359103
12:30:14 - INFO: Epoch 69 average loss: 0.143540
12:30:14 - INFO: Epoch 79 average loss: 0.154834
12:30:15 - INFO: Epoch 89 average loss: 0.147482
12:30:15 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... Now sampling 85st new convMatrix.... torch.Size([86, 16])
feature shape after [Data Augmentation]:  (174, 16)
(174, 16)
====>MLP training... Epoch: 20 total loss: 1.3836
====>MLP training... Epoch: 40 total loss: 0.4011
====>MLP training... Epoch: 60 total loss: 0.3661
====>MLP training... Epoch: 80 total loss: 0.3262
====>MLP training... Epoch: 100 total loss: 0.3871
====>MLP training... Epoch: 120 total loss: 0.3347
====>MLP training... Epoch: 140 total loss: 0.3090
====>MLP training... Epoch: 160 total loss: 0.3250
====>MLP training... Epoch: 180 total loss: 0.3559
====>MLP training... Epoch: 200 total loss: 0.3149
====>CNN training... Epoch: 0  total loss: 0.2567
====>CNN training... Epoch: 20  total loss: 0.2521
====>CNN training... Epoch: 40  total loss: 0.2467
====>CNN training... Epoch: 60  total loss: 0.2356
====>CNN training... Epoch: 80  total loss: 0.2113
====>CNN training... Epoch: 100  total loss: 0.1941
====>CNN training... Epoch: 120  total loss: 0.1845
====>CNN training... Epoch: 140  total loss: 0.1651
====>CNN training... Epoch: 160  total loss: 0.1607
====>CNN training... Epoch: 180  total loss: 0.1385
====>CNN training... Epoch: 200  total loss: 0.1255
====>RNN training... Epoch: 20 total loss: 0.2573
====>RNN training... Epoch: 40 total loss: 0.2500
====>RNN training... Epoch: 60 total loss: 0.2437
====>RNN training... Epoch: 80 total loss: 0.2382
====>RNN training... Epoch: 100 total loss: 0.2328
====>RNN training... Epoch: 120 total loss: 0.2273
====>RNN training... Epoch: 140 total loss: 0.2214
====>RNN training... Epoch: 160 total loss: 0.2148
====>RNN training... Epoch: 180 total loss: 0.2073
====>RNN training... Epoch: 200 total loss: 0.1985

test: Lang-2-fs_ddpm
fault line:  [200092]
feature shape before [Data Augmentation]:  (14, 93)
[0, 1, 2, 3, 4, 7, 8, 9, 11, 12, 14, 15, 29, 30, 32, 33, 34, 36, 38, 39, 41, 42, 44, 45, 46, 48, 50, 62, 64]
entropy 11.4256966644056 2.8523675236511474
req_shape:  16
statements selected: 16
statements selected: Index([100309, 100316, 100317, 100318, 100319, 200042, 200046, 200089, 200092,
       200093, 200096, 200097, 200117, 200118, 200121, 200134],
      dtype='int64')
Dataset shape: torch.Size([14, 1, 1, 16]) torch.Size([14])
Batch Dataloader shape: torch.Size([8, 1, 1, 16]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.173]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.139]100%|██████████| 2/2 [00:00<00:00, 46.34it/s, MSE=0.139]
12:30:15 - INFO: Epoch 99 average loss: 0.156169
12:30:16 - INFO: Epoch 109 average loss: 0.133392
12:30:16 - INFO: Epoch 119 average loss: 0.156818
12:30:17 - INFO: Epoch 129 average loss: 0.127666
12:30:17 - INFO: Epoch 139 average loss: 0.063235
12:30:18 - INFO: Epoch 149 average loss: 0.184293
12:30:18 - INFO: Epoch 159 average loss: 0.185853
12:30:19 - INFO: Epoch 169 average loss: 0.167329
12:30:19 - INFO: Epoch 179 average loss: 0.145322
12:30:20 - INFO: Epoch 189 average loss: 0.133761
12:30:20 - INFO: Starting epoch 199:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0368]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.119] 100%|██████████| 2/2 [00:00<00:00, 43.24it/s, MSE=0.119]
12:30:20 - INFO: Epoch 199 average loss: 0.077760
12:30:21 - INFO: Epoch 209 average loss: 0.143447
12:30:21 - INFO: Epoch 219 average loss: 0.139474
12:30:22 - INFO: Epoch 229 average loss: 0.105922
12:30:22 - INFO: Epoch 239 average loss: 0.151678
12:30:23 - INFO: Epoch 249 average loss: 0.104121
12:30:23 - INFO: Epoch 259 average loss: 0.105088
12:30:23 - INFO: Epoch 269 average loss: 0.103688
12:30:24 - INFO: Epoch 279 average loss: 0.111316
12:30:24 - INFO: Epoch 289 average loss: 0.102443
12:30:25 - INFO: Starting epoch 299:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.116]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.116]100%|██████████| 2/2 [00:00<00:00, 42.27it/s, MSE=0.116]
12:30:25 - INFO: Epoch 299 average loss: 0.116127
12:30:25 - INFO: Epoch 309 average loss: 0.100974
12:30:26 - INFO: Epoch 319 average loss: 0.108253
12:30:26 - INFO: Epoch 329 average loss: 0.080533
12:30:27 - INFO: Epoch 339 average loss: 0.060560
12:30:27 - INFO: Epoch 349 average loss: 0.093748
12:30:28 - INFO: Epoch 359 average loss: 0.078200
12:30:28 - INFO: Epoch 369 average loss: 0.127593
12:30:29 - INFO: Epoch 379 average loss: 0.139122
12:30:29 - INFO: Epoch 389 average loss: 0.161467
12:30:30 - INFO: Starting epoch 399:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.175]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0884]100%|██████████| 2/2 [00:00<00:00, 46.92it/s, MSE=0.0884]
12:30:30 - INFO: Epoch 399 average loss: 0.131596
12:30:30 - INFO: Epoch 409 average loss: 0.143430
12:30:31 - INFO: Epoch 419 average loss: 0.095078
12:30:31 - INFO: Epoch 429 average loss: 0.066275
12:30:32 - INFO: Epoch 439 average loss: 0.110018
12:30:32 - INFO: Epoch 449 average loss: 0.096573
12:30:33 - INFO: Epoch 459 average loss: 0.097409
12:30:33 - INFO: Epoch 469 average loss: 0.119096
12:30:34 - INFO: Epoch 479 average loss: 0.091893
12:30:34 - INFO: Epoch 489 average loss: 0.091507
12:30:34 - INFO: Starting epoch 499:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.243]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.114]100%|██████████| 2/2 [00:00<00:00, 40.42it/s, MSE=0.114]
12:30:34 - INFO: Epoch 499 average loss: 0.178832
12:30:35 - INFO: Epoch 509 average loss: 0.159671
12:30:36 - INFO: Epoch 519 average loss: 0.091054
12:30:36 - INFO: Epoch 529 average loss: 0.085583
12:30:36 - INFO: Epoch 539 average loss: 0.083872
12:30:37 - INFO: Epoch 549 average loss: 0.086575
12:30:37 - INFO: Epoch 559 average loss: 0.079329
12:30:37 - INFO: Epoch 569 average loss: 0.053997
12:30:38 - INFO: Epoch 579 average loss: 0.135401
12:30:38 - INFO: Epoch 589 average loss: 0.097881
12:30:39 - INFO: Starting epoch 599:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.172]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.102]100%|██████████| 2/2 [00:00<00:00, 50.57it/s, MSE=0.102]
12:30:39 - INFO: Epoch 599 average loss: 0.137199
12:30:40 - INFO: Epoch 609 average loss: 0.121975
12:30:40 - INFO: Epoch 619 average loss: 0.096752
12:30:40 - INFO: Epoch 629 average loss: 0.061321
12:30:41 - INFO: Epoch 639 average loss: 0.078975
12:30:41 - INFO: Epoch 649 average loss: 0.070885
12:30:42 - INFO: Epoch 659 average loss: 0.136411
12:30:42 - INFO: Epoch 669 average loss: 0.098252
12:30:43 - INFO: Epoch 679 average loss: 0.050448
12:30:43 - INFO: Epoch 689 average loss: 0.083196
12:30:43 - INFO: Starting epoch 699:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0428]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0676]100%|██████████| 2/2 [00:00<00:00, 43.64it/s, MSE=0.0676]
12:30:43 - INFO: Epoch 699 average loss: 0.055171
12:30:44 - INFO: Epoch 709 average loss: 0.075000
12:30:45 - INFO: Epoch 719 average loss: 0.045420
12:30:45 - INFO: Epoch 729 average loss: 0.040159
12:30:46 - INFO: Epoch 739 average loss: 0.090877
12:30:46 - INFO: Epoch 749 average loss: 0.117305
12:30:46 - INFO: Epoch 759 average loss: 0.047765
12:30:47 - INFO: Epoch 769 average loss: 0.174578
12:30:47 - INFO: Epoch 779 average loss: 0.116321
12:30:48 - INFO: Epoch 789 average loss: 0.127461
12:30:48 - INFO: Starting epoch 799:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0584]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0744]100%|██████████| 2/2 [00:00<00:00, 47.64it/s, MSE=0.0744]
12:30:48 - INFO: Epoch 799 average loss: 0.066420
12:30:49 - INFO: Epoch 809 average loss: 0.148240
12:30:49 - INFO: Epoch 819 average loss: 0.052855
12:30:49 - INFO: Epoch 829 average loss: 0.048438
12:30:50 - INFO: Epoch 839 average loss: 0.088640
12:30:50 - INFO: Epoch 849 average loss: 0.104599
12:30:51 - INFO: Epoch 859 average loss: 0.109974
12:30:51 - INFO: Epoch 869 average loss: 0.102737
12:30:52 - INFO: Epoch 879 average loss: 0.060966
12:30:52 - INFO: Epoch 889 average loss: 0.116501
12:30:53 - INFO: Starting epoch 899:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0626]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0642]100%|██████████| 2/2 [00:00<00:00, 53.68it/s, MSE=0.0642]
12:30:53 - INFO: Epoch 899 average loss: 0.063394
12:30:53 - INFO: Epoch 909 average loss: 0.090554
12:30:54 - INFO: Epoch 919 average loss: 0.095240
12:30:54 - INFO: Epoch 929 average loss: 0.120667
12:30:55 - INFO: Epoch 939 average loss: 0.179459
12:30:55 - INFO: Epoch 949 average loss: 0.103292
12:30:56 - INFO: Epoch 959 average loss: 0.048400
12:30:56 - INFO: Epoch 969 average loss: 0.073823
12:30:57 - INFO: Epoch 979 average loss: 0.112468
12:30:57 - INFO: Epoch 989 average loss: 0.077052
12:30:58 - INFO: Starting epoch 999:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0803]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.064] 100%|██████████| 2/2 [00:00<00:00, 34.45it/s, MSE=0.064]
12:30:58 - INFO: Epoch 999 average loss: 0.072109
12:30:58 - INFO: Total sampling 12 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12:31:35 - INFO: Epoch 9 average loss: 0.172088
12:31:36 - INFO: Epoch 19 average loss: 0.104208
12:31:37 - INFO: Epoch 29 average loss: 0.078498
12:31:39 - INFO: Epoch 39 average loss: 0.061492
12:31:40 - INFO: Epoch 49 average loss: 0.086110
12:31:41 - INFO: Epoch 59 average loss: 0.079166
12:31:43 - INFO: Epoch 69 average loss: 0.076424
12:31:44 - INFO: Epoch 79 average loss: 0.054341
12:31:45 - INFO: Epoch 89 average loss: 0.083473
12:31:46 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... torch.Size([12, 16])
feature shape after [Data Augmentation]:  (26, 16)
(26, 16)
====>MLP training... Epoch: 20 total loss: 0.9605
====>MLP training... Epoch: 40 total loss: 0.9142
====>MLP training... Epoch: 60 total loss: 0.5648
====>MLP training... Epoch: 80 total loss: 0.4165
====>MLP training... Epoch: 100 total loss: 0.6715
====>MLP training... Epoch: 120 total loss: 0.1762
====>MLP training... Epoch: 140 total loss: 0.4055
====>MLP training... Epoch: 160 total loss: 0.2882
====>MLP training... Epoch: 180 total loss: 0.2391
====>MLP training... Epoch: 200 total loss: 0.2329
====>CNN training... Epoch: 0  total loss: 0.2537
====>CNN training... Epoch: 20  total loss: 0.2504
====>CNN training... Epoch: 40  total loss: 0.2456
====>CNN training... Epoch: 60  total loss: 0.2426
====>CNN training... Epoch: 80  total loss: 0.2354
====>CNN training... Epoch: 100  total loss: 0.2250
====>CNN training... Epoch: 120  total loss: 0.2013
====>CNN training... Epoch: 140  total loss: 0.1738
====>CNN training... Epoch: 160  total loss: 0.1544
====>CNN training... Epoch: 180  total loss: 0.1236
====>CNN training... Epoch: 200  total loss: 0.1084
====>RNN training... Epoch: 20 total loss: 0.2503
====>RNN training... Epoch: 40 total loss: 0.2487
====>RNN training... Epoch: 60 total loss: 0.2472
====>RNN training... Epoch: 80 total loss: 0.2458
====>RNN training... Epoch: 100 total loss: 0.2445
====>RNN training... Epoch: 120 total loss: 0.2431
====>RNN training... Epoch: 140 total loss: 0.2418
====>RNN training... Epoch: 160 total loss: 0.2404
====>RNN training... Epoch: 180 total loss: 0.2390
====>RNN training... Epoch: 200 total loss: 0.2376

test: Lang-3-fs_ddpm
fault line:  [100593, 100597, 100601, 100605]
feature shape before [Data Augmentation]:  (87, 1723)
[128, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 139, 140, 141, 143, 145, 137, 56, 58, 60, 61, 62, 63, 66, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 91, 381, 116, 117, 118, 119, 374, 377, 379, 380, 125, 126, 127]
entropy 10.727832742901022 1.7891516418263516
req_shape:  16
statements selected: 16
statements selected: Index([100034, 100036, 100038, 100040, 100042, 100044, 100046, 100048, 100050,
       100052, 100054, 100056, 100058, 100060, 100062, 100621],
      dtype='int64')
Dataset shape: torch.Size([87, 1, 1, 16]) torch.Size([87])
Batch Dataloader shape: torch.Size([16, 1, 1, 16]) torch.Size([16])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0534]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0343]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0384]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0422]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.068]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0963]100%|██████████| 6/6 [00:00<00:00, 53.66it/s, MSE=0.0963]100%|██████████| 6/6 [00:00<00:00, 53.59it/s, MSE=0.0963]
12:31:47 - INFO: Epoch 99 average loss: 0.055432
12:31:48 - INFO: Epoch 109 average loss: 0.072633
12:31:50 - INFO: Epoch 119 average loss: 0.056894
12:31:51 - INFO: Epoch 129 average loss: 0.060018
12:31:52 - INFO: Epoch 139 average loss: 0.052998
12:31:53 - INFO: Epoch 149 average loss: 0.068249
12:31:55 - INFO: Epoch 159 average loss: 0.076750
12:31:56 - INFO: Epoch 169 average loss: 0.061754
12:31:58 - INFO: Epoch 179 average loss: 0.074934
12:31:59 - INFO: Epoch 189 average loss: 0.051278
12:32:01 - INFO: Starting epoch 199:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0512]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0505]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0653]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0436] 67%|██████▋   | 4/6 [00:00<00:00, 38.72it/s, MSE=0.0436] 67%|██████▋   | 4/6 [00:00<00:00, 38.72it/s, MSE=0.059]  67%|██████▋   | 4/6 [00:00<00:00, 38.72it/s, MSE=0.0788]100%|██████████| 6/6 [00:00<00:00, 38.12it/s, MSE=0.0788]
12:32:01 - INFO: Epoch 199 average loss: 0.058060
12:32:02 - INFO: Epoch 209 average loss: 0.061806
12:32:04 - INFO: Epoch 219 average loss: 0.059209
12:32:05 - INFO: Epoch 229 average loss: 0.054041
12:32:07 - INFO: Epoch 239 average loss: 0.062373
12:32:08 - INFO: Epoch 249 average loss: 0.056182
12:32:10 - INFO: Epoch 259 average loss: 0.055252
12:32:11 - INFO: Epoch 269 average loss: 0.072891
12:32:12 - INFO: Epoch 279 average loss: 0.041539
12:32:14 - INFO: Epoch 289 average loss: 0.042257
12:32:15 - INFO: Starting epoch 299:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0252]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.056]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0546]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0391]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0424] 83%|████████▎ | 5/6 [00:00<00:00, 42.92it/s, MSE=0.0424] 83%|████████▎ | 5/6 [00:00<00:00, 42.92it/s, MSE=0.0399]100%|██████████| 6/6 [00:00<00:00, 44.94it/s, MSE=0.0399]
12:32:15 - INFO: Epoch 299 average loss: 0.042840
12:32:17 - INFO: Epoch 309 average loss: 0.056216
12:32:18 - INFO: Epoch 319 average loss: 0.060648
12:32:19 - INFO: Epoch 329 average loss: 0.049137
12:32:21 - INFO: Epoch 339 average loss: 0.071336
12:32:22 - INFO: Epoch 349 average loss: 0.058254
12:32:24 - INFO: Epoch 359 average loss: 0.055923
12:32:25 - INFO: Epoch 369 average loss: 0.054945
12:32:26 - INFO: Epoch 379 average loss: 0.055601
12:32:27 - INFO: Epoch 389 average loss: 0.048255
12:32:29 - INFO: Starting epoch 399:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0275]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0736]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0402]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0507] 67%|██████▋   | 4/6 [00:00<00:00, 37.44it/s, MSE=0.0507] 67%|██████▋   | 4/6 [00:00<00:00, 37.44it/s, MSE=0.0398] 67%|██████▋   | 4/6 [00:00<00:00, 37.44it/s, MSE=0.0758]100%|██████████| 6/6 [00:00<00:00, 38.67it/s, MSE=0.0758]
12:32:29 - INFO: Epoch 399 average loss: 0.051259
12:32:31 - INFO: Epoch 409 average loss: 0.060049
12:32:32 - INFO: Epoch 419 average loss: 0.054084
12:32:33 - INFO: Epoch 429 average loss: 0.054063
12:32:35 - INFO: Epoch 439 average loss: 0.050782
12:32:36 - INFO: Epoch 449 average loss: 0.064986
12:32:37 - INFO: Epoch 459 average loss: 0.045816
12:32:38 - INFO: Epoch 469 average loss: 0.051003
12:32:40 - INFO: Epoch 479 average loss: 0.039917
12:32:41 - INFO: Epoch 489 average loss: 0.068716
12:32:42 - INFO: Starting epoch 499:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0892]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.063]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0504]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0514]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0562] 83%|████████▎ | 5/6 [00:00<00:00, 41.34it/s, MSE=0.0562] 83%|████████▎ | 5/6 [00:00<00:00, 41.34it/s, MSE=0.0458]100%|██████████| 6/6 [00:00<00:00, 40.72it/s, MSE=0.0458]
12:32:42 - INFO: Epoch 499 average loss: 0.059334
12:32:44 - INFO: Epoch 509 average loss: 0.049880
12:32:45 - INFO: Epoch 519 average loss: 0.043380
12:32:47 - INFO: Epoch 529 average loss: 0.059272
12:32:48 - INFO: Epoch 539 average loss: 0.046556
12:32:50 - INFO: Epoch 549 average loss: 0.046391
12:32:51 - INFO: Epoch 559 average loss: 0.068456
12:32:53 - INFO: Epoch 569 average loss: 0.046127
12:32:54 - INFO: Epoch 579 average loss: 0.054687
12:32:55 - INFO: Epoch 589 average loss: 0.042867
12:32:56 - INFO: Starting epoch 599:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0568]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0593]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0529]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0758]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.044]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0387]100%|██████████| 6/6 [00:00<00:00, 50.72it/s, MSE=0.0387]100%|██████████| 6/6 [00:00<00:00, 50.60it/s, MSE=0.0387]
12:32:56 - INFO: Epoch 599 average loss: 0.054599
12:32:58 - INFO: Epoch 609 average loss: 0.055543
12:32:59 - INFO: Epoch 619 average loss: 0.045038
12:33:00 - INFO: Epoch 629 average loss: 0.051765
12:33:01 - INFO: Epoch 639 average loss: 0.050929
12:33:03 - INFO: Epoch 649 average loss: 0.060994
12:33:04 - INFO: Epoch 659 average loss: 0.056714
12:33:06 - INFO: Epoch 669 average loss: 0.054411
12:33:07 - INFO: Epoch 679 average loss: 0.062713
12:33:08 - INFO: Epoch 689 average loss: 0.065544
12:33:10 - INFO: Starting epoch 699:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0458]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0353]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0372]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0361] 67%|██████▋   | 4/6 [00:00<00:00, 37.99it/s, MSE=0.0361] 67%|██████▋   | 4/6 [00:00<00:00, 37.99it/s, MSE=0.0629] 67%|██████▋   | 4/6 [00:00<00:00, 37.99it/s, MSE=0.0842]100%|██████████| 6/6 [00:00<00:00, 38.60it/s, MSE=0.0842]
12:33:10 - INFO: Epoch 699 average loss: 0.050259
12:33:11 - INFO: Epoch 709 average loss: 0.047206
12:33:13 - INFO: Epoch 719 average loss: 0.052094
12:33:14 - INFO: Epoch 729 average loss: 0.039925
12:33:15 - INFO: Epoch 739 average loss: 0.055472
12:33:17 - INFO: Epoch 749 average loss: 0.053910
12:33:18 - INFO: Epoch 759 average loss: 0.068843
12:33:20 - INFO: Epoch 769 average loss: 0.052043
12:33:21 - INFO: Epoch 779 average loss: 0.058093
12:33:23 - INFO: Epoch 789 average loss: 0.056384
12:33:24 - INFO: Starting epoch 799:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0375]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0311]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.1]     0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0361] 67%|██████▋   | 4/6 [00:00<00:00, 38.22it/s, MSE=0.0361] 67%|██████▋   | 4/6 [00:00<00:00, 38.22it/s, MSE=0.0604] 67%|██████▋   | 4/6 [00:00<00:00, 38.22it/s, MSE=0.0217]100%|██████████| 6/6 [00:00<00:00, 36.86it/s, MSE=0.0217]
12:33:24 - INFO: Epoch 799 average loss: 0.047816
12:33:26 - INFO: Epoch 809 average loss: 0.058848
12:33:27 - INFO: Epoch 819 average loss: 0.049154
12:33:29 - INFO: Epoch 829 average loss: 0.077798
12:33:30 - INFO: Epoch 839 average loss: 0.051456
12:33:31 - INFO: Epoch 849 average loss: 0.063427
12:33:33 - INFO: Epoch 859 average loss: 0.046803
12:33:34 - INFO: Epoch 869 average loss: 0.055836
12:33:35 - INFO: Epoch 879 average loss: 0.060212
12:33:37 - INFO: Epoch 889 average loss: 0.054737
12:33:38 - INFO: Starting epoch 899:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0243]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0268]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.056]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0539]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0332] 83%|████████▎ | 5/6 [00:00<00:00, 42.10it/s, MSE=0.0332] 83%|████████▎ | 5/6 [00:00<00:00, 42.10it/s, MSE=0.0151]100%|██████████| 6/6 [00:00<00:00, 42.94it/s, MSE=0.0151]
12:33:38 - INFO: Epoch 899 average loss: 0.034894
12:33:40 - INFO: Epoch 909 average loss: 0.043951
12:33:41 - INFO: Epoch 919 average loss: 0.075999
12:33:42 - INFO: Epoch 929 average loss: 0.046775
12:33:44 - INFO: Epoch 939 average loss: 0.046316
12:33:46 - INFO: Epoch 949 average loss: 0.046869
12:33:47 - INFO: Epoch 959 average loss: 0.043646
12:33:49 - INFO: Epoch 969 average loss: 0.048817
12:33:50 - INFO: Epoch 979 average loss: 0.061917
12:33:52 - INFO: Epoch 989 average loss: 0.049985
12:33:53 - INFO: Starting epoch 999:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0287]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0384]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0502]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0433]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0482]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0358]100%|██████████| 6/6 [00:00<00:00, 51.28it/s, MSE=0.0358]100%|██████████| 6/6 [00:00<00:00, 51.21it/s, MSE=0.0358]
12:33:53 - INFO: Epoch 999 average loss: 0.040762
12:33:54 - INFO: Total sampling 85 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
12:36:27 - INFO: Epoch 9 average loss: 0.288394
12:36:28 - INFO: Epoch 19 average loss: 0.170495
12:36:28 - INFO: Epoch 29 average loss: 0.146002
12:36:29 - INFO: Epoch 39 average loss: 0.132881
12:36:30 - INFO: Epoch 49 average loss: 0.105414
12:36:31 - INFO: Epoch 59 average loss: 0.088107
12:36:32 - INFO: Epoch 69 average loss: 0.096574
12:36:33 - INFO: Epoch 79 average loss: 0.079445
12:36:34 - INFO: Epoch 89 average loss: 0.134677
12:36:35 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... torch.Size([85, 16])
feature shape after [Data Augmentation]:  (172, 16)
(172, 16)
====>MLP training... Epoch: 20 total loss: 1.4752
====>MLP training... Epoch: 40 total loss: 1.0226
====>MLP training... Epoch: 60 total loss: 0.3097
====>MLP training... Epoch: 80 total loss: 0.2046
====>MLP training... Epoch: 100 total loss: 0.2126
====>MLP training... Epoch: 120 total loss: 0.2203
====>MLP training... Epoch: 140 total loss: 0.1971
====>MLP training... Epoch: 160 total loss: 0.2242
====>MLP training... Epoch: 180 total loss: 0.1925
====>MLP training... Epoch: 200 total loss: 0.2271
====>CNN training... Epoch: 0  total loss: 0.2491
====>CNN training... Epoch: 20  total loss: 0.2483
====>CNN training... Epoch: 40  total loss: 0.2441
====>CNN training... Epoch: 60  total loss: 0.2383
====>CNN training... Epoch: 80  total loss: 0.2166
====>CNN training... Epoch: 100  total loss: 0.1870
====>CNN training... Epoch: 120  total loss: 0.1569
====>CNN training... Epoch: 140  total loss: 0.1404
====>CNN training... Epoch: 160  total loss: 0.1177
====>CNN training... Epoch: 180  total loss: 0.1085
====>CNN training... Epoch: 200  total loss: 0.1076
====>RNN training... Epoch: 20 total loss: 0.2550
====>RNN training... Epoch: 40 total loss: 0.2431
====>RNN training... Epoch: 60 total loss: 0.2353
====>RNN training... Epoch: 80 total loss: 0.2301
====>RNN training... Epoch: 100 total loss: 0.2260
====>RNN training... Epoch: 120 total loss: 0.2217
====>RNN training... Epoch: 140 total loss: 0.2170
====>RNN training... Epoch: 160 total loss: 0.2115
====>RNN training... Epoch: 180 total loss: 0.2048
====>RNN training... Epoch: 200 total loss: 0.1968

test: Lang-4-fs_ddpm
fault line:  [300031, 300046, 300051, 300077]
feature shape before [Data Augmentation]:  (31, 63)
[8, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 62]
entropy 3.290719876273501 2.312393227306619
req_shape:  16
statements selected: 16
statements selected: Index([200032, 300045, 300046, 300047, 300048, 300049, 300050, 300051, 300052,
       300053, 300054, 300056, 300070, 300071, 300075, 300077],
      dtype='int64')
Dataset shape: torch.Size([31, 1, 1, 16]) torch.Size([31])
Batch Dataloader shape: torch.Size([8, 1, 1, 16]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.197]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0337]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.121]   0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0487]100%|██████████| 4/4 [00:00<00:00, 46.67it/s, MSE=0.0487]
12:36:35 - INFO: Epoch 99 average loss: 0.099966
12:36:36 - INFO: Epoch 109 average loss: 0.092502
12:36:37 - INFO: Epoch 119 average loss: 0.100390
12:36:37 - INFO: Epoch 129 average loss: 0.072380
12:36:38 - INFO: Epoch 139 average loss: 0.101653
12:36:39 - INFO: Epoch 149 average loss: 0.107930
12:36:40 - INFO: Epoch 159 average loss: 0.094952
12:36:41 - INFO: Epoch 169 average loss: 0.085411
12:36:42 - INFO: Epoch 179 average loss: 0.096885
12:36:43 - INFO: Epoch 189 average loss: 0.067056
12:36:43 - INFO: Starting epoch 199:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.065]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0625]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.134]   0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0639]100%|██████████| 4/4 [00:00<00:00, 38.52it/s, MSE=0.0639]100%|██████████| 4/4 [00:00<00:00, 38.41it/s, MSE=0.0639]
12:36:44 - INFO: Epoch 199 average loss: 0.081469
12:36:45 - INFO: Epoch 209 average loss: 0.091010
12:36:46 - INFO: Epoch 219 average loss: 0.072510
12:36:47 - INFO: Epoch 229 average loss: 0.089214
12:36:48 - INFO: Epoch 239 average loss: 0.065110
12:36:49 - INFO: Epoch 249 average loss: 0.088435
12:36:49 - INFO: Epoch 259 average loss: 0.107639
12:36:50 - INFO: Epoch 269 average loss: 0.071178
12:36:51 - INFO: Epoch 279 average loss: 0.084611
12:36:52 - INFO: Epoch 289 average loss: 0.066983
12:36:53 - INFO: Starting epoch 299:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.112]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0395]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.103]   0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0632]100%|██████████| 4/4 [00:00<00:00, 43.11it/s, MSE=0.0632]
12:36:53 - INFO: Epoch 299 average loss: 0.079475
12:36:54 - INFO: Epoch 309 average loss: 0.050011
12:36:55 - INFO: Epoch 319 average loss: 0.095507
12:36:56 - INFO: Epoch 329 average loss: 0.042239
12:36:57 - INFO: Epoch 339 average loss: 0.060485
12:36:58 - INFO: Epoch 349 average loss: 0.106330
12:36:59 - INFO: Epoch 359 average loss: 0.090865
12:37:00 - INFO: Epoch 369 average loss: 0.069557
12:37:01 - INFO: Epoch 379 average loss: 0.065436
12:37:02 - INFO: Epoch 389 average loss: 0.061070
12:37:02 - INFO: Starting epoch 399:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0507]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0686]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.102]   0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0386]100%|██████████| 4/4 [00:00<00:00, 49.92it/s, MSE=0.0386]
12:37:03 - INFO: Epoch 399 average loss: 0.064976
12:37:04 - INFO: Epoch 409 average loss: 0.059631
12:37:04 - INFO: Epoch 419 average loss: 0.067233
12:37:05 - INFO: Epoch 429 average loss: 0.085857
12:37:06 - INFO: Epoch 439 average loss: 0.061354
12:37:07 - INFO: Epoch 449 average loss: 0.095764
12:37:08 - INFO: Epoch 459 average loss: 0.058614
12:37:09 - INFO: Epoch 469 average loss: 0.086930
12:37:10 - INFO: Epoch 479 average loss: 0.085410
12:37:11 - INFO: Epoch 489 average loss: 0.059570
12:37:11 - INFO: Starting epoch 499:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.108]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0387]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0742]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0722]100%|██████████| 4/4 [00:00<00:00, 38.26it/s, MSE=0.0722]100%|██████████| 4/4 [00:00<00:00, 38.10it/s, MSE=0.0722]
12:37:11 - INFO: Epoch 499 average loss: 0.073233
12:37:13 - INFO: Epoch 509 average loss: 0.078330
12:37:13 - INFO: Epoch 519 average loss: 0.048395
12:37:14 - INFO: Epoch 529 average loss: 0.063375
12:37:15 - INFO: Epoch 539 average loss: 0.051220
12:37:16 - INFO: Epoch 549 average loss: 0.087913
12:37:17 - INFO: Epoch 559 average loss: 0.065618
12:37:18 - INFO: Epoch 569 average loss: 0.053108
12:37:19 - INFO: Epoch 579 average loss: 0.074413
12:37:20 - INFO: Epoch 589 average loss: 0.087133
12:37:20 - INFO: Starting epoch 599:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0478]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0398]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0348]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0964]100%|██████████| 4/4 [00:00<00:00, 38.75it/s, MSE=0.0964]100%|██████████| 4/4 [00:00<00:00, 38.60it/s, MSE=0.0964]
12:37:20 - INFO: Epoch 599 average loss: 0.054710
12:37:22 - INFO: Epoch 609 average loss: 0.072072
12:37:22 - INFO: Epoch 619 average loss: 0.065306
12:37:23 - INFO: Epoch 629 average loss: 0.071584
12:37:24 - INFO: Epoch 639 average loss: 0.064502
12:37:25 - INFO: Epoch 649 average loss: 0.052538
12:37:26 - INFO: Epoch 659 average loss: 0.042608
12:37:27 - INFO: Epoch 669 average loss: 0.060859
12:37:28 - INFO: Epoch 679 average loss: 0.056135
12:37:29 - INFO: Epoch 689 average loss: 0.107229
12:37:30 - INFO: Starting epoch 699:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0384]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0655]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0494]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0194]100%|██████████| 4/4 [00:00<00:00, 43.88it/s, MSE=0.0194]
12:37:30 - INFO: Epoch 699 average loss: 0.043188
12:37:31 - INFO: Epoch 709 average loss: 0.064082
12:37:32 - INFO: Epoch 719 average loss: 0.071021
12:37:33 - INFO: Epoch 729 average loss: 0.049972
12:37:34 - INFO: Epoch 739 average loss: 0.059678
12:37:34 - INFO: Epoch 749 average loss: 0.065489
12:37:35 - INFO: Epoch 759 average loss: 0.091470
12:37:36 - INFO: Epoch 769 average loss: 0.056741
12:37:37 - INFO: Epoch 779 average loss: 0.056747
12:37:38 - INFO: Epoch 789 average loss: 0.056542
12:37:39 - INFO: Starting epoch 799:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0648]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0366]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0702]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0207]100%|██████████| 4/4 [00:00<00:00, 55.73it/s, MSE=0.0207]
12:37:39 - INFO: Epoch 799 average loss: 0.048072
12:37:40 - INFO: Epoch 809 average loss: 0.063351
12:37:41 - INFO: Epoch 819 average loss: 0.059511
12:37:42 - INFO: Epoch 829 average loss: 0.067941
12:37:43 - INFO: Epoch 839 average loss: 0.053743
12:37:44 - INFO: Epoch 849 average loss: 0.063034
12:37:44 - INFO: Epoch 859 average loss: 0.073163
12:37:45 - INFO: Epoch 869 average loss: 0.054118
12:37:46 - INFO: Epoch 879 average loss: 0.038754
12:37:47 - INFO: Epoch 889 average loss: 0.060147
12:37:48 - INFO: Starting epoch 899:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0212]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0581]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0909]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0977]100%|██████████| 4/4 [00:00<00:00, 44.62it/s, MSE=0.0977]
12:37:48 - INFO: Epoch 899 average loss: 0.066966
12:37:49 - INFO: Epoch 909 average loss: 0.079305
12:37:50 - INFO: Epoch 919 average loss: 0.056685
12:37:51 - INFO: Epoch 929 average loss: 0.070874
12:37:52 - INFO: Epoch 939 average loss: 0.053540
12:37:53 - INFO: Epoch 949 average loss: 0.052606
12:37:54 - INFO: Epoch 959 average loss: 0.055856
12:37:55 - INFO: Epoch 969 average loss: 0.092492
12:37:56 - INFO: Epoch 979 average loss: 0.035945
12:37:57 - INFO: Epoch 989 average loss: 0.068072
12:37:57 - INFO: Starting epoch 999:
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0539]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0589]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0552]  0%|          | 0/4 [00:00<?, ?it/s, MSE=0.0449]100%|██████████| 4/4 [00:00<00:00, 41.54it/s, MSE=0.0449]
12:37:57 - INFO: Epoch 999 average loss: 0.053225
12:37:58 - INFO: Total sampling 29 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12:39:06 - INFO: Epoch 9 average loss: 0.250433
12:39:06 - INFO: Epoch 19 average loss: 0.231580
12:39:07 - INFO: Epoch 29 average loss: 0.268207
12:39:07 - INFO: Epoch 39 average loss: 0.192904
12:39:08 - INFO: Epoch 49 average loss: 0.183234
12:39:08 - INFO: Epoch 59 average loss: 0.147223
12:39:09 - INFO: Epoch 69 average loss: 0.096072
12:39:09 - INFO: Epoch 79 average loss: 0.126572
12:39:10 - INFO: Epoch 89 average loss: 0.130012
12:39:10 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... torch.Size([29, 16])
feature shape after [Data Augmentation]:  (60, 16)
(60, 16)
====>MLP training... Epoch: 20 total loss: 1.9074
====>MLP training... Epoch: 40 total loss: 1.5613
====>MLP training... Epoch: 60 total loss: 1.1624
====>MLP training... Epoch: 80 total loss: 1.0536
====>MLP training... Epoch: 100 total loss: 1.0268
====>MLP training... Epoch: 120 total loss: 1.0116
====>MLP training... Epoch: 140 total loss: 0.9566
====>MLP training... Epoch: 160 total loss: 0.8535
====>MLP training... Epoch: 180 total loss: 0.9739
====>MLP training... Epoch: 200 total loss: 0.8822
====>CNN training... Epoch: 0  total loss: 0.2533
====>CNN training... Epoch: 20  total loss: 0.2504
====>CNN training... Epoch: 40  total loss: 0.2486
====>CNN training... Epoch: 60  total loss: 0.2458
====>CNN training... Epoch: 80  total loss: 0.2367
====>CNN training... Epoch: 100  total loss: 0.2273
====>CNN training... Epoch: 120  total loss: 0.2019
====>CNN training... Epoch: 140  total loss: 0.1848
====>CNN training... Epoch: 160  total loss: 0.1655
====>CNN training... Epoch: 180  total loss: 0.1513
====>CNN training... Epoch: 200  total loss: 0.1447
====>RNN training... Epoch: 20 total loss: 0.2488
====>RNN training... Epoch: 40 total loss: 0.2464
====>RNN training... Epoch: 60 total loss: 0.2446
====>RNN training... Epoch: 80 total loss: 0.2433
====>RNN training... Epoch: 100 total loss: 0.2423
====>RNN training... Epoch: 120 total loss: 0.2414
====>RNN training... Epoch: 140 total loss: 0.2406
====>RNN training... Epoch: 160 total loss: 0.2399
====>RNN training... Epoch: 180 total loss: 0.2391
====>RNN training... Epoch: 200 total loss: 0.2383

test: Lang-5-fs_ddpm
fault line:  [200097, 200128]
feature shape before [Data Augmentation]:  (13, 79)
[0, 1, 2, 3, 4, 7, 8, 9, 11, 12, 14, 15, 16, 17, 48, 50]
entropy 4.537874818500184 1.0847574921673764
req_shape:  16
statements selected: 16
statements selected: Index([100288, 100295, 100296, 100297, 100298, 200042, 200046, 200089, 200092,
       200093, 200096, 200097, 200098, 200099, 200193, 200218],
      dtype='int64')
Dataset shape: torch.Size([13, 1, 1, 16]) torch.Size([13])
Batch Dataloader shape: torch.Size([8, 1, 1, 16]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.109]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.222]100%|██████████| 2/2 [00:00<00:00, 51.73it/s, MSE=0.222]
12:39:10 - INFO: Epoch 99 average loss: 0.165460
12:39:11 - INFO: Epoch 109 average loss: 0.079068
12:39:11 - INFO: Epoch 119 average loss: 0.140655
12:39:11 - INFO: Epoch 129 average loss: 0.110852
12:39:12 - INFO: Epoch 139 average loss: 0.148052
12:39:12 - INFO: Epoch 149 average loss: 0.101497
12:39:13 - INFO: Epoch 159 average loss: 0.187866
12:39:13 - INFO: Epoch 169 average loss: 0.069290
12:39:14 - INFO: Epoch 179 average loss: 0.098615
12:39:14 - INFO: Epoch 189 average loss: 0.102583
12:39:15 - INFO: Starting epoch 199:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.053]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.17] 100%|██████████| 2/2 [00:00<00:00, 49.60it/s, MSE=0.17]
12:39:15 - INFO: Epoch 199 average loss: 0.111383
12:39:15 - INFO: Epoch 209 average loss: 0.089239
12:39:16 - INFO: Epoch 219 average loss: 0.176970
12:39:16 - INFO: Epoch 229 average loss: 0.227606
12:39:16 - INFO: Epoch 239 average loss: 0.093980
12:39:17 - INFO: Epoch 249 average loss: 0.104729
12:39:17 - INFO: Epoch 259 average loss: 0.048054
12:39:18 - INFO: Epoch 269 average loss: 0.203306
12:39:18 - INFO: Epoch 279 average loss: 0.180811
12:39:19 - INFO: Epoch 289 average loss: 0.195398
12:39:19 - INFO: Starting epoch 299:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.132]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0668]100%|██████████| 2/2 [00:00<00:00, 50.61it/s, MSE=0.0668]
12:39:19 - INFO: Epoch 299 average loss: 0.099197
12:39:20 - INFO: Epoch 309 average loss: 0.164987
12:39:20 - INFO: Epoch 319 average loss: 0.127570
12:39:21 - INFO: Epoch 329 average loss: 0.102425
12:39:21 - INFO: Epoch 339 average loss: 0.092646
12:39:22 - INFO: Epoch 349 average loss: 0.113760
12:39:22 - INFO: Epoch 359 average loss: 0.066462
12:39:22 - INFO: Epoch 369 average loss: 0.072663
12:39:23 - INFO: Epoch 379 average loss: 0.206537
12:39:23 - INFO: Epoch 389 average loss: 0.089494
12:39:24 - INFO: Starting epoch 399:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0579]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0567]100%|██████████| 2/2 [00:00<00:00, 41.64it/s, MSE=0.0567]
12:39:24 - INFO: Epoch 399 average loss: 0.057314
12:39:25 - INFO: Epoch 409 average loss: 0.247160
12:39:25 - INFO: Epoch 419 average loss: 0.098237
12:39:25 - INFO: Epoch 429 average loss: 0.072336
12:39:26 - INFO: Epoch 439 average loss: 0.135301
12:39:26 - INFO: Epoch 449 average loss: 0.137557
12:39:27 - INFO: Epoch 459 average loss: 0.031460
12:39:27 - INFO: Epoch 469 average loss: 0.113083
12:39:28 - INFO: Epoch 479 average loss: 0.143960
12:39:28 - INFO: Epoch 489 average loss: 0.074097
12:39:29 - INFO: Starting epoch 499:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.277]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0905]100%|██████████| 2/2 [00:00<00:00, 61.32it/s, MSE=0.0905]
12:39:29 - INFO: Epoch 499 average loss: 0.183586
12:39:29 - INFO: Epoch 509 average loss: 0.129905
12:39:30 - INFO: Epoch 519 average loss: 0.219956
12:39:30 - INFO: Epoch 529 average loss: 0.105653
12:39:31 - INFO: Epoch 539 average loss: 0.111879
12:39:31 - INFO: Epoch 549 average loss: 0.126393
12:39:32 - INFO: Epoch 559 average loss: 0.047567
12:39:32 - INFO: Epoch 569 average loss: 0.060792
12:39:32 - INFO: Epoch 579 average loss: 0.047938
12:39:33 - INFO: Epoch 589 average loss: 0.058430
12:39:33 - INFO: Starting epoch 599:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0766]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0894]100%|██████████| 2/2 [00:00<00:00, 42.53it/s, MSE=0.0894]
12:39:33 - INFO: Epoch 599 average loss: 0.083022
12:39:34 - INFO: Epoch 609 average loss: 0.101354
12:39:34 - INFO: Epoch 619 average loss: 0.112380
12:39:35 - INFO: Epoch 629 average loss: 0.122002
12:39:35 - INFO: Epoch 639 average loss: 0.048069
12:39:36 - INFO: Epoch 649 average loss: 0.073502
12:39:36 - INFO: Epoch 659 average loss: 0.095337
12:39:37 - INFO: Epoch 669 average loss: 0.043661
12:39:37 - INFO: Epoch 679 average loss: 0.121391
12:39:38 - INFO: Epoch 689 average loss: 0.100850
12:39:38 - INFO: Starting epoch 699:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0507]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0949]100%|██████████| 2/2 [00:00<00:00, 35.67it/s, MSE=0.0949]
12:39:38 - INFO: Epoch 699 average loss: 0.072834
12:39:39 - INFO: Epoch 709 average loss: 0.170015
12:39:39 - INFO: Epoch 719 average loss: 0.114157
12:39:40 - INFO: Epoch 729 average loss: 0.132989
12:39:40 - INFO: Epoch 739 average loss: 0.075300
12:39:41 - INFO: Epoch 749 average loss: 0.119703
12:39:41 - INFO: Epoch 759 average loss: 0.141936
12:39:42 - INFO: Epoch 769 average loss: 0.078528
12:39:42 - INFO: Epoch 779 average loss: 0.068149
12:39:43 - INFO: Epoch 789 average loss: 0.143392
12:39:43 - INFO: Starting epoch 799:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0872]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.108] 100%|██████████| 2/2 [00:00<00:00, 42.36it/s, MSE=0.108]
12:39:43 - INFO: Epoch 799 average loss: 0.097580
12:39:44 - INFO: Epoch 809 average loss: 0.087300
12:39:44 - INFO: Epoch 819 average loss: 0.113672
12:39:45 - INFO: Epoch 829 average loss: 0.150890
12:39:45 - INFO: Epoch 839 average loss: 0.199406
12:39:46 - INFO: Epoch 849 average loss: 0.095026
12:39:46 - INFO: Epoch 859 average loss: 0.116775
12:39:46 - INFO: Epoch 869 average loss: 0.073073
12:39:47 - INFO: Epoch 879 average loss: 0.094695
12:39:47 - INFO: Epoch 889 average loss: 0.093005
12:39:48 - INFO: Starting epoch 899:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.195]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0644]100%|██████████| 2/2 [00:00<00:00, 40.97it/s, MSE=0.0644]
12:39:48 - INFO: Epoch 899 average loss: 0.129705
12:39:48 - INFO: Epoch 909 average loss: 0.051810
12:39:49 - INFO: Epoch 919 average loss: 0.078897
12:39:49 - INFO: Epoch 929 average loss: 0.069349
12:39:50 - INFO: Epoch 939 average loss: 0.099950
12:39:50 - INFO: Epoch 949 average loss: 0.121253
12:39:51 - INFO: Epoch 959 average loss: 0.051954
12:39:51 - INFO: Epoch 969 average loss: 0.174045
12:39:52 - INFO: Epoch 979 average loss: 0.062702
12:39:52 - INFO: Epoch 989 average loss: 0.077245
12:39:53 - INFO: Starting epoch 999:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.126]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.183]100%|██████████| 2/2 [00:00<00:00, 39.73it/s, MSE=0.183]
12:39:53 - INFO: Epoch 999 average loss: 0.154362
12:39:53 - INFO: Total sampling 11 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12:40:24 - INFO: Epoch 9 average loss: 0.167897
12:40:25 - INFO: Epoch 19 average loss: 0.082464
12:40:26 - INFO: Epoch 29 average loss: 0.148334
12:40:27 - INFO: Epoch 39 average loss: 0.093831
12:40:28 - INFO: Epoch 49 average loss: 0.089742
12:40:29 - INFO: Epoch 59 average loss: 0.392198
12:40:30 - INFO: Epoch 69 average loss: 0.131317
12:40:31 - INFO: Epoch 79 average loss: 0.084966
12:40:33 - INFO: Epoch 89 average loss: 0.063418
12:40:33 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... torch.Size([11, 16])
feature shape after [Data Augmentation]:  (24, 16)
(24, 16)
====>MLP training... Epoch: 20 total loss: 0.7396
====>MLP training... Epoch: 40 total loss: 0.7108
====>MLP training... Epoch: 60 total loss: 0.6558
====>MLP training... Epoch: 80 total loss: 0.5374
====>MLP training... Epoch: 100 total loss: 0.4933
====>MLP training... Epoch: 120 total loss: 0.3589
====>MLP training... Epoch: 140 total loss: 0.3263
====>MLP training... Epoch: 160 total loss: 0.3563
====>MLP training... Epoch: 180 total loss: 0.1839
====>MLP training... Epoch: 200 total loss: 0.2554
====>CNN training... Epoch: 0  total loss: 0.2599
====>CNN training... Epoch: 20  total loss: 0.2517
====>CNN training... Epoch: 40  total loss: 0.2465
====>CNN training... Epoch: 60  total loss: 0.2424
====>CNN training... Epoch: 80  total loss: 0.2320
====>CNN training... Epoch: 100  total loss: 0.2195
====>CNN training... Epoch: 120  total loss: 0.1724
====>CNN training... Epoch: 140  total loss: 0.1416
====>CNN training... Epoch: 160  total loss: 0.1154
====>CNN training... Epoch: 180  total loss: 0.0781
====>CNN training... Epoch: 200  total loss: 0.0760
====>RNN training... Epoch: 20 total loss: 0.2491
====>RNN training... Epoch: 40 total loss: 0.2469
====>RNN training... Epoch: 60 total loss: 0.2448
====>RNN training... Epoch: 80 total loss: 0.2428
====>RNN training... Epoch: 100 total loss: 0.2410
====>RNN training... Epoch: 120 total loss: 0.2392
====>RNN training... Epoch: 140 total loss: 0.2374
====>RNN training... Epoch: 160 total loss: 0.2355
====>RNN training... Epoch: 180 total loss: 0.2337
====>RNN training... Epoch: 200 total loss: 0.2317

test: Lang-6-fs_ddpm
fault line:  [200095]
feature shape before [Data Augmentation]:  (130, 2817)
[0, 1, 2, 13, 14, 16, 17, 21, 23, 25, 26, 27, 28, 29, 34, 35, 38, 39, 40, 41, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 67, 69, 70, 71, 77, 78, 79, 80, 82, 83, 87, 90, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2689, 2706, 2707, 2708, 2710, 2711, 2746, 2747, 2759, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2793, 2794, 2795, 2796, 2797, 2800, 328, 330, 331, 332, 333, 334, 335, 336, 337, 344, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1465, 1467]
entropy 62.40543051246282 2.076195488082423
req_shape:  16
statements selected: 16
statements selected: Index([100268, 100272, 100273, 200032, 200054, 200058, 200059, 200076, 200079,
       200082, 200083, 200084, 200085, 200086, 200094, 200095],
      dtype='int64')
Dataset shape: torch.Size([130, 1, 1, 16]) torch.Size([130])
Batch Dataloader shape: torch.Size([32, 1, 1, 16]) torch.Size([32])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0755]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0621]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.122]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0482]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0472]100%|██████████| 5/5 [00:00<00:00, 51.96it/s, MSE=0.0472]
12:40:34 - INFO: Epoch 99 average loss: 0.070992
12:40:35 - INFO: Epoch 109 average loss: 0.069938
12:40:36 - INFO: Epoch 119 average loss: 0.104490
12:40:37 - INFO: Epoch 129 average loss: 0.080400
12:40:38 - INFO: Epoch 139 average loss: 0.085796
12:40:39 - INFO: Epoch 149 average loss: 0.065271
12:40:40 - INFO: Epoch 159 average loss: 0.060368
12:40:41 - INFO: Epoch 169 average loss: 0.070618
12:40:42 - INFO: Epoch 179 average loss: 0.056531
12:40:43 - INFO: Epoch 189 average loss: 0.066945
12:40:44 - INFO: Starting epoch 199:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0653]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0745]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0797]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0573]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0772]100%|██████████| 5/5 [00:00<00:00, 56.69it/s, MSE=0.0772]
12:40:44 - INFO: Epoch 199 average loss: 0.070790
12:40:46 - INFO: Epoch 209 average loss: 0.063479
12:40:47 - INFO: Epoch 219 average loss: 0.059199
12:40:48 - INFO: Epoch 229 average loss: 0.076383
12:40:49 - INFO: Epoch 239 average loss: 0.055557
12:40:50 - INFO: Epoch 249 average loss: 0.051421
12:40:51 - INFO: Epoch 259 average loss: 0.076443
12:40:52 - INFO: Epoch 269 average loss: 0.078189
12:40:53 - INFO: Epoch 279 average loss: 0.067905
12:40:54 - INFO: Epoch 289 average loss: 0.059800
12:40:55 - INFO: Starting epoch 299:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0543]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0455]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0904]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0522]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.00606]100%|██████████| 5/5 [00:00<00:00, 44.23it/s, MSE=0.00606]100%|██████████| 5/5 [00:00<00:00, 44.03it/s, MSE=0.00606]
12:40:55 - INFO: Epoch 299 average loss: 0.049685
12:40:57 - INFO: Epoch 309 average loss: 0.062838
12:40:58 - INFO: Epoch 319 average loss: 0.063151
12:40:59 - INFO: Epoch 329 average loss: 0.068468
12:41:00 - INFO: Epoch 339 average loss: 0.053879
12:41:01 - INFO: Epoch 349 average loss: 0.061446
12:41:02 - INFO: Epoch 359 average loss: 0.092084
12:41:04 - INFO: Epoch 369 average loss: 0.058977
12:41:05 - INFO: Epoch 379 average loss: 0.091057
12:41:06 - INFO: Epoch 389 average loss: 0.066419
12:41:07 - INFO: Starting epoch 399:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0587]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0411]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0894]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0719]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.158] 100%|██████████| 5/5 [00:00<00:00, 42.12it/s, MSE=0.158]100%|██████████| 5/5 [00:00<00:00, 41.92it/s, MSE=0.158]
12:41:07 - INFO: Epoch 399 average loss: 0.083907
12:41:08 - INFO: Epoch 409 average loss: 0.038812
12:41:09 - INFO: Epoch 419 average loss: 0.109174
12:41:10 - INFO: Epoch 429 average loss: 0.093868
12:41:12 - INFO: Epoch 439 average loss: 0.078628
12:41:13 - INFO: Epoch 449 average loss: 0.061928
12:41:14 - INFO: Epoch 459 average loss: 0.080501
12:41:15 - INFO: Epoch 469 average loss: 0.070855
12:41:16 - INFO: Epoch 479 average loss: 0.067284
12:41:17 - INFO: Epoch 489 average loss: 0.057787
12:41:18 - INFO: Starting epoch 499:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0868]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0725]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0843]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.057]  80%|████████  | 4/5 [00:00<00:00, 34.21it/s, MSE=0.057] 80%|████████  | 4/5 [00:00<00:00, 34.21it/s, MSE=0.017]100%|██████████| 5/5 [00:00<00:00, 32.97it/s, MSE=0.017]
12:41:18 - INFO: Epoch 499 average loss: 0.063528
12:41:20 - INFO: Epoch 509 average loss: 0.060985
12:41:21 - INFO: Epoch 519 average loss: 0.064167
12:41:22 - INFO: Epoch 529 average loss: 0.060082
12:41:23 - INFO: Epoch 539 average loss: 0.063519
12:41:24 - INFO: Epoch 549 average loss: 0.059160
12:41:25 - INFO: Epoch 559 average loss: 0.057860
12:41:26 - INFO: Epoch 569 average loss: 0.078933
12:41:27 - INFO: Epoch 579 average loss: 0.043166
12:41:28 - INFO: Epoch 589 average loss: 0.059718
12:41:30 - INFO: Starting epoch 599:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0663]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.051]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0634]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0534] 80%|████████  | 4/5 [00:00<00:00, 35.53it/s, MSE=0.0534] 80%|████████  | 4/5 [00:00<00:00, 35.53it/s, MSE=0.0767]100%|██████████| 5/5 [00:00<00:00, 36.54it/s, MSE=0.0767]
12:41:30 - INFO: Epoch 599 average loss: 0.062159
12:41:31 - INFO: Epoch 609 average loss: 0.061777
12:41:32 - INFO: Epoch 619 average loss: 0.080461
12:41:33 - INFO: Epoch 629 average loss: 0.080002
12:41:34 - INFO: Epoch 639 average loss: 0.065175
12:41:35 - INFO: Epoch 649 average loss: 0.054076
12:41:36 - INFO: Epoch 659 average loss: 0.079370
12:41:38 - INFO: Epoch 669 average loss: 0.068421
12:41:38 - INFO: Epoch 679 average loss: 0.078435
12:41:40 - INFO: Epoch 689 average loss: 0.056758
12:41:41 - INFO: Starting epoch 699:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0624]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0463]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0494]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0637]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.124] 100%|██████████| 5/5 [00:00<00:00, 40.93it/s, MSE=0.124]100%|██████████| 5/5 [00:00<00:00, 40.74it/s, MSE=0.124]
12:41:41 - INFO: Epoch 699 average loss: 0.069178
12:41:42 - INFO: Epoch 709 average loss: 0.055764
12:41:43 - INFO: Epoch 719 average loss: 0.057571
12:41:44 - INFO: Epoch 729 average loss: 0.228629
12:41:45 - INFO: Epoch 739 average loss: 0.073424
12:41:47 - INFO: Epoch 749 average loss: 0.084708
12:41:48 - INFO: Epoch 759 average loss: 0.048778
12:41:49 - INFO: Epoch 769 average loss: 0.072781
12:41:50 - INFO: Epoch 779 average loss: 0.064264
12:41:51 - INFO: Epoch 789 average loss: 0.051747
12:41:52 - INFO: Starting epoch 799:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0253]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0578]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0616]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0578]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.202] 100%|██████████| 5/5 [00:00<00:00, 44.07it/s, MSE=0.202]100%|██████████| 5/5 [00:00<00:00, 43.99it/s, MSE=0.202]
12:41:52 - INFO: Epoch 799 average loss: 0.081003
12:41:53 - INFO: Epoch 809 average loss: 0.045452
12:41:54 - INFO: Epoch 819 average loss: 0.065647
12:41:55 - INFO: Epoch 829 average loss: 0.076083
12:41:57 - INFO: Epoch 839 average loss: 0.050793
12:41:58 - INFO: Epoch 849 average loss: 0.064710
12:41:59 - INFO: Epoch 859 average loss: 0.063829
12:42:00 - INFO: Epoch 869 average loss: 0.049491
12:42:01 - INFO: Epoch 879 average loss: 0.072172
12:42:02 - INFO: Epoch 889 average loss: 0.061610
12:42:03 - INFO: Starting epoch 899:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0613]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0651]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0528]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0627]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0219]100%|██████████| 5/5 [00:00<00:00, 60.84it/s, MSE=0.0219]
12:42:03 - INFO: Epoch 899 average loss: 0.052757
12:42:04 - INFO: Epoch 909 average loss: 0.049545
12:42:05 - INFO: Epoch 919 average loss: 0.057721
12:42:06 - INFO: Epoch 929 average loss: 0.074153
12:42:07 - INFO: Epoch 939 average loss: 0.047733
12:42:08 - INFO: Epoch 949 average loss: 0.059590
12:42:09 - INFO: Epoch 959 average loss: 0.053783
12:42:10 - INFO: Epoch 969 average loss: 0.067428
12:42:11 - INFO: Epoch 979 average loss: 0.109821
12:42:13 - INFO: Epoch 989 average loss: 0.042778
12:42:14 - INFO: Starting epoch 999:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0664]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0729]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0823]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0516]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.00848]100%|██████████| 5/5 [00:00<00:00, 53.37it/s, MSE=0.00848]
12:42:14 - INFO: Epoch 999 average loss: 0.056340
12:42:14 - INFO: Total sampling 128 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... Now sampling 85st new convMatrix.... Now sampling 86st new convMatrix.... Now sampling 87st new convMatrix.... Now sampling 88st new convMatrix.... Now sampling 89st new convMatrix.... Now sampling 90st new convMatrix.... Now sampling 91st new convMatrix.... Now sampling 92st new convMatrix.... Now sampling 93st new convMatrix.... Now sampling 94st new convMatrix.... Now sampling 95st new convMatrix.... Now sampling 96st new convMatrix.... Now sampling 97st new convMatrix.... Now sampling 98st new convMatrix.... Now sampling 99st new convMatrix.... Now sampling 100st new convMatrix.... Now sampling 101st new convMatrix.... Now sampling 102st new convMatrix.... Now sampling 103st new convMatrix.... Now sampling 104st new convMatrix.... Now sampling 105st new convMatrix.... Now sampling 106st new convMatrix.... Now sampling 107st new convMatrix.... Now sampling 108st new convMatrix.... Now sampling 109st new convMatrix.... Now sampling 110st new convMatrix.... Now sampling 111st new convMatrix.... Now sampling 112st new convMatrix.... Now sampling 113st new convMatrix.... Now sampling 114st new convMatrix.... Now sampling 115st new convMatrix.... Now sampling 116st new convMatrix.... Now sampling 117st new convMatrix.... Now sampling 118st new convMatrix.... Now sampling 119st new convMatrix.... Now sampling 120st new convMatrix.... Now sampling 121st new convMatrix.... Now sampling 122st new convMatrix.... Now sampling 123st new convMatrix.... Now sampling 124st new convMatrix.... Now sampling 125st new convMatrix.... Now sampling 126st new convMatrix.... Now sampling 127st new convMatrix.... torch.Size([128, 16])
feature shape after [Data Augmentation]:  (258, 16)
(258, 16)
====>MLP training... Epoch: 20 total loss: 1.1065
====>MLP training... Epoch: 40 total loss: 0.9767
====>MLP training... Epoch: 60 total loss: 0.5915
====>MLP training... Epoch: 80 total loss: 0.6124
====>MLP training... Epoch: 100 total loss: 0.4321
====>MLP training... Epoch: 120 total loss: 0.3313
====>MLP training... Epoch: 140 total loss: 0.2465
====>MLP training... Epoch: 160 total loss: 0.2940
====>MLP training... Epoch: 180 total loss: 0.2126
====>MLP training... Epoch: 200 total loss: 0.2327
====>CNN training... Epoch: 0  total loss: 0.2592
====>CNN training... Epoch: 20  total loss: 0.2560
====>CNN training... Epoch: 40  total loss: 0.2518
====>CNN training... Epoch: 60  total loss: 0.2480
====>CNN training... Epoch: 80  total loss: 0.2432
====>CNN training... Epoch: 100  total loss: 0.2344
====>CNN training... Epoch: 120  total loss: 0.2179
====>CNN training... Epoch: 140  total loss: 0.2028
====>CNN training... Epoch: 160  total loss: 0.1848
====>CNN training... Epoch: 180  total loss: 0.1740
====>CNN training... Epoch: 200  total loss: 0.1649
====>RNN training... Epoch: 20 total loss: 0.2345
====>RNN training... Epoch: 40 total loss: 0.2286
====>RNN training... Epoch: 60 total loss: 0.2221
====>RNN training... Epoch: 80 total loss: 0.2152
====>RNN training... Epoch: 100 total loss: 0.2081
====>RNN training... Epoch: 120 total loss: 0.2008
====>RNN training... Epoch: 140 total loss: 0.1933
====>RNN training... Epoch: 160 total loss: 0.1859
====>RNN training... Epoch: 180 total loss: 0.1788
====>RNN training... Epoch: 200 total loss: 0.1721

test: Lang-7-fs_ddpm
fault line:  [400452, 400453, 400454, 400721, 400725]
feature shape before [Data Augmentation]:  (82, 1847)
[1536, 1537, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 1540, 1541, 79, 80, 81, 82, 1542, 1611, 1613, 1614, 1616, 1617, 89, 90, 1544, 92, 93, 1545, 95, 96, 1624, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 1548, 115, 1549, 117, 119, 121, 123, 1551, 125, 127, 129, 130, 1552, 1553, 137, 138, 1554, 140, 142, 143, 144, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1565, 1566, 1567, 1568, 1570, 1571, 1572, 1573, 1575, 1785, 1787, 1788, 1790, 1547, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1538, 1586, 1587, 1588, 1589, 1590, 1539, 1591, 1593, 1594, 1595, 1599, 1601, 1602, 1603, 1605, 1607, 1608, 1610, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1619, 1620, 1622, 1546, 1524, 1525, 1526, 1528, 1529, 1530, 1531, 1532, 1533, 1534]
entropy 25.999512776862208 5.174406229332051
req_shape:  16
statements selected: 16
statements selected: Index([100089, 100111, 100129, 100147, 100165, 100186, 100204, 100222, 400045,
       400047, 400049, 400051, 400460, 400484, 400494, 400563],
      dtype='int64')
Dataset shape: torch.Size([82, 1, 1, 16]) 12:45:54 - INFO: Epoch 9 average loss: 0.209609
12:45:56 - INFO: Epoch 19 average loss: 0.103998
12:45:57 - INFO: Epoch 29 average loss: 0.152891
12:45:58 - INFO: Epoch 39 average loss: 0.098759
12:46:00 - INFO: Epoch 49 average loss: 0.107102
12:46:01 - INFO: Epoch 59 average loss: 0.100784
12:46:03 - INFO: Epoch 69 average loss: 0.097616
12:46:04 - INFO: Epoch 79 average loss: 0.117101
12:46:05 - INFO: Epoch 89 average loss: 0.113288
12:46:07 - INFO: Starting epoch 99:
torch.Size([82])
Batch Dataloader shape: torch.Size([16, 1, 1, 16]) torch.Size([16])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.11]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.103]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0493]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.103]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.147]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.095]100%|██████████| 6/6 [00:00<00:00, 50.37it/s, MSE=0.095]100%|██████████| 6/6 [00:00<00:00, 50.05it/s, MSE=0.095]
12:46:07 - INFO: Epoch 99 average loss: 0.101048
12:46:09 - INFO: Epoch 109 average loss: 0.082690
12:46:10 - INFO: Epoch 119 average loss: 0.089611
12:46:11 - INFO: Epoch 129 average loss: 0.100587
12:46:13 - INFO: Epoch 139 average loss: 0.079559
12:46:14 - INFO: Epoch 149 average loss: 0.080697
12:46:16 - INFO: Epoch 159 average loss: 0.085783
12:46:17 - INFO: Epoch 169 average loss: 0.098493
12:46:18 - INFO: Epoch 179 average loss: 0.078787
12:46:20 - INFO: Epoch 189 average loss: 0.098161
12:46:21 - INFO: Starting epoch 199:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0729]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0686]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0504]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0517] 67%|██████▋   | 4/6 [00:00<00:00, 36.21it/s, MSE=0.0517] 67%|██████▋   | 4/6 [00:00<00:00, 36.21it/s, MSE=0.0908] 67%|██████▋   | 4/6 [00:00<00:00, 36.21it/s, MSE=0.0582]100%|██████████| 6/6 [00:00<00:00, 40.28it/s, MSE=0.0582]
12:46:21 - INFO: Epoch 199 average loss: 0.065437
12:46:23 - INFO: Epoch 209 average loss: 0.073294
12:46:24 - INFO: Epoch 219 average loss: 0.074205
12:46:25 - INFO: Epoch 229 average loss: 0.076243
12:46:27 - INFO: Epoch 239 average loss: 0.079999
12:46:28 - INFO: Epoch 249 average loss: 0.072089
12:46:29 - INFO: Epoch 259 average loss: 0.060723
12:46:31 - INFO: Epoch 269 average loss: 0.085714
12:46:32 - INFO: Epoch 279 average loss: 0.067185
12:46:34 - INFO: Epoch 289 average loss: 0.058147
12:46:35 - INFO: Starting epoch 299:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0391]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0792]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0673]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0594]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0597]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0183]100%|██████████| 6/6 [00:00<00:00, 54.95it/s, MSE=0.0183]100%|██████████| 6/6 [00:00<00:00, 54.88it/s, MSE=0.0183]
12:46:35 - INFO: Epoch 299 average loss: 0.053852
12:46:37 - INFO: Epoch 309 average loss: 0.065334
12:46:38 - INFO: Epoch 319 average loss: 0.069376
12:46:39 - INFO: Epoch 329 average loss: 0.066387
12:46:41 - INFO: Epoch 339 average loss: 0.081508
12:46:42 - INFO: Epoch 349 average loss: 0.092140
12:46:44 - INFO: Epoch 359 average loss: 0.053800
12:46:45 - INFO: Epoch 369 average loss: 0.077444
12:46:47 - INFO: Epoch 379 average loss: 0.057576
12:46:48 - INFO: Epoch 389 average loss: 0.073604
12:46:49 - INFO: Starting epoch 399:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0459]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.081]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0473]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0746]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.12]   83%|████████▎ | 5/6 [00:00<00:00, 43.38it/s, MSE=0.12] 83%|████████▎ | 5/6 [00:00<00:00, 43.38it/s, MSE=0.0833]100%|██████████| 6/6 [00:00<00:00, 45.68it/s, MSE=0.0833]
12:46:49 - INFO: Epoch 399 average loss: 0.075340
12:46:51 - INFO: Epoch 409 average loss: 0.080129
12:46:52 - INFO: Epoch 419 average loss: 0.063244
12:46:53 - INFO: Epoch 429 average loss: 0.064672
12:46:55 - INFO: Epoch 439 average loss: 0.056928
12:46:56 - INFO: Epoch 449 average loss: 0.058815
12:46:58 - INFO: Epoch 459 average loss: 0.083349
12:46:59 - INFO: Epoch 469 average loss: 0.110295
12:47:00 - INFO: Epoch 479 average loss: 0.043281
12:47:02 - INFO: Epoch 489 average loss: 0.047125
12:47:03 - INFO: Starting epoch 499:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0734]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.041]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0515]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0655]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0328] 83%|████████▎ | 5/6 [00:00<00:00, 42.25it/s, MSE=0.0328] 83%|████████▎ | 5/6 [00:00<00:00, 42.25it/s, MSE=0.0529]100%|██████████| 6/6 [00:00<00:00, 43.68it/s, MSE=0.0529]
12:47:03 - INFO: Epoch 499 average loss: 0.052831
12:47:05 - INFO: Epoch 509 average loss: 0.045128
12:47:06 - INFO: Epoch 519 average loss: 0.052904
12:47:07 - INFO: Epoch 529 average loss: 0.041042
12:47:09 - INFO: Epoch 539 average loss: 0.051421
12:47:10 - INFO: Epoch 549 average loss: 0.073656
12:47:11 - INFO: Epoch 559 average loss: 0.057429
12:47:13 - INFO: Epoch 569 average loss: 0.041579
12:47:14 - INFO: Epoch 579 average loss: 0.057138
12:47:15 - INFO: Epoch 589 average loss: 0.062535
12:47:17 - INFO: Starting epoch 599:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0562]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0474]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0287]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0387]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0351] 83%|████████▎ | 5/6 [00:00<00:00, 44.34it/s, MSE=0.0351] 83%|████████▎ | 5/6 [00:00<00:00, 44.34it/s, MSE=0.041] 100%|██████████| 6/6 [00:00<00:00, 41.75it/s, MSE=0.041]
12:47:17 - INFO: Epoch 599 average loss: 0.041171
12:47:18 - INFO: Epoch 609 average loss: 0.039555
12:47:19 - INFO: Epoch 619 average loss: 0.074741
12:47:21 - INFO: Epoch 629 average loss: 0.065052
12:47:22 - INFO: Epoch 639 average loss: 0.044751
12:47:23 - INFO: Epoch 649 average loss: 0.059970
12:47:25 - INFO: Epoch 659 average loss: 0.069778
12:47:26 - INFO: Epoch 669 average loss: 0.088486
12:47:28 - INFO: Epoch 679 average loss: 0.073681
12:47:29 - INFO: Epoch 689 average loss: 0.048373
12:47:30 - INFO: Starting epoch 699:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0615]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0793]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0425]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0317]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0504]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.111] 100%|██████████| 6/6 [00:00<00:00, 51.77it/s, MSE=0.111]100%|██████████| 6/6 [00:00<00:00, 51.66it/s, MSE=0.111]
12:47:30 - INFO: Epoch 699 average loss: 0.062728
12:47:32 - INFO: Epoch 709 average loss: 0.057754
12:47:33 - INFO: Epoch 719 average loss: 0.053192
12:47:35 - INFO: Epoch 729 average loss: 0.057271
12:47:36 - INFO: Epoch 739 average loss: 0.054805
12:47:37 - INFO: Epoch 749 average loss: 0.103753
12:47:39 - INFO: Epoch 759 average loss: 0.051561
12:47:40 - INFO: Epoch 769 average loss: 0.052932
12:47:41 - INFO: Epoch 779 average loss: 0.042572
12:47:43 - INFO: Epoch 789 average loss: 0.070328
12:47:44 - INFO: Starting epoch 799:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0435]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0547]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0417]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0318] 67%|██████▋   | 4/6 [00:00<00:00, 39.37it/s, MSE=0.0318] 67%|██████▋   | 4/6 [00:00<00:00, 39.37it/s, MSE=0.0306] 67%|██████▋   | 4/6 [00:00<00:00, 39.37it/s, MSE=0.0108]100%|██████████| 6/6 [00:00<00:00, 35.90it/s, MSE=0.0108]
12:47:44 - INFO: Epoch 799 average loss: 0.035500
12:47:46 - INFO: Epoch 809 average loss: 0.044204
12:47:47 - INFO: Epoch 819 average loss: 0.036829
12:47:49 - INFO: Epoch 829 average loss: 0.062532
12:47:50 - INFO: Epoch 839 average loss: 0.055845
12:47:52 - INFO: Epoch 849 average loss: 0.065382
12:47:53 - INFO: Epoch 859 average loss: 0.052044
12:47:54 - INFO: Epoch 869 average loss: 0.068021
12:47:55 - INFO: Epoch 879 average loss: 0.039702
12:47:57 - INFO: Epoch 889 average loss: 0.048916
12:47:58 - INFO: Starting epoch 899:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0623]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0286]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.055]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0608]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0578]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.133] 100%|██████████| 6/6 [00:00<00:00, 51.79it/s, MSE=0.133]100%|██████████| 6/6 [00:00<00:00, 51.72it/s, MSE=0.133]
12:47:58 - INFO: Epoch 899 average loss: 0.066175
12:47:59 - INFO: Epoch 909 average loss: 0.053370
12:48:01 - INFO: Epoch 919 average loss: 0.070955
12:48:02 - INFO: Epoch 929 average loss: 0.047211
12:48:03 - INFO: Epoch 939 average loss: 0.058079
12:48:05 - INFO: Epoch 949 average loss: 0.052788
12:48:06 - INFO: Epoch 959 average loss: 0.070230
12:48:08 - INFO: Epoch 969 average loss: 0.058724
12:48:09 - INFO: Epoch 979 average loss: 0.050446
12:48:10 - INFO: Epoch 989 average loss: 0.048378
12:48:12 - INFO: Starting epoch 999:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0308]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0496]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0806]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0765]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0309] 83%|████████▎ | 5/6 [00:00<00:00, 45.25it/s, MSE=0.0309] 83%|████████▎ | 5/6 [00:00<00:00, 45.25it/s, MSE=0.0375]100%|██████████| 6/6 [00:00<00:00, 45.28it/s, MSE=0.0375]
12:48:12 - INFO: Epoch 999 average loss: 0.050965
12:48:13 - INFO: Total sampling 80 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12:50:44 - INFO: Epoch 9 average loss: 0.161469
12:50:45 - INFO: Epoch 19 average loss: 0.173624
12:50:47 - INFO: Epoch 29 average loss: 0.142812
12:50:48 - INFO: Epoch 39 average loss: 0.121930
12:50:50 - INFO: Epoch 49 average loss: 0.104106
12:50:51 - INFO: Epoch 59 average loss: 0.091238
12:50:53 - INFO: Epoch 69 average loss: 0.095493
12:50:54 - INFO: Epoch 79 average loss: 0.095496
12:50:56 - INFO: Epoch 89 average loss: 0.069818
12:50:57 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... torch.Size([80, 16])
feature shape after [Data Augmentation]:  (162, 16)
(162, 16)
====>MLP training... Epoch: 20 total loss: 0.2976
====>MLP training... Epoch: 40 total loss: 0.0698
====>MLP training... Epoch: 60 total loss: 0.0470
====>MLP training... Epoch: 80 total loss: 0.0499
====>MLP training... Epoch: 100 total loss: 0.0392
====>MLP training... Epoch: 120 total loss: 0.4943
====>MLP training... Epoch: 140 total loss: 0.0401
====>MLP training... Epoch: 160 total loss: 0.0335
====>MLP training... Epoch: 180 total loss: 0.5004
====>MLP training... Epoch: 200 total loss: 0.0322
====>CNN training... Epoch: 0  total loss: 0.2507
====>CNN training... Epoch: 20  total loss: 0.2488
====>CNN training... Epoch: 40  total loss: 0.2468
====>CNN training... Epoch: 60  total loss: 0.2426
====>CNN training... Epoch: 80  total loss: 0.2322
====>CNN training... Epoch: 100  total loss: 0.2039
====>CNN training... Epoch: 120  total loss: 0.1779
====>CNN training... Epoch: 140  total loss: 0.1416
====>CNN training... Epoch: 160  total loss: 0.1341
====>CNN training... Epoch: 180  total loss: 0.1204
====>CNN training... Epoch: 200  total loss: 0.1062
====>RNN training... Epoch: 20 total loss: 0.2408
====>RNN training... Epoch: 40 total loss: 0.2370
====>RNN training... Epoch: 60 total loss: 0.2324
====>RNN training... Epoch: 80 total loss: 0.2272
====>RNN training... Epoch: 100 total loss: 0.2209
====>RNN training... Epoch: 120 total loss: 0.2132
====>RNN training... Epoch: 140 total loss: 0.2039
====>RNN training... Epoch: 160 total loss: 0.1923
====>RNN training... Epoch: 180 total loss: 0.1781
====>RNN training... Epoch: 200 total loss: 0.1611

test: Lang-8-fs_ddpm
fault line:  [1701098, 1701112, 1701134]
feature shape before [Data Augmentation]:  (103, 658)
[0, 1, 2, 3, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 72, 74, 75, 76, 78, 80, 81, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 170, 171, 172, 173, 174, 185, 186, 187, 188, 193, 195, 198, 208, 210, 212, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 279, 280, 281, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 408, 409, 426, 427, 430, 431, 446, 447, 452, 454, 455, 460, 461, 462, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 489, 490, 491, 492, 493, 494, 495, 496, 497, 510, 515, 516, 517, 518, 532, 533, 534, 535, 536, 537, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 559, 560, 561, 562, 565, 567, 568, 569, 571, 572, 573, 574, 575, 576, 578, 579, 580, 582, 583, 584, 585, 586, 587, 588, 590, 609, 627, 635, 636, 637, 638, 639, 640, 641, 642, 643, 645, 646, 647]
entropy 120.35569973056039 22.90292821838616
req_shape:  16
statements selected: 96
statements selected: Index([ 100035,  100041,  100171,  100044,  100070,  100073,  100074,  100076,
        100077,  100079,  100080,  100081,  100082,  100083,  600772,  600774,
        600109,  600110,  600121,  600122,  600125,  600126,  600127,  600129,
        600132,  600135,  600137,  600140,  600144,  600300,  600301,  600331,
        600334,  600450,  600463,  600492,  600493,  600494,  600498,  600499,
        600065, 1000563, 1000580, 1000582, 1100180, 1100208, 1200863, 1200872,
       1200889, 1200890, 1300988, 1500703, 1500721, 1500722, 1700137, 1700138,
       1700175, 1700176, 1700178, 1700179, 1700181, 1700182, 1700220, 1700250,
       1700251, 1700273, 1700274, 1700359, 1700361, 1700453, 1700454, 1800667,
       1800668, 1800669, 1800670, 1800677, 1800679, 1800693, 1900660, 2000707,
       2000708, 2000709, 2000716, 2100518, 2100525, 2100526, 2100529, 2100537,
       2100538, 2200088, 2200091, 2300368, 2300371, 2300088, 2400760, 2601111],
      dtype='int64')
Dataset shape: torch.Size([103, 1, 1, 96]) torch.Size([103])
Batch Dataloader shape: torch.Size([16, 1, 1, 96]) torch.Size([16])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0722]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.146]   0%|          | 0/7 [00:00<?, ?it/s, MSE=0.075]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0652]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0716] 71%|███████▏  | 5/7 [00:00<00:00, 42.84it/s, MSE=0.0716] 71%|███████▏  | 5/7 [00:00<00:00, 42.84it/s, MSE=0.0896] 71%|███████▏  | 5/7 [00:00<00:00, 42.84it/s, MSE=0.0858]100%|██████████| 7/7 [00:00<00:00, 43.59it/s, MSE=0.0858]
12:50:57 - INFO: Epoch 99 average loss: 0.086525
12:50:59 - INFO: Epoch 109 average loss: 0.073898
12:51:00 - INFO: Epoch 119 average loss: 0.104396
12:51:02 - INFO: Epoch 129 average loss: 0.069318
12:51:04 - INFO: Epoch 139 average loss: 0.075728
12:51:05 - INFO: Epoch 149 average loss: 0.065085
12:51:07 - INFO: Epoch 159 average loss: 0.074801
12:51:08 - INFO: Epoch 169 average loss: 0.060929
12:51:10 - INFO: Epoch 179 average loss: 0.066560
12:51:11 - INFO: Epoch 189 average loss: 0.057131
12:51:13 - INFO: Starting epoch 199:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.117]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0598]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0606]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0314]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0529]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0718] 86%|████████▌ | 6/7 [00:00<00:00, 59.89it/s, MSE=0.0718] 86%|████████▌ | 6/7 [00:00<00:00, 59.89it/s, MSE=0.0454]100%|██████████| 7/7 [00:00<00:00, 60.04it/s, MSE=0.0454]
12:51:13 - INFO: Epoch 199 average loss: 0.062645
12:51:14 - INFO: Epoch 209 average loss: 0.068670
12:51:16 - INFO: Epoch 219 average loss: 0.064592
12:51:18 - INFO: Epoch 229 average loss: 0.069203
12:51:19 - INFO: Epoch 239 average loss: 0.058586
12:51:21 - INFO: Epoch 249 average loss: 0.065165
12:51:22 - INFO: Epoch 259 average loss: 0.077969
12:51:24 - INFO: Epoch 269 average loss: 0.060418
12:51:25 - INFO: Epoch 279 average loss: 0.071513
12:51:27 - INFO: Epoch 289 average loss: 0.070910
12:51:28 - INFO: Starting epoch 299:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0303]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.071]   0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0841]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0569]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0944] 71%|███████▏  | 5/7 [00:00<00:00, 47.42it/s, MSE=0.0944] 71%|███████▏  | 5/7 [00:00<00:00, 47.42it/s, MSE=0.066]  71%|███████▏  | 5/7 [00:00<00:00, 47.42it/s, MSE=0.041]100%|██████████| 7/7 [00:00<00:00, 49.89it/s, MSE=0.041]
12:51:28 - INFO: Epoch 299 average loss: 0.063379
12:51:30 - INFO: Epoch 309 average loss: 0.065549
12:51:31 - INFO: Epoch 319 average loss: 0.068661
12:51:33 - INFO: Epoch 329 average loss: 0.051385
12:51:35 - INFO: Epoch 339 average loss: 0.055998
12:51:36 - INFO: Epoch 349 average loss: 0.059445
12:51:38 - INFO: Epoch 359 average loss: 0.065485
12:51:39 - INFO: Epoch 369 average loss: 0.059256
12:51:41 - INFO: Epoch 379 average loss: 0.053088
12:51:42 - INFO: Epoch 389 average loss: 0.060694
12:51:44 - INFO: Starting epoch 399:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0529]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.101]   0%|          | 0/7 [00:00<?, ?it/s, MSE=0.056]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0378] 57%|█████▋    | 4/7 [00:00<00:00, 39.76it/s, MSE=0.0378] 57%|█████▋    | 4/7 [00:00<00:00, 39.76it/s, MSE=0.0749] 57%|█████▋    | 4/7 [00:00<00:00, 39.76it/s, MSE=0.0527] 57%|█████▋    | 4/7 [00:00<00:00, 39.76it/s, MSE=0.0128]100%|██████████| 7/7 [00:00<00:00, 42.51it/s, MSE=0.0128]
12:51:44 - INFO: Epoch 399 average loss: 0.055441
12:51:46 - INFO: Epoch 409 average loss: 0.067095
12:51:47 - INFO: Epoch 419 average loss: 0.066293
12:51:49 - INFO: Epoch 429 average loss: 0.058637
12:51:50 - INFO: Epoch 439 average loss: 0.071817
12:51:52 - INFO: Epoch 449 average loss: 0.069509
12:51:53 - INFO: Epoch 459 average loss: 0.075622
12:51:55 - INFO: Epoch 469 average loss: 0.064757
12:51:57 - INFO: Epoch 479 average loss: 0.066803
12:51:58 - INFO: Epoch 489 average loss: 0.060584
12:51:59 - INFO: Starting epoch 499:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0742]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0422]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0874]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0724]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0495] 71%|███████▏  | 5/7 [00:00<00:00, 46.91it/s, MSE=0.0495] 71%|███████▏  | 5/7 [00:00<00:00, 46.91it/s, MSE=0.0373] 71%|███████▏  | 5/7 [00:00<00:00, 46.91it/s, MSE=0.0664]100%|██████████| 7/7 [00:00<00:00, 48.92it/s, MSE=0.0664]
12:51:59 - INFO: Epoch 499 average loss: 0.061362
12:52:01 - INFO: Epoch 509 average loss: 0.062332
12:52:02 - INFO: Epoch 519 average loss: 0.069890
12:52:04 - INFO: Epoch 529 average loss: 0.049088
12:52:06 - INFO: Epoch 539 average loss: 0.089758
12:52:07 - INFO: Epoch 549 average loss: 0.073710
12:52:09 - INFO: Epoch 559 average loss: 0.064714
12:52:10 - INFO: Epoch 569 average loss: 0.064173
12:52:11 - INFO: Epoch 579 average loss: 0.078699
12:52:13 - INFO: Epoch 589 average loss: 0.074280
12:52:15 - INFO: Starting epoch 599:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0423]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.025]   0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0878]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0748] 57%|█████▋    | 4/7 [00:00<00:00, 35.80it/s, MSE=0.0748] 57%|█████▋    | 4/7 [00:00<00:00, 35.80it/s, MSE=0.0602] 57%|█████▋    | 4/7 [00:00<00:00, 35.80it/s, MSE=0.0642] 57%|█████▋    | 4/7 [00:00<00:00, 35.80it/s, MSE=0.0462]100%|██████████| 7/7 [00:00<00:00, 33.93it/s, MSE=0.0462]
12:52:15 - INFO: Epoch 599 average loss: 0.057190
12:52:16 - INFO: Epoch 609 average loss: 0.068852
12:52:18 - INFO: Epoch 619 average loss: 0.045887
12:52:19 - INFO: Epoch 629 average loss: 0.048671
12:52:21 - INFO: Epoch 639 average loss: 0.063752
12:52:22 - INFO: Epoch 649 average loss: 0.062124
12:52:24 - INFO: Epoch 659 average loss: 0.061880
12:52:26 - INFO: Epoch 669 average loss: 0.046233
12:52:27 - INFO: Epoch 679 average loss: 0.047430
12:52:29 - INFO: Epoch 689 average loss: 0.055891
12:52:30 - INFO: Starting epoch 699:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0582]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0661]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0498]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0938]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0912]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.042]  86%|████████▌ | 6/7 [00:00<00:00, 56.68it/s, MSE=0.042] 86%|████████▌ | 6/7 [00:00<00:00, 56.68it/s, MSE=0.0714]100%|██████████| 7/7 [00:00<00:00, 57.59it/s, MSE=0.0714]
12:52:30 - INFO: Epoch 699 average loss: 0.067471
12:52:32 - INFO: Epoch 709 average loss: 0.044188
12:52:33 - INFO: Epoch 719 average loss: 0.050082
12:52:34 - INFO: Epoch 729 average loss: 0.061620
12:52:36 - INFO: Epoch 739 average loss: 0.067313
12:52:37 - INFO: Epoch 749 average loss: 0.058508
12:52:38 - INFO: Epoch 759 average loss: 0.064276
12:52:40 - INFO: Epoch 769 average loss: 0.064003
12:52:42 - INFO: Epoch 779 average loss: 0.055841
12:52:43 - INFO: Epoch 789 average loss: 0.059578
12:52:45 - INFO: Starting epoch 799:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0438]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0705]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0289]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0389]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0363] 71%|███████▏  | 5/7 [00:00<00:00, 40.98it/s, MSE=0.0363] 71%|███████▏  | 5/7 [00:00<00:00, 40.98it/s, MSE=0.0527] 71%|███████▏  | 5/7 [00:00<00:00, 40.98it/s, MSE=0.0241]100%|██████████| 7/7 [00:00<00:00, 43.24it/s, MSE=0.0241]
12:52:45 - INFO: Epoch 799 average loss: 0.042186
12:52:46 - INFO: Epoch 809 average loss: 0.050441
12:52:48 - INFO: Epoch 819 average loss: 0.051151
12:52:49 - INFO: Epoch 829 average loss: 0.059297
12:52:51 - INFO: Epoch 839 average loss: 0.059947
12:52:52 - INFO: Epoch 849 average loss: 0.064979
12:52:54 - INFO: Epoch 859 average loss: 0.062241
12:52:55 - INFO: Epoch 869 average loss: 0.057661
12:52:56 - INFO: Epoch 879 average loss: 0.055258
12:52:58 - INFO: Epoch 889 average loss: 0.059961
12:52:59 - INFO: Starting epoch 899:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0292]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0538]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0189]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0553]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0501] 71%|███████▏  | 5/7 [00:00<00:00, 43.25it/s, MSE=0.0501] 71%|███████▏  | 5/7 [00:00<00:00, 43.25it/s, MSE=0.0398] 71%|███████▏  | 5/7 [00:00<00:00, 43.25it/s, MSE=0.0267]100%|██████████| 7/7 [00:00<00:00, 45.43it/s, MSE=0.0267]
12:53:00 - INFO: Epoch 899 average loss: 0.039120
12:53:01 - INFO: Epoch 909 average loss: 0.062874
12:53:03 - INFO: Epoch 919 average loss: 0.049275
12:53:05 - INFO: Epoch 929 average loss: 0.049600
12:53:06 - INFO: Epoch 939 average loss: 0.053758
12:53:08 - INFO: Epoch 949 average loss: 0.051962
12:53:09 - INFO: Epoch 959 average loss: 0.040565
12:53:11 - INFO: Epoch 969 average loss: 0.042919
12:53:12 - INFO: Epoch 979 average loss: 0.048240
12:53:14 - INFO: Epoch 989 average loss: 0.059117
12:53:15 - INFO: Starting epoch 999:
  0%|          | 0/7 [00:00<?, ?it/s]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.044]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0686]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0517]  0%|          | 0/7 [00:00<?, ?it/s, MSE=0.0587] 57%|█████▋    | 4/7 [00:00<00:00, 37.62it/s, MSE=0.0587] 57%|█████▋    | 4/7 [00:00<00:00, 37.62it/s, MSE=0.0485] 57%|█████▋    | 4/7 [00:00<00:00, 37.62it/s, MSE=0.0397] 57%|█████▋    | 4/7 [00:00<00:00, 37.62it/s, MSE=0.0465]100%|██████████| 7/7 [00:00<00:00, 38.12it/s, MSE=0.0465]
12:53:16 - INFO: Epoch 999 average loss: 0.051118
12:53:16 - INFO: Total sampling 99 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12:56:10 - INFO: Epoch 9 average loss: 0.153404
12:56:12 - INFO: Epoch 19 average loss: 0.144098
12:56:14 - INFO: Epoch 29 average loss: 0.159665
12:56:16 - INFO: Epoch 39 average loss: 0.105629
12:56:18 - INFO: Epoch 49 average loss: 0.106080
12:56:19 - INFO: Epoch 59 average loss: 0.106938
12:56:21 - INFO: Epoch 69 average loss: 0.108918
12:56:23 - INFO: Epoch 79 average loss: 0.108792
12:56:25 - INFO: Epoch 89 average loss: 0.086520
12:56:27 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... Now sampling 85st new convMatrix.... Now sampling 86st new convMatrix.... Now sampling 87st new convMatrix.... Now sampling 88st new convMatrix.... Now sampling 89st new convMatrix.... Now sampling 90st new convMatrix.... Now sampling 91st new convMatrix.... Now sampling 92st new convMatrix.... Now sampling 93st new convMatrix.... Now sampling 94st new convMatrix.... Now sampling 95st new convMatrix.... Now sampling 96st new convMatrix.... Now sampling 97st new convMatrix.... Now sampling 98st new convMatrix.... torch.Size([99, 96])
feature shape after [Data Augmentation]:  (202, 96)
(202, 96)
====>MLP training... Epoch: 20 total loss: 1.3495
====>MLP training... Epoch: 40 total loss: 0.1207
====>MLP training... Epoch: 60 total loss: 0.0546
====>MLP training... Epoch: 80 total loss: 0.0174
====>MLP training... Epoch: 100 total loss: 0.0063
====>MLP training... Epoch: 120 total loss: 0.0038
====>MLP training... Epoch: 140 total loss: 0.0048
====>MLP training... Epoch: 160 total loss: 0.0021
====>MLP training... Epoch: 180 total loss: 0.0010
====>MLP training... Epoch: 200 total loss: 0.0025
====>CNN training... Epoch: 0  total loss: 0.2514
====>CNN training... Epoch: 20  total loss: 0.2469
====>CNN training... Epoch: 40  total loss: 0.2348
====>CNN training... Epoch: 60  total loss: 0.1993
====>CNN training... Epoch: 80  total loss: 0.1516
====>CNN training... Epoch: 100  total loss: 0.1249
====>CNN training... Epoch: 120  total loss: 0.1047
====>CNN training... Epoch: 140  total loss: 0.0955
====>CNN training... Epoch: 160  total loss: 0.0886
====>CNN training... Epoch: 180  total loss: 0.0817
====>CNN training... Epoch: 200  total loss: 0.0784
====>RNN training... Epoch: 20 total loss: 0.2397
====>RNN training... Epoch: 40 total loss: 0.2282
====>RNN training... Epoch: 60 total loss: 0.2151
====>RNN training... Epoch: 80 total loss: 0.1998
====>RNN training... Epoch: 100 total loss: 0.1812
====>RNN training... Epoch: 120 total loss: 0.1586
====>RNN training... Epoch: 140 total loss: 0.1322
====>RNN training... Epoch: 160 total loss: 0.1038
====>RNN training... Epoch: 180 total loss: 0.0772
====>RNN training... Epoch: 200 total loss: 0.0557

test: Lang-9-fs_ddpm
fault line:  [600144]
feature shape before [Data Augmentation]:  (115, 694)
[0, 1, 2, 3, 512, 5, 516, 7, 517, 9, 518, 11, 12, 13, 14, 15, 16, 520, 18, 524, 525, 526, 527, 528, 529, 530, 531, 532, 617, 618, 612, 620, 519, 621, 622, 521, 623, 624, 72, 625, 589, 78, 79, 80, 611, 82, 595, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 631, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 651, 147, 665, 667, 668, 669, 158, 159, 670, 163, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 683, 185, 186, 237, 239, 240, 241, 242, 254, 268, 269, 270, 271, 272, 295, 296, 297, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 626, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 597, 598, 599, 600, 460, 461, 610, 496, 497, 498, 499, 500, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]
entropy 80.48933322022872 17.07849886442332
req_shape:  16
statements selected: 32
statements selected: Index([ 100035,  100041,  100171,  100044,  100070,  100073,  100076,  100079,
        100080,  600120,  600123,  600124,  600128,  600133,  600134,  600136,
        600147,  600148,  600150,  600282,  600283,  600305,  600306,  600485,
        600535,  600068, 1100179, 1100180, 1100199, 1700277, 1700279, 2300603],
      dtype='int64')
Dataset shape: torch.Size([115, 1, 1, 32]) torch.Size([115])
Batch Dataloader shape: torch.Size([16, 1, 1, 32]) torch.Size([16])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0852]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.118]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.109]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0833]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0354] 62%|██████▎   | 5/8 [00:00<00:00, 46.65it/s, MSE=0.0354] 62%|██████▎   | 5/8 [00:00<00:00, 46.65it/s, MSE=0.0534] 62%|██████▎   | 5/8 [00:00<00:00, 46.65it/s, MSE=0.0975] 62%|██████▎   | 5/8 [00:00<00:00, 46.65it/s, MSE=0.0322]100%|██████████| 8/8 [00:00<00:00, 45.68it/s, MSE=0.0322]
12:56:27 - INFO: Epoch 99 average loss: 0.076745
12:56:29 - INFO: Epoch 109 average loss: 0.080353
12:56:31 - INFO: Epoch 119 average loss: 0.092981
12:56:32 - INFO: Epoch 129 average loss: 0.067934
12:56:34 - INFO: Epoch 139 average loss: 0.071912
12:56:36 - INFO: Epoch 149 average loss: 0.083340
12:56:38 - INFO: Epoch 159 average loss: 0.079472
12:56:39 - INFO: Epoch 169 average loss: 0.074459
12:56:41 - INFO: Epoch 179 average loss: 0.060196
12:56:43 - INFO: Epoch 189 average loss: 0.077209
12:56:44 - INFO: Starting epoch 199:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.131]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0803]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0736]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0468]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0542] 62%|██████▎   | 5/8 [00:00<00:00, 47.16it/s, MSE=0.0542] 62%|██████▎   | 5/8 [00:00<00:00, 47.16it/s, MSE=0.0385] 62%|██████▎   | 5/8 [00:00<00:00, 47.16it/s, MSE=0.0529] 62%|██████▎   | 5/8 [00:00<00:00, 47.16it/s, MSE=0.0916]100%|██████████| 8/8 [00:00<00:00, 42.83it/s, MSE=0.0916]
12:56:44 - INFO: Epoch 199 average loss: 0.071100
12:56:46 - INFO: Epoch 209 average loss: 0.082593
12:56:48 - INFO: Epoch 219 average loss: 0.071962
12:56:50 - INFO: Epoch 229 average loss: 0.079805
12:56:52 - INFO: Epoch 239 average loss: 0.060160
12:56:54 - INFO: Epoch 249 average loss: 0.061220
12:56:56 - INFO: Epoch 259 average loss: 0.070663
12:56:58 - INFO: Epoch 269 average loss: 0.061198
12:57:00 - INFO: Epoch 279 average loss: 0.056910
12:57:01 - INFO: Epoch 289 average loss: 0.077752
12:57:03 - INFO: Starting epoch 299:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0517]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0551]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.071]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.045]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0734]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0463] 75%|███████▌  | 6/8 [00:00<00:00, 57.41it/s, MSE=0.0463] 75%|███████▌  | 6/8 [00:00<00:00, 57.41it/s, MSE=0.0768] 75%|███████▌  | 6/8 [00:00<00:00, 57.41it/s, MSE=0.0508]100%|██████████| 8/8 [00:00<00:00, 53.51it/s, MSE=0.0508]
12:57:03 - INFO: Epoch 299 average loss: 0.058767
12:57:05 - INFO: Epoch 309 average loss: 0.052987
12:57:07 - INFO: Epoch 319 average loss: 0.071853
12:57:08 - INFO: Epoch 329 average loss: 0.056623
12:57:10 - INFO: Epoch 339 average loss: 0.077773
12:57:12 - INFO: Epoch 349 average loss: 0.054726
12:57:14 - INFO: Epoch 359 average loss: 0.062157
12:57:16 - INFO: Epoch 369 average loss: 0.066301
12:57:17 - INFO: Epoch 379 average loss: 0.062468
12:57:19 - INFO: Epoch 389 average loss: 0.079459
12:57:21 - INFO: Starting epoch 399:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0528]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0602]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0442]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0678]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0429] 62%|██████▎   | 5/8 [00:00<00:00, 39.85it/s, MSE=0.0429] 62%|██████▎   | 5/8 [00:00<00:00, 39.85it/s, MSE=0.0635] 62%|██████▎   | 5/8 [00:00<00:00, 39.85it/s, MSE=0.0606] 62%|██████▎   | 5/8 [00:00<00:00, 39.85it/s, MSE=0.242] 100%|██████████| 8/8 [00:00<00:00, 41.57it/s, MSE=0.242]
12:57:21 - INFO: Epoch 399 average loss: 0.079215
12:57:23 - INFO: Epoch 409 average loss: 0.048571
12:57:25 - INFO: Epoch 419 average loss: 0.053889
12:57:27 - INFO: Epoch 429 average loss: 0.063489
12:57:28 - INFO: Epoch 439 average loss: 0.070173
12:57:30 - INFO: Epoch 449 average loss: 0.060706
12:57:32 - INFO: Epoch 459 average loss: 0.063072
12:57:33 - INFO: Epoch 469 average loss: 0.067511
12:57:35 - INFO: Epoch 479 average loss: 0.051967
12:57:36 - INFO: Epoch 489 average loss: 0.050218
12:57:38 - INFO: Starting epoch 499:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0457]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0361]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0866]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.045]  50%|█████     | 4/8 [00:00<00:00, 39.50it/s, MSE=0.045] 50%|█████     | 4/8 [00:00<00:00, 39.50it/s, MSE=0.0469] 50%|█████     | 4/8 [00:00<00:00, 39.50it/s, MSE=0.0593] 50%|█████     | 4/8 [00:00<00:00, 39.50it/s, MSE=0.0411] 50%|█████     | 4/8 [00:00<00:00, 39.50it/s, MSE=0.0363]100%|██████████| 8/8 [00:00<00:00, 40.10it/s, MSE=0.0363]
12:57:38 - INFO: Epoch 499 average loss: 0.049621
12:57:40 - INFO: Epoch 509 average loss: 0.057920
12:57:42 - INFO: Epoch 519 average loss: 0.074636
12:57:43 - INFO: Epoch 529 average loss: 0.061631
12:57:45 - INFO: Epoch 539 average loss: 0.053406
12:57:47 - INFO: Epoch 549 average loss: 0.064474
12:57:49 - INFO: Epoch 559 average loss: 0.050049
12:57:51 - INFO: Epoch 569 average loss: 0.064747
12:57:53 - INFO: Epoch 579 average loss: 0.045177
12:57:54 - INFO: Epoch 589 average loss: 0.044840
12:57:56 - INFO: Starting epoch 599:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0511]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0764]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0817]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.1]     0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0609] 62%|██████▎   | 5/8 [00:00<00:00, 46.08it/s, MSE=0.0609] 62%|██████▎   | 5/8 [00:00<00:00, 46.08it/s, MSE=0.0427] 62%|██████▎   | 5/8 [00:00<00:00, 46.08it/s, MSE=0.0796] 62%|██████▎   | 5/8 [00:00<00:00, 46.08it/s, MSE=0.0328]100%|██████████| 8/8 [00:00<00:00, 43.63it/s, MSE=0.0328]
12:57:56 - INFO: Epoch 599 average loss: 0.065659
12:57:58 - INFO: Epoch 609 average loss: 0.047543
12:58:00 - INFO: Epoch 619 average loss: 0.051285
12:58:02 - INFO: Epoch 629 average loss: 0.051981
12:58:03 - INFO: Epoch 639 average loss: 0.050694
12:58:05 - INFO: Epoch 649 average loss: 0.079911
12:58:07 - INFO: Epoch 659 average loss: 0.045574
12:58:08 - INFO: Epoch 669 average loss: 0.068655
12:58:10 - INFO: Epoch 679 average loss: 0.041889
12:58:12 - INFO: Epoch 689 average loss: 0.045678
12:58:14 - INFO: Starting epoch 699:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0305]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0515]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0448]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.059]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0334]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.058]  75%|███████▌  | 6/8 [00:00<00:00, 58.60it/s, MSE=0.058] 75%|███████▌  | 6/8 [00:00<00:00, 58.60it/s, MSE=0.0434] 75%|███████▌  | 6/8 [00:00<00:00, 58.60it/s, MSE=0.0378]100%|██████████| 8/8 [00:00<00:00, 53.25it/s, MSE=0.0378]
12:58:14 - INFO: Epoch 699 average loss: 0.044801
12:58:16 - INFO: Epoch 709 average loss: 0.049209
12:58:18 - INFO: Epoch 719 average loss: 0.050685
12:58:20 - INFO: Epoch 729 average loss: 0.036769
12:58:22 - INFO: Epoch 739 average loss: 0.040586
12:58:23 - INFO: Epoch 749 average loss: 0.074073
12:58:25 - INFO: Epoch 759 average loss: 0.057214
12:58:27 - INFO: Epoch 769 average loss: 0.036053
12:58:29 - INFO: Epoch 779 average loss: 0.046709
12:58:30 - INFO: Epoch 789 average loss: 0.036649
12:58:32 - INFO: Starting epoch 799:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.04]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0394]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0537]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.037]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0506]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0941] 75%|███████▌  | 6/8 [00:00<00:00, 55.16it/s, MSE=0.0941] 75%|███████▌  | 6/8 [00:00<00:00, 55.16it/s, MSE=0.0446] 75%|███████▌  | 6/8 [00:00<00:00, 55.16it/s, MSE=0.032] 100%|██████████| 8/8 [00:00<00:00, 56.24it/s, MSE=0.032]
12:58:32 - INFO: Epoch 799 average loss: 0.048948
12:58:34 - INFO: Epoch 809 average loss: 0.054003
12:58:36 - INFO: Epoch 819 average loss: 0.038104
12:58:38 - INFO: Epoch 829 average loss: 0.052165
12:58:40 - INFO: Epoch 839 average loss: 0.042692
12:58:41 - INFO: Epoch 849 average loss: 0.039417
12:58:43 - INFO: Epoch 859 average loss: 0.084458
12:58:45 - INFO: Epoch 869 average loss: 0.042900
12:58:47 - INFO: Epoch 879 average loss: 0.046473
12:58:48 - INFO: Epoch 889 average loss: 0.041454
12:58:50 - INFO: Starting epoch 899:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0488]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0434]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0397]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0439]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0594] 62%|██████▎   | 5/8 [00:00<00:00, 42.18it/s, MSE=0.0594] 62%|██████▎   | 5/8 [00:00<00:00, 42.18it/s, MSE=0.0321] 62%|██████▎   | 5/8 [00:00<00:00, 42.18it/s, MSE=0.0376] 62%|██████▎   | 5/8 [00:00<00:00, 42.18it/s, MSE=0.0343]100%|██████████| 8/8 [00:00<00:00, 47.39it/s, MSE=0.0343]
12:58:50 - INFO: Epoch 899 average loss: 0.042379
12:58:52 - INFO: Epoch 909 average loss: 0.058172
12:58:54 - INFO: Epoch 919 average loss: 0.043664
12:58:55 - INFO: Epoch 929 average loss: 0.082518
12:58:57 - INFO: Epoch 939 average loss: 0.036005
12:58:59 - INFO: Epoch 949 average loss: 0.053030
12:59:00 - INFO: Epoch 959 average loss: 0.041004
12:59:02 - INFO: Epoch 969 average loss: 0.037126
12:59:03 - INFO: Epoch 979 average loss: 0.044261
12:59:05 - INFO: Epoch 989 average loss: 0.042504
12:59:06 - INFO: Starting epoch 999:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0505]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0413]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0463]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0529]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.057]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0324] 75%|███████▌  | 6/8 [00:00<00:00, 55.90it/s, MSE=0.0324] 75%|███████▌  | 6/8 [00:00<00:00, 55.90it/s, MSE=0.0228] 75%|███████▌  | 6/8 [00:00<00:00, 55.90it/s, MSE=0.0378]100%|██████████| 8/8 [00:00<00:00, 50.41it/s, MSE=0.0378]
12:59:07 - INFO: Epoch 999 average loss: 0.042625
12:59:07 - INFO: Total sampling 111 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... Now sampling 85st new convMatrix.... Now sampling 86st new convMatrix.... Now sampling 87st new convMatrix.... Now sampling 88st new convMatrix.... Now sampling 89st new convMatrix.... Now sampling 90st new convMatrix.... Now sampling 91st new convMatrix.... Now sampling 92st new convMatrix.... Now sampling 93st new convMatrix.... Now sampling 94st new convMatrix.... Now sampling 95st new convMatrix.... Now sampling 96st new convMatrix.... Now sampling 97st new convMatrix.... Now sampling 98st new convMatrix.... Now sampling 99st new convMatrix.... Now sampling 100st new convMatrix.... Now sampling 101st new convMatrix.... Now sampling 102st new convMatrix.... Now sampling 103st new convMatrix.... Now sampling 104st new convMatrix.... Now sampling 105st new convMatrix.... Now sampling 106st new convMatrix.... Now sampling 107st new convMatrix.... Now sampling 108st new convMatrix.... Now sampling 109st new convMatrix.... Now sampling 110st new convMatrix.... torch.Size([111, 32])
feature shape after [Data Augmentation]:  (226, 32)
(226, 32)
====>MLP training... Epoch: 20 total loss: 1.9476
====>MLP training... Epoch: 40 total loss: 1.3114
====>MLP training... Epoch: 60 total loss: 0.6876
====>MLP training... Epoch: 80 total loss: 0.3377
====>MLP training... Epoch: 100 total loss: 0.1099
====>MLP training... Epoch: 120 total loss: 0.0817
====>MLP training... Epoch: 140 total loss: 0.0875
====>MLP training... Epoch: 160 total loss: 0.0684
====>MLP training... Epoch: 180 total loss: 0.0673
====>MLP training... Epoch: 200 total loss: 0.0667
====>CNN training... Epoch: 0  total loss: 0.2530
====>CNN training... Epoch: 20  total loss: 0.2493
====>CNN training... Epoch: 40  total loss: 0.2384
====>CNN training... Epoch: 60  total loss: 0.2053
====>CNN training... Epoch: 80  total loss: 0.1522
====>CNN training... Epoch: 100  total loss: 0.1205
====>CNN training... Epoch: 120  total loss: 0.1085
====>CNN training... Epoch: 140  total loss: 0.0970
====>CNN training... Epoch: 160  total loss: 0.0875
====>CNN training... Epoch: 180  total loss: 0.0822
====>CNN training... Epoch: 200  total loss: 0.0689
====>RNN training... Epoch: 20 total loss: 0.2581
====>RNN training... Epoch: 40 total loss: 0.2546
====>RNN training... Epoch: 60 total loss: 0.2523
====>RNN training... Epoch: 80 total loss: 0.2507
====>RNN training... Epoch: 100 total loss: 0.2493
====>RNN training... Epoch: 120 total loss: 0.2481
====>RNN training... Epoch: 140 total loss: 0.2469
====>RNN training... Epoch: 160 total loss: 0.2457
====>RNN training... Epoch: 180 total loss: 0.2446
====>RNN training... Epoch: 200 total loss: 0.2434

test: Lang-10-fs_ddpm
fault line:  [300304, 300307, 300308, 300309, 300310, 300311, 300312, 300313, 300314]
feature shape before [Data Augmentation]:  (113, 700)
[0, 512, 2, 3, 4, 513, 6, 514, 8, 9, 10, 11, 515, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 517, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 584, 585, 524, 82, 83, 525, 87, 526, 89, 90, 91, 527, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 115, 116, 117, 118, 119, 120, 121, 533, 638, 127, 128, 534, 639, 640, 535, 641, 134, 135, 644, 137, 536, 537, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 666, 667, 668, 669, 167, 672, 169, 673, 674, 172, 676, 174, 678, 679, 680, 681, 179, 683, 684, 685, 686, 516, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 538, 228, 229, 230, 231, 233, 235, 518, 237, 239, 240, 241, 242, 243, 244, 246, 519, 276, 277, 278, 279, 280, 281, 520, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 670, 307, 308, 677, 360, 682, 376, 377, 378, 379, 380, 385, 405, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 452, 453, 460, 462, 464, 466, 467, 480, 481, 528, 504, 505, 506, 510, 511]
entropy 113.08405839535763 29.136324528890977
req_shape:  16
statements selected: 48
statements selected: Index([ 100860,  200710,  200711,  200712,  200727,  200731,  200733,  300290,
        300307,  300314,  300315,  300340,  300447,  300494,  300565,  400359,
        700171,  700070,  700073,  700082,  700090, 1100674, 1100676, 1600088,
       1700088, 1700091, 1800142, 1800148, 1800149, 1800151, 1800152, 1800153,
       1800156, 1800157, 1800176, 1800182, 1800187, 1800192, 1800194, 1800208,
       1800210, 1800309, 1800311, 1800340, 1800346, 1900850, 2400601, 2800633],
      dtype='int64')
Dataset shape: torch.Size([113, 1, 1, 48]) torch.Size([113])
Batch Dataloader shape: torch.Size([16, 1, 1, 48]) torch.Size([16])

start train
model save to:  /home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
01:02:30 - INFO: Epoch 9 average loss: 0.153100
01:02:32 - INFO: Epoch 19 average loss: 0.134427
01:02:33 - INFO: Epoch 29 average loss: 0.119828
01:02:35 - INFO: Epoch 39 average loss: 0.114694
01:02:37 - INFO: Epoch 49 average loss: 0.100704
01:02:39 - INFO: Epoch 59 average loss: 0.087576
01:02:40 - INFO: Epoch 69 average loss: 0.106444
01:02:42 - INFO: Epoch 79 average loss: 0.100054
01:02:44 - INFO: Epoch 89 average loss: 0.084564
01:02:46 - INFO: Starting epoch 99:
models/DDPM_conditional/ckpt.pt
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0645]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0539]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0715]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.105]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0968] 62%|██████▎   | 5/8 [00:00<00:00, 45.07it/s, MSE=0.0968] 62%|██████▎   | 5/8 [00:00<00:00, 45.07it/s, MSE=0.0537] 62%|██████▎   | 5/8 [00:00<00:00, 45.07it/s, MSE=0.109]  62%|██████▎   | 5/8 [00:00<00:00, 45.07it/s, MSE=0.155]100%|██████████| 8/8 [00:00<00:00, 46.97it/s, MSE=0.155]
01:02:46 - INFO: Epoch 99 average loss: 0.088657
01:02:48 - INFO: Epoch 109 average loss: 0.089985
01:02:50 - INFO: Epoch 119 average loss: 0.086152
01:02:52 - INFO: Epoch 129 average loss: 0.080221
01:02:53 - INFO: Epoch 139 average loss: 0.113657
01:02:55 - INFO: Epoch 149 average loss: 0.087959
01:02:57 - INFO: Epoch 159 average loss: 0.051274
01:02:59 - INFO: Epoch 169 average loss: 0.077526
01:03:01 - INFO: Epoch 179 average loss: 0.071749
01:03:03 - INFO: Epoch 189 average loss: 0.100358
01:03:04 - INFO: Starting epoch 199:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0501]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0787]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0426]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.148]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0962]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.129]  75%|███████▌  | 6/8 [00:00<00:00, 52.50it/s, MSE=0.129] 75%|███████▌  | 6/8 [00:00<00:00, 52.50it/s, MSE=0.0695] 75%|███████▌  | 6/8 [00:00<00:00, 52.50it/s, MSE=0.00817]100%|██████████| 8/8 [00:00<00:00, 47.47it/s, MSE=0.00817]
01:03:04 - INFO: Epoch 199 average loss: 0.077800
01:03:06 - INFO: Epoch 209 average loss: 0.052460
01:03:08 - INFO: Epoch 219 average loss: 0.101674
01:03:10 - INFO: Epoch 229 average loss: 0.052758
01:03:12 - INFO: Epoch 239 average loss: 0.068908
01:03:14 - INFO: Epoch 249 average loss: 0.058105
01:03:16 - INFO: Epoch 259 average loss: 0.078604
01:03:18 - INFO: Epoch 269 average loss: 0.079823
01:03:20 - INFO: Epoch 279 average loss: 0.071926
01:03:22 - INFO: Epoch 289 average loss: 0.113485
01:03:23 - INFO: Starting epoch 299:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.148]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.146]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0685]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.146]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0737] 62%|██████▎   | 5/8 [00:00<00:00, 43.41it/s, MSE=0.0737] 62%|██████▎   | 5/8 [00:00<00:00, 43.41it/s, MSE=0.107]  62%|██████▎   | 5/8 [00:00<00:00, 43.41it/s, MSE=0.0535] 62%|██████▎   | 5/8 [00:00<00:00, 43.41it/s, MSE=0.132] 100%|██████████| 8/8 [00:00<00:00, 44.96it/s, MSE=0.132]
01:03:23 - INFO: Epoch 299 average loss: 0.109308
01:03:26 - INFO: Epoch 309 average loss: 0.057469
01:03:27 - INFO: Epoch 319 average loss: 0.071270
01:03:29 - INFO: Epoch 329 average loss: 0.069969
01:03:31 - INFO: Epoch 339 average loss: 0.128746
01:03:33 - INFO: Epoch 349 average loss: 0.076158
01:03:34 - INFO: Epoch 359 average loss: 0.063368
01:03:36 - INFO: Epoch 369 average loss: 0.057626
01:03:38 - INFO: Epoch 379 average loss: 0.090104
01:03:40 - INFO: Epoch 389 average loss: 0.063372
01:03:41 - INFO: Starting epoch 399:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0335]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0945]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0783]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0797]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0942]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.115]  75%|███████▌  | 6/8 [00:00<00:00, 58.66it/s, MSE=0.115] 75%|███████▌  | 6/8 [00:00<00:00, 58.66it/s, MSE=0.0348] 75%|███████▌  | 6/8 [00:00<00:00, 58.66it/s, MSE=0.0498]100%|██████████| 8/8 [00:00<00:00, 53.56it/s, MSE=0.0498]
01:03:41 - INFO: Epoch 399 average loss: 0.072486
01:03:43 - INFO: Epoch 409 average loss: 0.051619
01:03:45 - INFO: Epoch 419 average loss: 0.055964
01:03:47 - INFO: Epoch 429 average loss: 0.055607
01:03:49 - INFO: Epoch 439 average loss: 0.083063
01:03:50 - INFO: Epoch 449 average loss: 0.072159
01:03:52 - INFO: Epoch 459 average loss: 0.068539
01:03:54 - INFO: Epoch 469 average loss: 0.051717
01:03:56 - INFO: Epoch 479 average loss: 0.050328
01:03:58 - INFO: Epoch 489 average loss: 0.076591
01:03:59 - INFO: Starting epoch 499:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0708]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0506]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0768]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0553]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0723] 62%|██████▎   | 5/8 [00:00<00:00, 43.58it/s, MSE=0.0723] 62%|██████▎   | 5/8 [00:00<00:00, 43.58it/s, MSE=0.0367] 62%|██████▎   | 5/8 [00:00<00:00, 43.58it/s, MSE=0.0644] 62%|██████▎   | 5/8 [00:00<00:00, 43.58it/s, MSE=0.0375]100%|██████████| 8/8 [00:00<00:00, 45.07it/s, MSE=0.0375]
01:04:00 - INFO: Epoch 499 average loss: 0.058033
01:04:02 - INFO: Epoch 509 average loss: 0.056598
01:04:03 - INFO: Epoch 519 average loss: 0.067143
01:04:05 - INFO: Epoch 529 average loss: 0.072204
01:04:07 - INFO: Epoch 539 average loss: 0.061804
01:04:09 - INFO: Epoch 549 average loss: 0.054360
01:04:11 - INFO: Epoch 559 average loss: 0.053865
01:04:13 - INFO: Epoch 569 average loss: 0.057814
01:04:14 - INFO: Epoch 579 average loss: 0.067802
01:04:16 - INFO: Epoch 589 average loss: 0.055291
01:04:18 - INFO: Starting epoch 599:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0615]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0702]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.039]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0643]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0792] 62%|██████▎   | 5/8 [00:00<00:00, 41.71it/s, MSE=0.0792] 62%|██████▎   | 5/8 [00:00<00:00, 41.71it/s, MSE=0.0505] 62%|██████▎   | 5/8 [00:00<00:00, 41.71it/s, MSE=0.0535] 62%|██████▎   | 5/8 [00:00<00:00, 41.71it/s, MSE=0.00591]100%|██████████| 8/8 [00:00<00:00, 43.48it/s, MSE=0.00591]
01:04:18 - INFO: Epoch 599 average loss: 0.053009
01:04:20 - INFO: Epoch 609 average loss: 0.066498
01:04:22 - INFO: Epoch 619 average loss: 0.073372
01:04:24 - INFO: Epoch 629 average loss: 0.097826
01:04:25 - INFO: Epoch 639 average loss: 0.056249
01:04:27 - INFO: Epoch 649 average loss: 0.110447
01:04:29 - INFO: Epoch 659 average loss: 0.055710
01:04:31 - INFO: Epoch 669 average loss: 0.055605
01:04:32 - INFO: Epoch 679 average loss: 0.053373
01:04:34 - INFO: Epoch 689 average loss: 0.057710
01:04:36 - INFO: Starting epoch 699:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0352]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0816]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0526]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0448]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0731] 62%|██████▎   | 5/8 [00:00<00:00, 42.74it/s, MSE=0.0731] 62%|██████▎   | 5/8 [00:00<00:00, 42.74it/s, MSE=0.0399] 62%|██████▎   | 5/8 [00:00<00:00, 42.74it/s, MSE=0.023]  62%|██████▎   | 5/8 [00:00<00:00, 42.74it/s, MSE=0.00445]100%|██████████| 8/8 [00:00<00:00, 44.50it/s, MSE=0.00445]
01:04:36 - INFO: Epoch 699 average loss: 0.044333
01:04:38 - INFO: Epoch 709 average loss: 0.050376
01:04:40 - INFO: Epoch 719 average loss: 0.057843
01:04:42 - INFO: Epoch 729 average loss: 0.045498
01:04:44 - INFO: Epoch 739 average loss: 0.056221
01:04:45 - INFO: Epoch 749 average loss: 0.045931
01:04:47 - INFO: Epoch 759 average loss: 0.063534
01:04:49 - INFO: Epoch 769 average loss: 0.049511
01:04:51 - INFO: Epoch 779 average loss: 0.039316
01:04:52 - INFO: Epoch 789 average loss: 0.052324
01:04:54 - INFO: Starting epoch 799:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0589]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0725]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0425]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0505]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0452]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0357] 75%|███████▌  | 6/8 [00:00<00:00, 49.49it/s, MSE=0.0357] 75%|███████▌  | 6/8 [00:00<00:00, 49.49it/s, MSE=0.0577] 75%|███████▌  | 6/8 [00:00<00:00, 49.49it/s, MSE=0.0876]100%|██████████| 8/8 [00:00<00:00, 46.78it/s, MSE=0.0876]
01:04:54 - INFO: Epoch 799 average loss: 0.056332
01:04:56 - INFO: Epoch 809 average loss: 0.068708
01:04:58 - INFO: Epoch 819 average loss: 0.048016
01:05:00 - INFO: Epoch 829 average loss: 0.031423
01:05:01 - INFO: Epoch 839 average loss: 0.117114
01:05:03 - INFO: Epoch 849 average loss: 0.034403
01:05:05 - INFO: Epoch 859 average loss: 0.036174
01:05:07 - INFO: Epoch 869 average loss: 0.053447
01:05:09 - INFO: Epoch 879 average loss: 0.054242
01:05:11 - INFO: Epoch 889 average loss: 0.044438
01:05:12 - INFO: Starting epoch 899:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0518]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0479]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0511]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0435]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0848] 62%|██████▎   | 5/8 [00:00<00:00, 43.35it/s, MSE=0.0848] 62%|██████▎   | 5/8 [00:00<00:00, 43.35it/s, MSE=0.0533] 62%|██████▎   | 5/8 [00:00<00:00, 43.35it/s, MSE=0.0622] 62%|██████▎   | 5/8 [00:00<00:00, 43.35it/s, MSE=0.00212]100%|██████████| 8/8 [00:00<00:00, 42.35it/s, MSE=0.00212]
01:05:12 - INFO: Epoch 899 average loss: 0.049597
01:05:14 - INFO: Epoch 909 average loss: 0.051344
01:05:16 - INFO: Epoch 919 average loss: 0.039920
01:05:18 - INFO: Epoch 929 average loss: 0.035509
01:05:20 - INFO: Epoch 939 average loss: 0.063862
01:05:22 - INFO: Epoch 949 average loss: 0.067946
01:05:24 - INFO: Epoch 959 average loss: 0.057728
01:05:26 - INFO: Epoch 969 average loss: 0.049385
01:05:27 - INFO: Epoch 979 average loss: 0.045409
01:05:29 - INFO: Epoch 989 average loss: 0.031931
01:05:31 - INFO: Starting epoch 999:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0371]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0383]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0614]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.041]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0474] 62%|██████▎   | 5/8 [00:00<00:00, 45.54it/s, MSE=0.0474] 62%|██████▎   | 5/8 [00:00<00:00, 45.54it/s, MSE=0.0557] 62%|██████▎   | 5/8 [00:00<00:00, 45.54it/s, MSE=0.0441] 62%|██████▎   | 5/8 [00:00<00:00, 45.54it/s, MSE=0.0857]100%|██████████| 8/8 [00:00<00:00, 42.22it/s, MSE=0.0857]
01:05:31 - INFO: Epoch 999 average loss: 0.051326
01:05:32 - INFO: Total sampling 109 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
01:08:49 - INFO: Epoch 9 average loss: 0.266490
01:08:49 - INFO: Epoch 19 average loss: 0.148550
01:08:49 - INFO: Epoch 29 average loss: 0.154841
01:08:49 - INFO: Epoch 39 average loss: 0.119688
01:08:50 - INFO: Epoch 49 average loss: 0.101198
01:08:50 - INFO: Epoch 59 average loss: 0.207102
01:08:50 - INFO: Epoch 69 average loss: 0.110015
01:08:50 - INFO: Epoch 79 average loss: 0.090587
01:08:51 - INFO: Epoch 89 average loss: 0.136716
01:08:51 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... Now sampling 85st new convMatrix.... Now sampling 86st new convMatrix.... Now sampling 87st new convMatrix.... Now sampling 88st new convMatrix.... Now sampling 89st new convMatrix.... Now sampling 90st new convMatrix.... Now sampling 91st new convMatrix.... Now sampling 92st new convMatrix.... Now sampling 93st new convMatrix.... Now sampling 94st new convMatrix.... Now sampling 95st new convMatrix.... Now sampling 96st new convMatrix.... Now sampling 97st new convMatrix.... Now sampling 98st new convMatrix.... Now sampling 99st new convMatrix.... Now sampling 100st new convMatrix.... Now sampling 101st new convMatrix.... Now sampling 102st new convMatrix.... Now sampling 103st new convMatrix.... Now sampling 104st new convMatrix.... Now sampling 105st new convMatrix.... Now sampling 106st new convMatrix.... Now sampling 107st new convMatrix.... Now sampling 108st new convMatrix.... torch.Size([109, 48])
feature shape after [Data Augmentation]:  (222, 48)
(222, 48)
====>MLP training... Epoch: 20 total loss: 1.6349
====>MLP training... Epoch: 40 total loss: 0.8471
====>MLP training... Epoch: 60 total loss: 0.2408
====>MLP training... Epoch: 80 total loss: 0.1642
====>MLP training... Epoch: 100 total loss: 0.1304
====>MLP training... Epoch: 120 total loss: 0.1099
====>MLP training... Epoch: 140 total loss: 0.1003
====>MLP training... Epoch: 160 total loss: 0.1121
====>MLP training... Epoch: 180 total loss: 0.0998
====>MLP training... Epoch: 200 total loss: 0.0997
====>CNN training... Epoch: 0  total loss: 0.2519
====>CNN training... Epoch: 20  total loss: 0.2504
====>CNN training... Epoch: 40  total loss: 0.2464
====>CNN training... Epoch: 60  total loss: 0.2312
====>CNN training... Epoch: 80  total loss: 0.1833
====>CNN training... Epoch: 100  total loss: 0.1547
====>CNN training... Epoch: 120  total loss: 0.1310
====>CNN training... Epoch: 140  total loss: 0.1043
====>CNN training... Epoch: 160  total loss: 0.0821
====>CNN training... Epoch: 180  total loss: 0.0738
====>CNN training... Epoch: 200  total loss: 0.0642
====>RNN training... Epoch: 20 total loss: 0.2562
====>RNN training... Epoch: 40 total loss: 0.2507
====>RNN training... Epoch: 60 total loss: 0.2469
====>RNN training... Epoch: 80 total loss: 0.2439
====>RNN training... Epoch: 100 total loss: 0.2410
====>RNN training... Epoch: 120 total loss: 0.2381
====>RNN training... Epoch: 140 total loss: 0.2351
====>RNN training... Epoch: 160 total loss: 0.2318
====>RNN training... Epoch: 180 total loss: 0.2283
====>RNN training... Epoch: 200 total loss: 0.2245

test: Lang-11-fs_ddpm
fault line:  [100245]
feature shape before [Data Augmentation]:  (6, 55)
[2, 9, 11, 13, 15, 17, 24, 25, 26, 27, 28]
entropy 5.863915819330321 1.2730283365896256
req_shape:  16
statements selected: 16
statements selected: Index([100054, 100055, 100043, 100069, 100083, 100097, 100163, 100225, 100227,
       100230, 100234, 100247, 100248, 100250, 100252, 100253],
      dtype='int64')
Dataset shape: torch.Size([6, 1, 1, 16]) torch.Size([6])
Batch Dataloader shape: torch.Size([6, 1, 1, 16]) torch.Size([6])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.302]100%|██████████| 1/1 [00:00<00:00, 43.35it/s, MSE=0.302]
01:08:51 - INFO: Epoch 99 average loss: 0.301857
01:08:51 - INFO: Epoch 109 average loss: 0.507281
01:08:51 - INFO: Epoch 119 average loss: 0.202295
01:08:52 - INFO: Epoch 129 average loss: 0.186034
01:08:52 - INFO: Epoch 139 average loss: 0.091305
01:08:52 - INFO: Epoch 149 average loss: 0.142796
01:08:52 - INFO: Epoch 159 average loss: 0.130734
01:08:53 - INFO: Epoch 169 average loss: 0.167189
01:08:53 - INFO: Epoch 179 average loss: 0.163858
01:08:53 - INFO: Epoch 189 average loss: 0.231569
01:08:53 - INFO: Starting epoch 199:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.417]100%|██████████| 1/1 [00:00<00:00, 33.86it/s, MSE=0.417]
01:08:53 - INFO: Epoch 199 average loss: 0.417198
01:08:54 - INFO: Epoch 209 average loss: 0.080212
01:08:54 - INFO: Epoch 219 average loss: 0.065603
01:08:54 - INFO: Epoch 229 average loss: 0.353731
01:08:55 - INFO: Epoch 239 average loss: 0.055403
01:08:55 - INFO: Epoch 249 average loss: 0.130294
01:08:55 - INFO: Epoch 259 average loss: 0.083340
01:08:55 - INFO: Epoch 269 average loss: 0.174648
01:08:55 - INFO: Epoch 279 average loss: 0.235170
01:08:56 - INFO: Epoch 289 average loss: 0.164669
01:08:56 - INFO: Starting epoch 299:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.0904]100%|██████████| 1/1 [00:00<00:00, 42.86it/s, MSE=0.0904]
01:08:56 - INFO: Epoch 299 average loss: 0.090442
01:08:56 - INFO: Epoch 309 average loss: 0.115512
01:08:57 - INFO: Epoch 319 average loss: 0.134165
01:08:57 - INFO: Epoch 329 average loss: 0.052446
01:08:57 - INFO: Epoch 339 average loss: 0.285083
01:08:57 - INFO: Epoch 349 average loss: 0.043706
01:08:58 - INFO: Epoch 359 average loss: 0.099814
01:08:58 - INFO: Epoch 369 average loss: 0.077311
01:08:58 - INFO: Epoch 379 average loss: 0.087010
01:08:58 - INFO: Epoch 389 average loss: 0.129844
01:08:58 - INFO: Starting epoch 399:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.186]100%|██████████| 1/1 [00:00<00:00, 48.49it/s, MSE=0.186]
01:08:58 - INFO: Epoch 399 average loss: 0.186262
01:08:59 - INFO: Epoch 409 average loss: 0.281614
01:08:59 - INFO: Epoch 419 average loss: 0.270554
01:08:59 - INFO: Epoch 429 average loss: 0.176643
01:08:59 - INFO: Epoch 439 average loss: 0.407673
01:09:00 - INFO: Epoch 449 average loss: 0.198279
01:09:00 - INFO: Epoch 459 average loss: 0.062187
01:09:00 - INFO: Epoch 469 average loss: 0.171959
01:09:00 - INFO: Epoch 479 average loss: 0.079767
01:09:01 - INFO: Epoch 489 average loss: 0.119243
01:09:01 - INFO: Starting epoch 499:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.136]100%|██████████| 1/1 [00:00<00:00, 49.19it/s, MSE=0.136]
01:09:01 - INFO: Epoch 499 average loss: 0.136163
01:09:01 - INFO: Epoch 509 average loss: 0.256378
01:09:01 - INFO: Epoch 519 average loss: 0.241983
01:09:02 - INFO: Epoch 529 average loss: 0.045190
01:09:02 - INFO: Epoch 539 average loss: 0.029793
01:09:02 - INFO: Epoch 549 average loss: 0.202024
01:09:02 - INFO: Epoch 559 average loss: 0.192284
01:09:03 - INFO: Epoch 569 average loss: 0.133366
01:09:03 - INFO: Epoch 579 average loss: 0.100921
01:09:03 - INFO: Epoch 589 average loss: 0.189246
01:09:03 - INFO: Starting epoch 599:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.123]100%|██████████| 1/1 [00:00<00:00, 42.88it/s, MSE=0.123]
01:09:03 - INFO: Epoch 599 average loss: 0.123006
01:09:04 - INFO: Epoch 609 average loss: 0.146212
01:09:04 - INFO: Epoch 619 average loss: 0.156843
01:09:04 - INFO: Epoch 629 average loss: 0.131588
01:09:05 - INFO: Epoch 639 average loss: 0.051444
01:09:05 - INFO: Epoch 649 average loss: 0.109966
01:09:05 - INFO: Epoch 659 average loss: 0.099650
01:09:05 - INFO: Epoch 669 average loss: 0.114264
01:09:06 - INFO: Epoch 679 average loss: 0.134853
01:09:06 - INFO: Epoch 689 average loss: 0.090572
01:09:06 - INFO: Starting epoch 699:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.0914]100%|██████████| 1/1 [00:00<00:00, 40.10it/s, MSE=0.0914]
01:09:06 - INFO: Epoch 699 average loss: 0.091351
01:09:07 - INFO: Epoch 709 average loss: 0.075731
01:09:07 - INFO: Epoch 719 average loss: 0.217261
01:09:07 - INFO: Epoch 729 average loss: 0.172629
01:09:07 - INFO: Epoch 739 average loss: 0.197593
01:09:08 - INFO: Epoch 749 average loss: 0.164915
01:09:08 - INFO: Epoch 759 average loss: 0.214427
01:09:08 - INFO: Epoch 769 average loss: 0.136652
01:09:08 - INFO: Epoch 779 average loss: 0.309277
01:09:08 - INFO: Epoch 789 average loss: 0.164576
01:09:09 - INFO: Starting epoch 799:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.111]100%|██████████| 1/1 [00:00<00:00, 43.20it/s, MSE=0.111]
01:09:09 - INFO: Epoch 799 average loss: 0.110817
01:09:09 - INFO: Epoch 809 average loss: 0.157119
01:09:09 - INFO: Epoch 819 average loss: 0.104427
01:09:09 - INFO: Epoch 829 average loss: 0.204228
01:09:10 - INFO: Epoch 839 average loss: 0.120738
01:09:10 - INFO: Epoch 849 average loss: 0.207606
01:09:10 - INFO: Epoch 859 average loss: 0.139168
01:09:10 - INFO: Epoch 869 average loss: 0.242598
01:09:11 - INFO: Epoch 879 average loss: 0.123718
01:09:11 - INFO: Epoch 889 average loss: 0.123762
01:09:11 - INFO: Starting epoch 899:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.122]100%|██████████| 1/1 [00:00<00:00, 42.27it/s, MSE=0.122]
01:09:11 - INFO: Epoch 899 average loss: 0.121936
01:09:11 - INFO: Epoch 909 average loss: 0.149351
01:09:12 - INFO: Epoch 919 average loss: 0.227079
01:09:12 - INFO: Epoch 929 average loss: 0.180639
01:09:12 - INFO: Epoch 939 average loss: 0.131941
01:09:12 - INFO: Epoch 949 average loss: 0.122127
01:09:13 - INFO: Epoch 959 average loss: 0.054389
01:09:13 - INFO: Epoch 969 average loss: 0.067459
01:09:13 - INFO: Epoch 979 average loss: 0.173922
01:09:13 - INFO: Epoch 989 average loss: 0.152890
01:09:14 - INFO: Starting epoch 999:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.254]100%|██████████| 1/1 [00:00<00:00, 47.64it/s, MSE=0.254]
01:09:14 - INFO: Epoch 999 average loss: 0.254187
01:09:14 - INFO: Total sampling 4 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
01:09:32 - INFO: Epoch 9 average loss: 0.430833
01:09:32 - INFO: Epoch 19 average loss: 0.206373
01:09:32 - INFO: Epoch 29 average loss: 0.124168
01:09:32 - INFO: Epoch 39 average loss: 0.185794
01:09:32 - INFO: Epoch 49 average loss: 0.379008
01:09:33 - INFO: Epoch 59 average loss: 0.109640
01:09:33 - INFO: Epoch 69 average loss: 0.479979
01:09:33 - INFO: Epoch 79 average loss: 0.095176
01:09:33 - INFO: Epoch 89 average loss: 0.321137
01:09:34 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... torch.Size([4, 16])
feature shape after [Data Augmentation]:  (10, 16)
(10, 16)
====>MLP training... Epoch: 20 total loss: 0.4881
====>MLP training... Epoch: 40 total loss: 0.5061
====>MLP training... Epoch: 60 total loss: 0.4773
====>MLP training... Epoch: 80 total loss: 0.4781
====>MLP training... Epoch: 100 total loss: 0.4809
====>MLP training... Epoch: 120 total loss: 0.3619
====>MLP training... Epoch: 140 total loss: 0.3640
====>MLP training... Epoch: 160 total loss: 0.0887
====>MLP training... Epoch: 180 total loss: 0.0205
====>MLP training... Epoch: 200 total loss: 0.0277
====>CNN training... Epoch: 0  total loss: 0.2563
====>CNN training... Epoch: 20  total loss: 0.2559
====>CNN training... Epoch: 40  total loss: 0.2467
====>CNN training... Epoch: 60  total loss: 0.2441
====>CNN training... Epoch: 80  total loss: 0.2259
====>CNN training... Epoch: 100  total loss: 0.1973
====>CNN training... Epoch: 120  total loss: 0.1395
====>CNN training... Epoch: 140  total loss: 0.1165
====>CNN training... Epoch: 160  total loss: 0.0791
====>CNN training... Epoch: 180  total loss: 0.0797
====>CNN training... Epoch: 200  total loss: 0.0636
====>RNN training... Epoch: 20 total loss: 0.2590
====>RNN training... Epoch: 40 total loss: 0.2573
====>RNN training... Epoch: 60 total loss: 0.2556
====>RNN training... Epoch: 80 total loss: 0.2541
====>RNN training... Epoch: 100 total loss: 0.2526
====>RNN training... Epoch: 120 total loss: 0.2512
====>RNN training... Epoch: 140 total loss: 0.2499
====>RNN training... Epoch: 160 total loss: 0.2487
====>RNN training... Epoch: 180 total loss: 0.2474
====>RNN training... Epoch: 200 total loss: 0.2462

test: Lang-12-fs_ddpm
fault line:  [100230, 100232, 100238]
feature shape before [Data Augmentation]:  (5, 51)
[2, 3, 8, 9, 11, 13, 14, 15, 16, 17, 48, 50, 20, 21, 22, 23, 25]
entropy 9.042094239840004 2.692046668037026
req_shape:  16
statements selected: 16
statements selected: Index([100043, 100069, 100143, 100163, 100225, 100227, 100228, 100231, 100232,
       100233, 100240, 100241, 100243, 100245, 100248, 100318],
      dtype='int64')
Dataset shape: torch.Size([5, 1, 1, 16]) torch.Size([5])
Batch Dataloader shape: torch.Size([5, 1, 1, 16]) torch.Size([5])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.42]100%|██████████| 1/1 [00:00<00:00, 46.63it/s, MSE=0.42]
01:09:34 - INFO: Epoch 99 average loss: 0.419571
01:09:34 - INFO: Epoch 109 average loss: 0.130497
01:09:34 - INFO: Epoch 119 average loss: 0.128598
01:09:35 - INFO: Epoch 129 average loss: 0.187562
01:09:35 - INFO: Epoch 139 average loss: 0.181981
01:09:35 - INFO: Epoch 149 average loss: 0.244667
01:09:35 - INFO: Epoch 159 average loss: 0.115910
01:09:35 - INFO: Epoch 169 average loss: 0.183265
01:09:36 - INFO: Epoch 179 average loss: 0.511932
01:09:36 - INFO: Epoch 189 average loss: 0.169390
01:09:36 - INFO: Starting epoch 199:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.133]100%|██████████| 1/1 [00:00<00:00, 44.18it/s, MSE=0.133]
01:09:36 - INFO: Epoch 199 average loss: 0.133380
01:09:37 - INFO: Epoch 209 average loss: 0.073239
01:09:37 - INFO: Epoch 219 average loss: 0.087470
01:09:37 - INFO: Epoch 229 average loss: 0.150795
01:09:37 - INFO: Epoch 239 average loss: 0.367203
01:09:38 - INFO: Epoch 249 average loss: 0.062502
01:09:38 - INFO: Epoch 259 average loss: 0.325723
01:09:38 - INFO: Epoch 269 average loss: 0.395120
01:09:38 - INFO: Epoch 279 average loss: 0.381177
01:09:38 - INFO: Epoch 289 average loss: 0.309388
01:09:39 - INFO: Starting epoch 299:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.142]100%|██████████| 1/1 [00:00<00:00, 48.73it/s, MSE=0.142]
01:09:39 - INFO: Epoch 299 average loss: 0.142224
01:09:39 - INFO: Epoch 309 average loss: 0.527891
01:09:39 - INFO: Epoch 319 average loss: 0.116807
01:09:39 - INFO: Epoch 329 average loss: 0.094142
01:09:40 - INFO: Epoch 339 average loss: 0.095281
01:09:40 - INFO: Epoch 349 average loss: 0.128251
01:09:40 - INFO: Epoch 359 average loss: 0.086667
01:09:40 - INFO: Epoch 369 average loss: 0.138403
01:09:40 - INFO: Epoch 379 average loss: 0.121911
01:09:41 - INFO: Epoch 389 average loss: 0.052752
01:09:41 - INFO: Starting epoch 399:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.218]100%|██████████| 1/1 [00:00<00:00, 42.53it/s, MSE=0.218]
01:09:41 - INFO: Epoch 399 average loss: 0.218386
01:09:41 - INFO: Epoch 409 average loss: 0.110068
01:09:41 - INFO: Epoch 419 average loss: 0.127972
01:09:42 - INFO: Epoch 429 average loss: 0.055356
01:09:42 - INFO: Epoch 439 average loss: 0.096127
01:09:42 - INFO: Epoch 449 average loss: 0.206776
01:09:42 - INFO: Epoch 459 average loss: 0.085887
01:09:43 - INFO: Epoch 469 average loss: 0.107217
01:09:43 - INFO: Epoch 479 average loss: 0.342533
01:09:43 - INFO: Epoch 489 average loss: 0.197758
01:09:43 - INFO: Starting epoch 499:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.0413]100%|██████████| 1/1 [00:00<00:00, 48.00it/s, MSE=0.0413]
01:09:43 - INFO: Epoch 499 average loss: 0.041296
01:09:44 - INFO: Epoch 509 average loss: 0.099952
01:09:44 - INFO: Epoch 519 average loss: 0.047765
01:09:44 - INFO: Epoch 529 average loss: 0.324557
01:09:44 - INFO: Epoch 539 average loss: 0.136895
01:09:44 - INFO: Epoch 549 average loss: 0.277755
01:09:45 - INFO: Epoch 559 average loss: 0.156054
01:09:45 - INFO: Epoch 569 average loss: 0.101391
01:09:45 - INFO: Epoch 579 average loss: 0.113139
01:09:45 - INFO: Epoch 589 average loss: 0.225044
01:09:46 - INFO: Starting epoch 599:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.121]100%|██████████| 1/1 [00:00<00:00, 52.27it/s, MSE=0.121]
01:09:46 - INFO: Epoch 599 average loss: 0.121301
01:09:46 - INFO: Epoch 609 average loss: 0.085955
01:09:46 - INFO: Epoch 619 average loss: 0.159900
01:09:46 - INFO: Epoch 629 average loss: 0.200897
01:09:47 - INFO: Epoch 639 average loss: 0.109200
01:09:47 - INFO: Epoch 649 average loss: 0.332609
01:09:47 - INFO: Epoch 659 average loss: 0.108918
01:09:47 - INFO: Epoch 669 average loss: 0.087188
01:09:48 - INFO: Epoch 679 average loss: 0.102496
01:09:48 - INFO: Epoch 689 average loss: 0.076762
01:09:48 - INFO: Starting epoch 699:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.129]100%|██████████| 1/1 [00:00<00:00, 45.66it/s, MSE=0.129]
01:09:48 - INFO: Epoch 699 average loss: 0.129007
01:09:48 - INFO: Epoch 709 average loss: 0.100916
01:09:49 - INFO: Epoch 719 average loss: 0.027094
01:09:49 - INFO: Epoch 729 average loss: 0.228064
01:09:49 - INFO: Epoch 739 average loss: 0.093846
01:09:49 - INFO: Epoch 749 average loss: 0.044260
01:09:50 - INFO: Epoch 759 average loss: 0.107443
01:09:50 - INFO: Epoch 769 average loss: 0.261058
01:09:50 - INFO: Epoch 779 average loss: 0.119079
01:09:50 - INFO: Epoch 789 average loss: 0.057334
01:09:51 - INFO: Starting epoch 799:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.142]100%|██████████| 1/1 [00:00<00:00, 46.96it/s, MSE=0.142]
01:09:51 - INFO: Epoch 799 average loss: 0.141721
01:09:51 - INFO: Epoch 809 average loss: 0.080269
01:09:51 - INFO: Epoch 819 average loss: 0.077160
01:09:52 - INFO: Epoch 829 average loss: 0.080633
01:09:52 - INFO: Epoch 839 average loss: 0.176922
01:09:52 - INFO: Epoch 849 average loss: 0.050303
01:09:52 - INFO: Epoch 859 average loss: 0.093629
01:09:52 - INFO: Epoch 869 average loss: 0.050862
01:09:53 - INFO: Epoch 879 average loss: 0.076120
01:09:53 - INFO: Epoch 889 average loss: 0.085933
01:09:53 - INFO: Starting epoch 899:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.115]100%|██████████| 1/1 [00:00<00:00, 34.28it/s, MSE=0.115]
01:09:53 - INFO: Epoch 899 average loss: 0.114787
01:09:54 - INFO: Epoch 909 average loss: 0.053358
01:09:54 - INFO: Epoch 919 average loss: 0.186451
01:09:54 - INFO: Epoch 929 average loss: 0.025091
01:09:54 - INFO: Epoch 939 average loss: 0.169385
01:09:54 - INFO: Epoch 949 average loss: 0.101605
01:09:55 - INFO: Epoch 959 average loss: 0.155227
01:09:55 - INFO: Epoch 969 average loss: 0.044199
01:09:55 - INFO: Epoch 979 average loss: 0.061216
01:09:55 - INFO: Epoch 989 average loss: 0.136311
01:09:56 - INFO: Starting epoch 999:
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s, MSE=0.0557]100%|██████████| 1/1 [00:00<00:00, 41.27it/s, MSE=0.0557]
01:09:56 - INFO: Epoch 999 average loss: 0.055737
01:09:56 - INFO: Total sampling 1 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
01:10:07 - INFO: Epoch 9 average loss: 0.263992
01:10:08 - INFO: Epoch 19 average loss: 0.210543
01:10:09 - INFO: Epoch 29 average loss: 0.172832
01:10:11 - INFO: Epoch 39 average loss: 0.140044
01:10:12 - INFO: Epoch 49 average loss: 0.143580
01:10:13 - INFO: Epoch 59 average loss: 0.093701
01:10:14 - INFO: Epoch 69 average loss: 0.116026
01:10:16 - INFO: Epoch 79 average loss: 0.130539
01:10:17 - INFO: Epoch 89 average loss: 0.116914
01:10:18 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... torch.Size([1, 16])
feature shape after [Data Augmentation]:  (6, 16)
(6, 16)
====>MLP training... Epoch: 20 total loss: 0.2483
====>MLP training... Epoch: 40 total loss: 0.2462
====>MLP training... Epoch: 60 total loss: 0.2456
====>MLP training... Epoch: 80 total loss: 0.2426
====>MLP training... Epoch: 100 total loss: 0.2445
====>MLP training... Epoch: 120 total loss: 0.2374
====>MLP training... Epoch: 140 total loss: 0.2245
====>MLP training... Epoch: 160 total loss: 0.2131
====>MLP training... Epoch: 180 total loss: 0.1817
====>MLP training... Epoch: 200 total loss: 0.1541
====>CNN training... Epoch: 0  total loss: 0.2517
====>CNN training... Epoch: 20  total loss: 0.2502
====>CNN training... Epoch: 40  total loss: 0.2428
====>CNN training... Epoch: 60  total loss: 0.2234
====>CNN training... Epoch: 80  total loss: 0.2090
====>CNN training... Epoch: 100  total loss: 0.1333
====>CNN training... Epoch: 120  total loss: 0.1041
====>CNN training... Epoch: 140  total loss: 0.0492
====>CNN training... Epoch: 160  total loss: 0.0437
====>CNN training... Epoch: 180  total loss: 0.0004
====>CNN training... Epoch: 200  total loss: 0.0274
====>RNN training... Epoch: 20 total loss: 0.2456
====>RNN training... Epoch: 40 total loss: 0.2422
====>RNN training... Epoch: 60 total loss: 0.2388
====>RNN training... Epoch: 80 total loss: 0.2352
====>RNN training... Epoch: 100 total loss: 0.2316
====>RNN training... Epoch: 120 total loss: 0.2280
====>RNN training... Epoch: 140 total loss: 0.2241
====>RNN training... Epoch: 160 total loss: 0.2202
====>RNN training... Epoch: 180 total loss: 0.2160
====>RNN training... Epoch: 200 total loss: 0.2117

test: Lang-13-fs_ddpm
fault line:  [100239, 100252, 100268, 100269]
feature shape before [Data Augmentation]:  (35, 70)
[2, 4, 5, 6, 7, 8, 15, 16, 19, 20, 22, 23, 24, 25, 26, 27, 33, 34, 35, 36, 61, 62, 63, 64, 65, 66, 67, 68, 69]
entropy 13.759198718290389 3.323505988070251
req_shape:  16
statements selected: 16
statements selected: Index([100079, 100082, 100083, 100085, 100088, 100095, 100098, 100099, 100103,
       100131, 100134, 100137, 100138, 100144, 100145, 100149],
      dtype='int64')
Dataset shape: torch.Size([35, 1, 1, 16]) torch.Size([35])
Batch Dataloader shape: torch.Size([8, 1, 1, 16]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.096]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0702]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0817]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.149]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.24] 100%|██████████| 5/5 [00:00<00:00, 47.56it/s, MSE=0.24]100%|██████████| 5/5 [00:00<00:00, 47.37it/s, MSE=0.24]
01:10:18 - INFO: Epoch 99 average loss: 0.127447
01:10:19 - INFO: Epoch 109 average loss: 0.181611
01:10:20 - INFO: Epoch 119 average loss: 0.098456
01:10:22 - INFO: Epoch 129 average loss: 0.145386
01:10:23 - INFO: Epoch 139 average loss: 0.097636
01:10:24 - INFO: Epoch 149 average loss: 0.174764
01:10:25 - INFO: Epoch 159 average loss: 0.166459
01:10:26 - INFO: Epoch 169 average loss: 0.106724
01:10:27 - INFO: Epoch 179 average loss: 0.123451
01:10:28 - INFO: Epoch 189 average loss: 0.149481
01:10:29 - INFO: Starting epoch 199:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.148]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0737]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0911]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.235]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.08] 100%|██████████| 5/5 [00:00<00:00, 57.23it/s, MSE=0.08]
01:10:29 - INFO: Epoch 199 average loss: 0.125570
01:10:30 - INFO: Epoch 209 average loss: 0.103128
01:10:32 - INFO: Epoch 219 average loss: 0.088759
01:10:33 - INFO: Epoch 229 average loss: 0.131337
01:10:34 - INFO: Epoch 239 average loss: 0.157737
01:10:35 - INFO: Epoch 249 average loss: 0.115631
01:10:36 - INFO: Epoch 259 average loss: 0.092683
01:10:37 - INFO: Epoch 269 average loss: 0.159549
01:10:38 - INFO: Epoch 279 average loss: 0.148396
01:10:40 - INFO: Epoch 289 average loss: 0.142254
01:10:41 - INFO: Starting epoch 299:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0863]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0865]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0583]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0581]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.176] 100%|██████████| 5/5 [00:00<00:00, 44.09it/s, MSE=0.176]100%|██████████| 5/5 [00:00<00:00, 44.02it/s, MSE=0.176]
01:10:41 - INFO: Epoch 299 average loss: 0.093079
01:10:42 - INFO: Epoch 309 average loss: 0.080071
01:10:43 - INFO: Epoch 319 average loss: 0.114084
01:10:45 - INFO: Epoch 329 average loss: 0.142410
01:10:46 - INFO: Epoch 339 average loss: 0.103197
01:10:47 - INFO: Epoch 349 average loss: 0.099769
01:10:48 - INFO: Epoch 359 average loss: 0.113362
01:10:49 - INFO: Epoch 369 average loss: 0.117934
01:10:51 - INFO: Epoch 379 average loss: 0.075458
01:10:52 - INFO: Epoch 389 average loss: 0.112783
01:10:53 - INFO: Starting epoch 399:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0941]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0425]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.126]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0622]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0608]100%|██████████| 5/5 [00:00<00:00, 49.08it/s, MSE=0.0608]100%|██████████| 5/5 [00:00<00:00, 49.01it/s, MSE=0.0608]
01:10:53 - INFO: Epoch 399 average loss: 0.077175
01:10:54 - INFO: Epoch 409 average loss: 0.065655
01:10:55 - INFO: Epoch 419 average loss: 0.093944
01:10:56 - INFO: Epoch 429 average loss: 0.070292
01:10:57 - INFO: Epoch 439 average loss: 0.068469
01:10:59 - INFO: Epoch 449 average loss: 0.090493
01:11:00 - INFO: Epoch 459 average loss: 0.092218
01:11:01 - INFO: Epoch 469 average loss: 0.101909
01:11:02 - INFO: Epoch 479 average loss: 0.078659
01:11:03 - INFO: Epoch 489 average loss: 0.094307
01:11:04 - INFO: Starting epoch 499:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.184]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0748]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0638]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.102]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.021]100%|██████████| 5/5 [00:00<00:00, 57.23it/s, MSE=0.021]
01:11:04 - INFO: Epoch 499 average loss: 0.089091
01:11:06 - INFO: Epoch 509 average loss: 0.064786
01:11:07 - INFO: Epoch 519 average loss: 0.090936
01:11:08 - INFO: Epoch 529 average loss: 0.072268
01:11:09 - INFO: Epoch 539 average loss: 0.170014
01:11:11 - INFO: Epoch 549 average loss: 0.069955
01:11:12 - INFO: Epoch 559 average loss: 0.108418
01:11:13 - INFO: Epoch 569 average loss: 0.090472
01:11:14 - INFO: Epoch 579 average loss: 0.084581
01:11:15 - INFO: Epoch 589 average loss: 0.112169
01:11:16 - INFO: Starting epoch 599:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0924]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.188]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.11]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0892]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0868]100%|██████████| 5/5 [00:00<00:00, 54.53it/s, MSE=0.0868]
01:11:16 - INFO: Epoch 599 average loss: 0.113362
01:11:17 - INFO: Epoch 609 average loss: 0.095038
01:11:19 - INFO: Epoch 619 average loss: 0.081929
01:11:20 - INFO: Epoch 629 average loss: 0.102685
01:11:21 - INFO: Epoch 639 average loss: 0.104656
01:11:22 - INFO: Epoch 649 average loss: 0.070668
01:11:24 - INFO: Epoch 659 average loss: 0.064887
01:11:25 - INFO: Epoch 669 average loss: 0.112753
01:11:26 - INFO: Epoch 679 average loss: 0.110283
01:11:27 - INFO: Epoch 689 average loss: 0.084279
01:11:28 - INFO: Starting epoch 699:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0632]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.23]    0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0292]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0399]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.14]  100%|██████████| 5/5 [00:00<00:00, 39.55it/s, MSE=0.14]100%|██████████| 5/5 [00:00<00:00, 39.37it/s, MSE=0.14]
01:11:28 - INFO: Epoch 699 average loss: 0.100504
01:11:29 - INFO: Epoch 709 average loss: 0.109739
01:11:31 - INFO: Epoch 719 average loss: 0.053589
01:11:32 - INFO: Epoch 729 average loss: 0.095011
01:11:33 - INFO: Epoch 739 average loss: 0.089137
01:11:34 - INFO: Epoch 749 average loss: 0.096728
01:11:35 - INFO: Epoch 759 average loss: 0.116700
01:11:36 - INFO: Epoch 769 average loss: 0.078087
01:11:38 - INFO: Epoch 779 average loss: 0.080022
01:11:39 - INFO: Epoch 789 average loss: 0.053558
01:11:40 - INFO: Starting epoch 799:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0796]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.104]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.107]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0848]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0866]100%|██████████| 5/5 [00:00<00:00, 50.49it/s, MSE=0.0866]
01:11:40 - INFO: Epoch 799 average loss: 0.092424
01:11:41 - INFO: Epoch 809 average loss: 0.066890
01:11:42 - INFO: Epoch 819 average loss: 0.123620
01:11:43 - INFO: Epoch 829 average loss: 0.089073
01:11:44 - INFO: Epoch 839 average loss: 0.069182
01:11:46 - INFO: Epoch 849 average loss: 0.059858
01:11:47 - INFO: Epoch 859 average loss: 0.068638
01:11:48 - INFO: Epoch 869 average loss: 0.110539
01:11:49 - INFO: Epoch 879 average loss: 0.079291
01:11:50 - INFO: Epoch 889 average loss: 0.061910
01:11:51 - INFO: Starting epoch 899:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0989]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0736]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0747]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.053]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.103]100%|██████████| 5/5 [00:00<00:00, 44.25it/s, MSE=0.103]100%|██████████| 5/5 [00:00<00:00, 44.13it/s, MSE=0.103]
01:11:51 - INFO: Epoch 899 average loss: 0.080605
01:11:53 - INFO: Epoch 909 average loss: 0.080741
01:11:54 - INFO: Epoch 919 average loss: 0.067186
01:11:55 - INFO: Epoch 929 average loss: 0.108359
01:11:56 - INFO: Epoch 939 average loss: 0.076928
01:11:57 - INFO: Epoch 949 average loss: 0.089680
01:11:58 - INFO: Epoch 959 average loss: 0.068124
01:12:00 - INFO: Epoch 969 average loss: 0.092560
01:12:01 - INFO: Epoch 979 average loss: 0.103559
01:12:02 - INFO: Epoch 989 average loss: 0.093674
01:12:03 - INFO: Starting epoch 999:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0968]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0323]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.094]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0666]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.12]  100%|██████████| 5/5 [00:00<00:00, 55.80it/s, MSE=0.12]
01:12:03 - INFO: Epoch 999 average loss: 0.081968
01:12:03 - INFO: Total sampling 33 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
01:13:14 - INFO: Epoch 9 average loss: 0.170472
01:13:15 - INFO: Epoch 19 average loss: 0.152546
01:13:17 - INFO: Epoch 29 average loss: 0.115184
01:13:18 - INFO: Epoch 39 average loss: 0.119581
01:13:19 - INFO: Epoch 49 average loss: 0.093984
01:13:20 - INFO: Epoch 59 average loss: 0.116824
01:13:21 - INFO: Epoch 69 average loss: 0.087845
01:13:22 - INFO: Epoch 79 average loss: 0.074326
01:13:24 - INFO: Epoch 89 average loss: 0.076733
01:13:25 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... torch.Size([33, 16])
feature shape after [Data Augmentation]:  (68, 16)
(68, 16)
====>MLP training... Epoch: 20 total loss: 0.9651
====>MLP training... Epoch: 40 total loss: 0.3427
====>MLP training... Epoch: 60 total loss: 0.0971
====>MLP training... Epoch: 80 total loss: 0.0762
====>MLP training... Epoch: 100 total loss: 0.0619
====>MLP training... Epoch: 120 total loss: 0.0760
====>MLP training... Epoch: 140 total loss: 0.0625
====>MLP training... Epoch: 160 total loss: 0.0666
====>MLP training... Epoch: 180 total loss: 0.0670
====>MLP training... Epoch: 200 total loss: 0.0637
====>CNN training... Epoch: 0  total loss: 0.2523
====>CNN training... Epoch: 20  total loss: 0.2518
====>CNN training... Epoch: 40  total loss: 0.2518
====>CNN training... Epoch: 60  total loss: 0.2486
====>CNN training... Epoch: 80  total loss: 0.2480
====>CNN training... Epoch: 100  total loss: 0.2476
====>CNN training... Epoch: 120  total loss: 0.2489
====>CNN training... Epoch: 140  total loss: 0.2487
====>CNN training... Epoch: 160  total loss: 0.2455
====>CNN training... Epoch: 180  total loss: 0.2433
====>CNN training... Epoch: 200  total loss: 0.2371
====>RNN training... Epoch: 20 total loss: 0.2602
====>RNN training... Epoch: 40 total loss: 0.2564
====>RNN training... Epoch: 60 total loss: 0.2528
====>RNN training... Epoch: 80 total loss: 0.2493
====>RNN training... Epoch: 100 total loss: 0.2459
====>RNN training... Epoch: 120 total loss: 0.2424
====>RNN training... Epoch: 140 total loss: 0.2388
====>RNN training... Epoch: 160 total loss: 0.2348
====>RNN training... Epoch: 180 total loss: 0.2304
====>RNN training... Epoch: 200 total loss: 0.2253

test: Lang-14-fs_ddpm
fault line:  [300788, 300789]
feature shape before [Data Augmentation]:  (313, 1310)
[142, 143, 144, 146, 63]
entropy 0.8440967859193959 0.02154825572686733
req_shape:  16
statements selected: 16
statements selected: Index([100707, 100711, 100716, 100720, 100721, 100722, 100723, 100724, 100728,
       100730, 100731, 300148, 300782, 300783, 300785, 300788],
      dtype='int64')
Dataset shape: torch.Size([313, 1, 1, 16]) torch.Size([313])
Batch Dataloader shape: torch.Size([64, 1, 1, 16]) torch.Size([64])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0728]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0671]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.062]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0719]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0794]100%|██████████| 5/5 [00:00<00:00, 40.95it/s, MSE=0.0794]100%|██████████| 5/5 [00:00<00:00, 40.85it/s, MSE=0.0794]
01:13:25 - INFO: Epoch 99 average loss: 0.070640
01:13:27 - INFO: Epoch 109 average loss: 0.068958
01:13:28 - INFO: Epoch 119 average loss: 0.061319
01:13:29 - INFO: Epoch 129 average loss: 0.066568
01:13:30 - INFO: Epoch 139 average loss: 0.057108
01:13:31 - INFO: Epoch 149 average loss: 0.062330
01:13:33 - INFO: Epoch 159 average loss: 0.077230
01:13:34 - INFO: Epoch 169 average loss: 0.049347
01:13:35 - INFO: Epoch 179 average loss: 0.044853
01:13:36 - INFO: Epoch 189 average loss: 0.049553
01:13:37 - INFO: Starting epoch 199:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.048]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0652]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0429]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0339] 80%|████████  | 4/5 [00:00<00:00, 32.31it/s, MSE=0.0339] 80%|████████  | 4/5 [00:00<00:00, 32.31it/s, MSE=0.0486]100%|██████████| 5/5 [00:00<00:00, 33.15it/s, MSE=0.0486]
01:13:37 - INFO: Epoch 199 average loss: 0.047711
01:13:39 - INFO: Epoch 209 average loss: 0.049969
01:13:40 - INFO: Epoch 219 average loss: 0.047649
01:13:41 - INFO: Epoch 229 average loss: 0.052095
01:13:43 - INFO: Epoch 239 average loss: 0.050072
01:13:44 - INFO: Epoch 249 average loss: 0.048646
01:13:45 - INFO: Epoch 259 average loss: 0.038171
01:13:46 - INFO: Epoch 269 average loss: 0.047160
01:13:48 - INFO: Epoch 279 average loss: 0.050397
01:13:49 - INFO: Epoch 289 average loss: 0.043358
01:13:50 - INFO: Starting epoch 299:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0451]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0544]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0344]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0358] 80%|████████  | 4/5 [00:00<00:00, 30.50it/s, MSE=0.0358] 80%|████████  | 4/5 [00:00<00:00, 30.50it/s, MSE=0.0548]100%|██████████| 5/5 [00:00<00:00, 30.50it/s, MSE=0.0548]
01:13:50 - INFO: Epoch 299 average loss: 0.044931
01:13:52 - INFO: Epoch 309 average loss: 0.045454
01:13:53 - INFO: Epoch 319 average loss: 0.042495
01:13:55 - INFO: Epoch 329 average loss: 0.065529
01:13:56 - INFO: Epoch 339 average loss: 0.049450
01:13:57 - INFO: Epoch 349 average loss: 0.043229
01:13:58 - INFO: Epoch 359 average loss: 0.041137
01:13:59 - INFO: Epoch 369 average loss: 0.041803
01:14:01 - INFO: Epoch 379 average loss: 0.035326
01:14:02 - INFO: Epoch 389 average loss: 0.040888
01:14:03 - INFO: Starting epoch 399:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0425]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0395]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0578]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0499] 80%|████████  | 4/5 [00:00<00:00, 39.69it/s, MSE=0.0499] 80%|████████  | 4/5 [00:00<00:00, 39.69it/s, MSE=0.05]  100%|██████████| 5/5 [00:00<00:00, 41.12it/s, MSE=0.05]
01:14:03 - INFO: Epoch 399 average loss: 0.047959
01:14:04 - INFO: Epoch 409 average loss: 0.046421
01:14:06 - INFO: Epoch 419 average loss: 0.051633
01:14:07 - INFO: Epoch 429 average loss: 0.041794
01:14:08 - INFO: Epoch 439 average loss: 0.043375
01:14:09 - INFO: Epoch 449 average loss: 0.040270
01:14:10 - INFO: Epoch 459 average loss: 0.056489
01:14:11 - INFO: Epoch 469 average loss: 0.042097
01:14:13 - INFO: Epoch 479 average loss: 0.047566
01:14:14 - INFO: Epoch 489 average loss: 0.055263
01:14:15 - INFO: Starting epoch 499:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0366]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0547]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0507]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0564] 80%|████████  | 4/5 [00:00<00:00, 35.25it/s, MSE=0.0564] 80%|████████  | 4/5 [00:00<00:00, 35.25it/s, MSE=0.0348]100%|██████████| 5/5 [00:00<00:00, 34.61it/s, MSE=0.0348]
01:14:15 - INFO: Epoch 499 average loss: 0.046672
01:14:16 - INFO: Epoch 509 average loss: 0.035520
01:14:18 - INFO: Epoch 519 average loss: 0.050736
01:14:19 - INFO: Epoch 529 average loss: 0.046353
01:14:20 - INFO: Epoch 539 average loss: 0.042984
01:14:21 - INFO: Epoch 549 average loss: 0.044600
01:14:22 - INFO: Epoch 559 average loss: 0.042415
01:14:23 - INFO: Epoch 569 average loss: 0.040339
01:14:24 - INFO: Epoch 579 average loss: 0.044807
01:14:26 - INFO: Epoch 589 average loss: 0.041239
01:14:27 - INFO: Starting epoch 599:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0404]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0526]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0482]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.043]  80%|████████  | 4/5 [00:00<00:00, 36.36it/s, MSE=0.043] 80%|████████  | 4/5 [00:00<00:00, 36.36it/s, MSE=0.0574]100%|██████████| 5/5 [00:00<00:00, 36.43it/s, MSE=0.0574]
01:14:27 - INFO: Epoch 599 average loss: 0.048341
01:14:28 - INFO: Epoch 609 average loss: 0.040427
01:14:29 - INFO: Epoch 619 average loss: 0.038247
01:14:30 - INFO: Epoch 629 average loss: 0.037844
01:14:31 - INFO: Epoch 639 average loss: 0.041877
01:14:32 - INFO: Epoch 649 average loss: 0.036709
01:14:34 - INFO: Epoch 659 average loss: 0.038656
01:14:35 - INFO: Epoch 669 average loss: 0.037615
01:14:36 - INFO: Epoch 679 average loss: 0.043022
01:14:37 - INFO: Epoch 689 average loss: 0.039280
01:14:39 - INFO: Starting epoch 699:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0428]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0442]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0403]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0433]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0519]100%|██████████| 5/5 [00:00<00:00, 42.35it/s, MSE=0.0519]100%|██████████| 5/5 [00:00<00:00, 42.24it/s, MSE=0.0519]
01:14:39 - INFO: Epoch 699 average loss: 0.044503
01:14:40 - INFO: Epoch 709 average loss: 0.038549
01:14:41 - INFO: Epoch 719 average loss: 0.042143
01:14:42 - INFO: Epoch 729 average loss: 0.045336
01:14:44 - INFO: Epoch 739 average loss: 0.038121
01:14:45 - INFO: Epoch 749 average loss: 0.046312
01:14:46 - INFO: Epoch 759 average loss: 0.042766
01:14:47 - INFO: Epoch 769 average loss: 0.047388
01:14:49 - INFO: Epoch 779 average loss: 0.041689
01:14:50 - INFO: Epoch 789 average loss: 0.038565
01:14:51 - INFO: Starting epoch 799:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0461]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0255]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0379]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0364]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0407]100%|██████████| 5/5 [00:00<00:00, 43.60it/s, MSE=0.0407]100%|██████████| 5/5 [00:00<00:00, 42.49it/s, MSE=0.0407]
01:14:51 - INFO: Epoch 799 average loss: 0.037333
01:14:52 - INFO: Epoch 809 average loss: 0.040649
01:14:53 - INFO: Epoch 819 average loss: 0.039855
01:14:55 - INFO: Epoch 829 average loss: 0.040358
01:14:56 - INFO: Epoch 839 average loss: 0.038206
01:14:57 - INFO: Epoch 849 average loss: 0.051159
01:14:58 - INFO: Epoch 859 average loss: 0.041728
01:15:00 - INFO: Epoch 869 average loss: 0.040761
01:15:01 - INFO: Epoch 879 average loss: 0.045287
01:15:02 - INFO: Epoch 889 average loss: 0.047269
01:15:03 - INFO: Starting epoch 899:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0328]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0554]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0448]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0415] 80%|████████  | 4/5 [00:00<00:00, 35.04it/s, MSE=0.0415] 80%|████████  | 4/5 [00:00<00:00, 35.04it/s, MSE=0.043] 100%|██████████| 5/5 [00:00<00:00, 34.88it/s, MSE=0.043]
01:15:03 - INFO: Epoch 899 average loss: 0.043501
01:15:05 - INFO: Epoch 909 average loss: 0.045710
01:15:06 - INFO: Epoch 919 average loss: 0.041266
01:15:07 - INFO: Epoch 929 average loss: 0.038630
01:15:09 - INFO: Epoch 939 average loss: 0.036636
01:15:10 - INFO: Epoch 949 average loss: 0.043229
01:15:11 - INFO: Epoch 959 average loss: 0.038563
01:15:12 - INFO: Epoch 969 average loss: 0.040641
01:15:14 - INFO: Epoch 979 average loss: 0.043127
01:15:15 - INFO: Epoch 989 average loss: 0.042123
01:15:16 - INFO: Starting epoch 999:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0324]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0488]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0354]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.048]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0564]100%|██████████| 5/5 [00:00<00:00, 41.52it/s, MSE=0.0564]100%|██████████| 5/5 [00:00<00:00, 41.45it/s, MSE=0.0564]
01:15:16 - INFO: Epoch 999 average loss: 0.044188
01:15:17 - INFO: Total sampling 311 new convMatrix....
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... Now sampling 85st new convMatrix.... Now sampling 86st new convMatrix.... Now sampling 87st new convMatrix.... Now sampling 88st new convMatrix.... Now sampling 89st new convMatrix.... Now sampling 90st new convMatrix.... Now sampling 91st new convMatrix.... Now sampling 92st new convMatrix.... Now sampling 93st new convMatrix.... Now sampling 94st new convMatrix.... Now sampling 95st new convMatrix.... Now sampling 96st new convMatrix.... Now sampling 97st new convMatrix.... Now sampling 98st new convMatrix.... Now sampling 99st new convMatrix.... Now sampling 100st new convMatrix.... Now sampling 101st new convMatrix.... Now sampling 102st new convMatrix.... Now sampling 103st new convMatrix.... Now sampling 104st new convMatrix.... Now sampling 105st new convMatrix.... Now sampling 106st new convMatrix.... Now sampling 107st new convMatrix.... Now sampling 108st new convMatrix.... Now sampling 109st new convMatrix.... Now sampling 110st new convMatrix.... Now sampling 111st new convMatrix.... Now sampling 112st new convMatrix.... Now sampling 113st new convMatrix.... Now sampling 114st new convMatrix.... Now sampling 115st new convMatrix.... Now sampling 116st new convMatrix.... Now sampling 117st new convMatrix.... Now sampling 118st new convMatrix.... Now sampling 119st new convMatrix.... Now sampling 120st new convMatrix.... Now sampling 121st new convMatrix.... Now sampling 122st new convMatrix.... Now sampling 123st new convMatrix.... Now sampling 124st new convMatrix.... Now sampling 125st new convMatrix.... Now sampling 126st new convMatrix.... Now sampling 127st new convMatrix.... Now sampling 128st new convMatrix.... Now sampling 129st new convMatrix.... Now sampling 130st new convMatrix.... Now sampling 131st new convMatrix.... Now sampling 132st new convMatrix.... Now sampling 133st new convMatrix.... Now sampling 134st new convMatrix.... Now sampling 135st new convMatrix.... Now sampling 136st new convMatrix.... Now sampling 137st new convMatrix.... Now sampling 138st new convMatrix.... Now sampling 139st new convMatrix.... Now sampling 140st new convMatrix.... Now sampling 141st new convMatrix.... Now sampling 142st new convMatrix.... Now sampling 143st new convMatrix.... Now sampling 144st new convMatrix.... Now sampling 145st new convMatrix.... Now sampling 146st new convMatrix.... Now sampling 147st new convMatrix.... Now sampling 148st new convMatrix.... Now sampling 149st new convMatrix.... Now sampling 150st new convMatrix.... Now sampling 151st new convMatrix.... Now sampling 152st new convMatrix.... Now sampling 153st new convMatrix.... Now sampling 154st new convMatrix.... Now sampling 155st new convMatrix.... Now sampling 156st new convMatrix.... Now sampling 157st new convMatrix.... Now sampling 158st new convMatrix.... Now sampling 159st new convMatrix.... Now sampling 160st new convMatrix.... Now sampling 161st new convMatrix.... Now sampling 162st new convMatrix.... Now sampling 163st new convMatrix.... Now sampling 164st new convMatrix.... Now sampling 165st new convMatrix.... Now sampling 166st new convMatrix.... Now sampling 167st new convMatrix.... Now sampling 168st new convMatrix.... Now sampling 169st new convMatrix.... Now sampling 170st new convMatrix.... Now sampling 171st new convMatrix.... Now sampling 172st new convMatrix.... Now sampling 173st new convMatrix.... Now sampling 174st new convMatrix.... Now sampling 175st new convMatrix.... Now sampling 176st new convMatrix.... Now sampling 177st new convMatrix.... Now sampling 178st new convMatrix.... Now sampling 179st new convMatrix.... Now sampling 180st new convMatrix.... Now sampling 181st new convMatrix.... Now sampling 182st new convMatrix.... Now sampling 183st new convMatrix.... Now sampling 184st new convMatrix.... Now sampling 185st new convMatrix.... Now sampling 186st new convMatrix.... Now sampling 187st new convMatrix.... Now sampling 188st new convMatrix.... Now sampling 189st new convMatrix.... Now sampling 190st new convMatrix.... Now sampling 191st new convMatrix.... Now sampling 192st new convMatrix.... Now sampling 193st new convMatrix.... Now sampling 194st new convMatrix.... Now sampling 195st new convMatrix.... Now sampling 196st new convMatrix.... Now sampling 197st new convMatrix.... Now sampling 198st new convMatrix.... Now sampling 199st new convMatrix.... Now sampling 200st new convMatrix.... Now sampling 201st new convMatrix.... Now sampling 202st new convMatrix.... Now sampling 203st new convMatrix.... Now sampling 204st new convMatrix.... Now sampling 205st new convMatrix.... Now sampling 206st new convMatrix.... Now sampling 207st new convMatrix.... Now sampling 208st new convMatrix.... Now sampling 209st new convMatrix.... /home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
Now sampling 210st new convMatrix.... Now sampling 211st new convMatrix.... Now sampling 212st new convMatrix.... Now sampling 213st new convMatrix.... Now sampling 214st new convMatrix.... Now sampling 215st new convMatrix.... Now sampling 216st new convMatrix.... Now sampling 217st new convMatrix.... Now sampling 218st new convMatrix.... Now sampling 219st new convMatrix.... Now sampling 220st new convMatrix.... Now sampling 221st new convMatrix.... Now sampling 222st new convMatrix.... Now sampling 223st new convMatrix.... Now sampling 224st new convMatrix.... Now sampling 225st new convMatrix.... Now sampling 226st new convMatrix.... Now sampling 227st new convMatrix.... Now sampling 228st new convMatrix.... Now sampling 229st new convMatrix.... Now sampling 230st new convMatrix.... Now sampling 231st new convMatrix.... Now sampling 232st new convMatrix.... Now sampling 233st new convMatrix.... Now sampling 234st new convMatrix.... Now sampling 235st new convMatrix.... Now sampling 236st new convMatrix.... Now sampling 237st new convMatrix.... Now sampling 238st new convMatrix.... Now sampling 239st new convMatrix.... Now sampling 240st new convMatrix.... Now sampling 241st new convMatrix.... Now sampling 242st new convMatrix.... Now sampling 243st new convMatrix.... Now sampling 244st new convMatrix.... Now sampling 245st new convMatrix.... Now sampling 246st new convMatrix.... Now sampling 247st new convMatrix.... Now sampling 248st new convMatrix.... Now sampling 249st new convMatrix.... Now sampling 250st new convMatrix.... Now sampling 251st new convMatrix.... Now sampling 252st new convMatrix.... Now sampling 253st new convMatrix.... Now sampling 254st new convMatrix.... Now sampling 255st new convMatrix.... Now sampling 256st new convMatrix.... Now sampling 257st new convMatrix.... Now sampling 258st new convMatrix.... Now sampling 259st new convMatrix.... Now sampling 260st new convMatrix.... Now sampling 261st new convMatrix.... Now sampling 262st new convMatrix.... Now sampling 263st new convMatrix.... Now sampling 264st new convMatrix.... Now sampling 265st new convMatrix.... Now sampling 266st new convMatrix.... Now sampling 267st new convMatrix.... Now sampling 268st new convMatrix.... Now sampling 269st new convMatrix.... Now sampling 270st new convMatrix.... Now sampling 271st new convMatrix.... Now sampling 272st new convMatrix.... Now sampling 273st new convMatrix.... Now sampling 274st new convMatrix.... Now sampling 275st new convMatrix.... Now sampling 276st new convMatrix.... Now sampling 277st new convMatrix.... Now sampling 278st new convMatrix.... Now sampling 279st new convMatrix.... Now sampling 280st new convMatrix.... Now sampling 281st new convMatrix.... Now sampling 282st new convMatrix.... Now sampling 283st new convMatrix.... Now sampling 284st new convMatrix.... Now sampling 285st new convMatrix.... Now sampling 286st new convMatrix.... Now sampling 287st new convMatrix.... Now sampling 288st new convMatrix.... Now sampling 289st new convMatrix.... Now sampling 290st new convMatrix.... Now sampling 291st new convMatrix.... Now sampling 292st new convMatrix.... Now sampling 293st new convMatrix.... Now sampling 294st new convMatrix.... Now sampling 295st new convMatrix.... Now sampling 296st new convMatrix.... Now sampling 297st new convMatrix.... Now sampling 298st new convMatrix.... Now sampling 299st new convMatrix.... Now sampling 300st new convMatrix.... Now sampling 301st new convMatrix.... Now sampling 302st new convMatrix.... Now sampling 303st new convMatrix.... Now sampling 304st new convMatrix.... Now sampling 305st new convMatrix.... Now sampling 306st new convMatrix.... Now sampling 307st new convMatrix.... Now sampling 308st new convMatrix.... Now sampling 309st new convMatrix.... Now sampling 310st new convMatrix.... torch.Size([311, 16])
feature shape after [Data Augmentation]:  (624, 16)
(624, 16)
====>MLP training... Epoch: 20 total loss: 0.8548
====>MLP training... Epoch: 40 total loss: 0.1276
====>MLP training... Epoch: 60 total loss: 0.0474
====>MLP training... Epoch: 80 total loss: 0.0346
====>MLP training... Epoch: 100 total loss: 0.0353
====>MLP training... Epoch: 120 total loss: 0.0317
====>MLP training... Epoch: 140 total loss: 0.0291
====>MLP training... Epoch: 160 total loss: 0.0264
====>MLP training... Epoch: 180 total loss: 0.0274
====>MLP training... Epoch: 200 total loss: 0.0257
====>CNN training... Epoch: 0  total loss: 0.2524
====>CNN training... Epoch: 20  total loss: 0.2517
====>CNN training... Epoch: 40  total loss: 0.2508
====>CNN training... Epoch: 60  total loss: 0.2501
====>CNN training... Epoch: 80  total loss: 0.2498
====>CNN training... Epoch: 100  total loss: 0.2496
====>CNN training... Epoch: 120  total loss: 0.2495
====>CNN training... Epoch: 140  total loss: 0.2491
====>CNN training... Epoch: 160  total loss: 0.2485
====>CNN training... Epoch: 180  total loss: 0.2483
====>CNN training... Epoch: 200  total loss: 0.2477
====>RNN training... Epoch: 20 total loss: 0.2252
====>RNN training... Epoch: 40 total loss: 0.2133
====>RNN training... Epoch: 60 total loss: 0.1996
====>RNN training... Epoch: 80 total loss: 0.1842
====>RNN training... Epoch: 100 total loss: 0.1665
====>RNN training... Epoch: 120 total loss: 0.1465
====>RNN training... Epoch: 140 total loss: 0.1253
====>RNN training... Epoch: 160 total loss: 0.1043
====>RNN training... Epoch: 180 total loss: 0.0854
====>RNN training... Epoch: 200 total loss: 0.0694

test: Lang-15-fs_ddpm
fault line:  [1000219, 1000220, 1000221, 1000675]
feature shape before [Data Augmentation]:  (10, 2173)
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 2117, 2118, 78, 79, 80, 81, 2051, 2131, 2132, 2052, 2133, 2134, 88, 89, 2135, 91, 92, 2136, 94, 95, 2054, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 2152, 114, 2154, 116, 2156, 118, 120, 122, 124, 126, 128, 129, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 152, 160, 161, 163, 164, 165, 168, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 209, 210, 211, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 2090, 2093, 2094, 2095, 2096, 2097, 2100, 358, 359, 361, 363, 364, 365, 366, 368, 372, 374, 394, 395, 396, 397, 398, 2055, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 2138, 2139, 2140, 2137, 2141, 2142, 2143, 548, 2144, 550, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2153, 2155, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1766, 1767, 1771, 1773, 1775, 1776, 1777, 1778, 1779, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1796, 1798, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1821, 1823, 1825, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1865, 1870, 1872, 1873, 1874, 1875, 1876, 1890, 1891, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1909, 1910, 1912, 1913, 1914, 1915, 1916, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1952, 1953, 1954, 1955, 1956, 1960, 1961, 1974, 1975, 1976, 1977, 1986, 1987, 1989, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045]
entropy 214.48457482982042 46.00723840525797
req_shape:  16
statements selected: 128
statements selected: /home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
01:23:57 - INFO: Epoch 9 average loss: 0.170670
01:23:57 - INFO: Epoch 19 average loss: 0.046845
01:23:57 - INFO: Epoch 29 average loss: 0.072569
01:23:58 - INFO: Epoch 39 average loss: 0.065365
01:23:58 - INFO: Epoch 49 average loss: 0.069362
01:23:59 - INFO: Epoch 59 average loss: 0.069638
01:23:59 - INFO: Epoch 69 average loss: 0.056285
01:24:00 - INFO: Epoch 79 average loss: 0.065821
01:24:00 - INFO: Epoch 89 average loss: 0.145068
01:24:01 - INFO: Starting epoch 99:
Index([ 100089,  100111,  100129,  100147,  100165,  100186,  100204,  100222,
        100240,  100259,
       ...
       1100051, 1100052, 1100056, 1100057, 1100058, 1100065, 1100066, 1100070,
       1100071, 1700422],
      dtype='int64', length=128)
Dataset shape: torch.Size([10, 1, 1, 128]) torch.Size([10])
Batch Dataloader shape: torch.Size([8, 1, 1, 128]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0462]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0171]100%|██████████| 2/2 [00:00<00:00, 40.39it/s, MSE=0.0171]
01:24:01 - INFO: Epoch 99 average loss: 0.031682
01:24:01 - INFO: Epoch 109 average loss: 0.147603
01:24:02 - INFO: Epoch 119 average loss: 0.196484
01:24:02 - INFO: Epoch 129 average loss: 0.049482
01:24:03 - INFO: Epoch 139 average loss: 0.071020
01:24:03 - INFO: Epoch 149 average loss: 0.055858
01:24:03 - INFO: Epoch 159 average loss: 0.031318
01:24:04 - INFO: Epoch 169 average loss: 0.162364
01:24:05 - INFO: Epoch 179 average loss: 0.078672
01:24:05 - INFO: Epoch 189 average loss: 0.028458
01:24:05 - INFO: Starting epoch 199:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.151]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0103]100%|██████████| 2/2 [00:00<00:00, 43.94it/s, MSE=0.0103]
01:24:05 - INFO: Epoch 199 average loss: 0.080752
01:24:06 - INFO: Epoch 209 average loss: 0.018649
01:24:07 - INFO: Epoch 219 average loss: 0.034131
01:24:07 - INFO: Epoch 229 average loss: 0.053582
01:24:07 - INFO: Epoch 239 average loss: 0.027706
01:24:08 - INFO: Epoch 249 average loss: 0.075564
01:24:08 - INFO: Epoch 259 average loss: 0.056834
01:24:09 - INFO: Epoch 269 average loss: 0.018664
01:24:09 - INFO: Epoch 279 average loss: 0.068369
01:24:10 - INFO: Epoch 289 average loss: 0.092866
01:24:10 - INFO: Starting epoch 299:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0313]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0325]100%|██████████| 2/2 [00:00<00:00, 31.03it/s, MSE=0.0325]
01:24:10 - INFO: Epoch 299 average loss: 0.031915
01:24:11 - INFO: Epoch 309 average loss: 0.020479
01:24:11 - INFO: Epoch 319 average loss: 0.066803
01:24:12 - INFO: Epoch 329 average loss: 0.113229
01:24:12 - INFO: Epoch 339 average loss: 0.080258
01:24:13 - INFO: Epoch 349 average loss: 0.047291
01:24:13 - INFO: Epoch 359 average loss: 0.029899
01:24:14 - INFO: Epoch 369 average loss: 0.069466
01:24:14 - INFO: Epoch 379 average loss: 0.024014
01:24:15 - INFO: Epoch 389 average loss: 0.092722
01:24:15 - INFO: Starting epoch 399:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.117]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.026]100%|██████████| 2/2 [00:00<00:00, 47.44it/s, MSE=0.026]
01:24:15 - INFO: Epoch 399 average loss: 0.071548
01:24:16 - INFO: Epoch 409 average loss: 0.117541
01:24:16 - INFO: Epoch 419 average loss: 0.037647
01:24:16 - INFO: Epoch 429 average loss: 0.061283
01:24:17 - INFO: Epoch 439 average loss: 0.030954
01:24:17 - INFO: Epoch 449 average loss: 0.056678
01:24:18 - INFO: Epoch 459 average loss: 0.056664
01:24:18 - INFO: Epoch 469 average loss: 0.050531
01:24:19 - INFO: Epoch 479 average loss: 0.063004
01:24:19 - INFO: Epoch 489 average loss: 0.035460
01:24:20 - INFO: Starting epoch 499:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0191]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0168]100%|██████████| 2/2 [00:00<00:00, 39.03it/s, MSE=0.0168]
01:24:20 - INFO: Epoch 499 average loss: 0.017951
01:24:20 - INFO: Epoch 509 average loss: 0.112077
01:24:21 - INFO: Epoch 519 average loss: 0.035515
01:24:21 - INFO: Epoch 529 average loss: 0.098223
01:24:22 - INFO: Epoch 539 average loss: 0.039714
01:24:22 - INFO: Epoch 549 average loss: 0.036520
01:24:23 - INFO: Epoch 559 average loss: 0.155856
01:24:23 - INFO: Epoch 569 average loss: 0.056908
01:24:23 - INFO: Epoch 579 average loss: 0.047403
01:24:24 - INFO: Epoch 589 average loss: 0.045313
01:24:24 - INFO: Starting epoch 599:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0532]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.00798]100%|██████████| 2/2 [00:00<00:00, 49.62it/s, MSE=0.00798]
01:24:24 - INFO: Epoch 599 average loss: 0.030604
01:24:25 - INFO: Epoch 609 average loss: 0.083122
01:24:26 - INFO: Epoch 619 average loss: 0.020181
01:24:26 - INFO: Epoch 629 average loss: 0.038645
01:24:26 - INFO: Epoch 639 average loss: 0.079157
01:24:27 - INFO: Epoch 649 average loss: 0.116346
01:24:27 - INFO: Epoch 659 average loss: 0.036560
01:24:28 - INFO: Epoch 669 average loss: 0.055589
01:24:28 - INFO: Epoch 679 average loss: 0.028671
01:24:29 - INFO: Epoch 689 average loss: 0.069988
01:24:29 - INFO: Starting epoch 699:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0731]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0164]100%|██████████| 2/2 [00:00<00:00, 29.37it/s, MSE=0.0164]
01:24:29 - INFO: Epoch 699 average loss: 0.044773
01:24:30 - INFO: Epoch 709 average loss: 0.036022
01:24:30 - INFO: Epoch 719 average loss: 0.068797
01:24:31 - INFO: Epoch 729 average loss: 0.029830
01:24:31 - INFO: Epoch 739 average loss: 0.019014
01:24:32 - INFO: Epoch 749 average loss: 0.044392
01:24:32 - INFO: Epoch 759 average loss: 0.034149
01:24:32 - INFO: Epoch 769 average loss: 0.080409
01:24:33 - INFO: Epoch 779 average loss: 0.040542
01:24:33 - INFO: Epoch 789 average loss: 0.025688
01:24:33 - INFO: Starting epoch 799:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0969]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0232]100%|██████████| 2/2 [00:00<00:00, 47.72it/s, MSE=0.0232]
01:24:33 - INFO: Epoch 799 average loss: 0.060040
01:24:34 - INFO: Epoch 809 average loss: 0.094778
01:24:34 - INFO: Epoch 819 average loss: 0.045289
01:24:35 - INFO: Epoch 829 average loss: 0.015484
01:24:35 - INFO: Epoch 839 average loss: 0.036178
01:24:36 - INFO: Epoch 849 average loss: 0.026568
01:24:36 - INFO: Epoch 859 average loss: 0.060048
01:24:37 - INFO: Epoch 869 average loss: 0.041158
01:24:37 - INFO: Epoch 879 average loss: 0.034816
01:24:38 - INFO: Epoch 889 average loss: 0.032828
01:24:38 - INFO: Starting epoch 899:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0408]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0167]100%|██████████| 2/2 [00:00<00:00, 37.32it/s, MSE=0.0167]
01:24:38 - INFO: Epoch 899 average loss: 0.028746
01:24:39 - INFO: Epoch 909 average loss: 0.025345
01:24:39 - INFO: Epoch 919 average loss: 0.049343
01:24:39 - INFO: Epoch 929 average loss: 0.052871
01:24:40 - INFO: Epoch 939 average loss: 0.010578
01:24:40 - INFO: Epoch 949 average loss: 0.065310
01:24:41 - INFO: Epoch 959 average loss: 0.034276
01:24:41 - INFO: Epoch 969 average loss: 0.039744
01:24:42 - INFO: Epoch 979 average loss: 0.206513
01:24:42 - INFO: Epoch 989 average loss: 0.053254
01:24:42 - INFO: Starting epoch 999:
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0342]  0%|          | 0/2 [00:00<?, ?it/s, MSE=0.0154]100%|██████████| 2/2 [00:00<00:00, 42.98it/s, MSE=0.0154]
01:24:42 - INFO: Epoch 999 average loss: 0.024814
01:24:43 - INFO: Total sampling 6 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
01:25:06 - INFO: Epoch 9 average loss: 0.150104
01:25:07 - INFO: Epoch 19 average loss: 0.180267
01:25:09 - INFO: Epoch 29 average loss: 0.118478
01:25:10 - INFO: Epoch 39 average loss: 0.107683
01:25:12 - INFO: Epoch 49 average loss: 0.100821
01:25:13 - INFO: Epoch 59 average loss: 0.121407
01:25:15 - INFO: Epoch 69 average loss: 0.098196
01:25:16 - INFO: Epoch 79 average loss: 0.087719
01:25:17 - INFO: Epoch 89 average loss: 0.073065
01:25:18 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... torch.Size([6, 128])
feature shape after [Data Augmentation]:  (16, 128)
(16, 128)
====>MLP training... Epoch: 20 total loss: 0.4645
====>MLP training... Epoch: 40 total loss: 0.3858
====>MLP training... Epoch: 60 total loss: 0.2453
====>MLP training... Epoch: 80 total loss: 0.1442
====>MLP training... Epoch: 100 total loss: 0.1320
====>MLP training... Epoch: 120 total loss: 0.1114
====>MLP training... Epoch: 140 total loss: 0.1039
====>MLP training... Epoch: 160 total loss: 0.0975
====>MLP training... Epoch: 180 total loss: 0.0855
====>MLP training... Epoch: 200 total loss: 0.0709
====>CNN training... Epoch: 0  total loss: 0.2516
====>CNN training... Epoch: 20  total loss: 0.2468
====>CNN training... Epoch: 40  total loss: 0.2377
====>CNN training... Epoch: 60  total loss: 0.2147
====>CNN training... Epoch: 80  total loss: 0.1717
====>CNN training... Epoch: 100  total loss: 0.1058
====>CNN training... Epoch: 120  total loss: 0.0614
====>CNN training... Epoch: 140  total loss: 0.0298
====>CNN training... Epoch: 160  total loss: 0.0215
====>CNN training... Epoch: 180  total loss: 0.0213
====>CNN training... Epoch: 200  total loss: 0.0238
====>RNN training... Epoch: 20 total loss: 0.2297
====>RNN training... Epoch: 40 total loss: 0.2018
====>RNN training... Epoch: 60 total loss: 0.1777
====>RNN training... Epoch: 80 total loss: 0.1550
====>RNN training... Epoch: 100 total loss: 0.1337
====>RNN training... Epoch: 120 total loss: 0.1141
====>RNN training... Epoch: 140 total loss: 0.0969
====>RNN training... Epoch: 160 total loss: 0.0824
====>RNN training... Epoch: 180 total loss: 0.0706
====>RNN training... Epoch: 200 total loss: 0.0610

test: Lang-16-fs_ddpm
fault line:  [300458]
feature shape before [Data Augmentation]:  (82, 1786)
[512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 530, 531, 536, 537, 540, 541, 543, 545, 546, 547, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 188, 190, 192, 194, 195, 196, 197, 198, 199, 200, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 224, 225, 226, 229, 230, 231, 236, 237, 239, 240, 241, 242, 243, 246, 247, 248, 249, 258, 260, 261, 262, 264, 266, 267, 269, 270, 272, 273, 275, 444, 446, 447, 449, 508, 509, 510, 511]
entropy 14.881168582811766 2.0369581686283826
req_shape:  16
statements selected: 16
statements selected: Index([300033, 300035, 300037, 300039, 300041, 300043, 300045, 300047, 300049,
       300051, 300053, 300055, 300059, 300067, 300554, 300574],
      dtype='int64')
Dataset shape: torch.Size([82, 1, 1, 16]) torch.Size([82])
Batch Dataloader shape: torch.Size([16, 1, 1, 16]) torch.Size([16])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0482]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0544]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0906]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0686]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0944] 83%|████████▎ | 5/6 [00:00<00:00, 49.68it/s, MSE=0.0944] 83%|████████▎ | 5/6 [00:00<00:00, 49.68it/s, MSE=0.111] 100%|██████████| 6/6 [00:00<00:00, 50.68it/s, MSE=0.111]
01:25:18 - INFO: Epoch 99 average loss: 0.077812
01:25:20 - INFO: Epoch 109 average loss: 0.081540
01:25:21 - INFO: Epoch 119 average loss: 0.073753
01:25:23 - INFO: Epoch 129 average loss: 0.047023
01:25:24 - INFO: Epoch 139 average loss: 0.080007
01:25:25 - INFO: Epoch 149 average loss: 0.063513
01:25:27 - INFO: Epoch 159 average loss: 0.051945
01:25:28 - INFO: Epoch 169 average loss: 0.066877
01:25:29 - INFO: Epoch 179 average loss: 0.063480
01:25:30 - INFO: Epoch 189 average loss: 0.050158
01:25:32 - INFO: Starting epoch 199:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0574]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0284]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0678]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0389]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.116]  83%|████████▎ | 5/6 [00:00<00:00, 44.83it/s, MSE=0.116] 83%|████████▎ | 5/6 [00:00<00:00, 44.83it/s, MSE=0.122]100%|██████████| 6/6 [00:00<00:00, 46.72it/s, MSE=0.122]
01:25:32 - INFO: Epoch 199 average loss: 0.071752
01:25:33 - INFO: Epoch 209 average loss: 0.050320
01:25:35 - INFO: Epoch 219 average loss: 0.087597
01:25:36 - INFO: Epoch 229 average loss: 0.060234
01:25:38 - INFO: Epoch 239 average loss: 0.050165
01:25:39 - INFO: Epoch 249 average loss: 0.088503
01:25:40 - INFO: Epoch 259 average loss: 0.068174
01:25:42 - INFO: Epoch 269 average loss: 0.049641
01:25:43 - INFO: Epoch 279 average loss: 0.057081
01:25:44 - INFO: Epoch 289 average loss: 0.057494
01:25:45 - INFO: Starting epoch 299:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0983]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0585]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0562]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0697]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0464] 83%|████████▎ | 5/6 [00:00<00:00, 48.88it/s, MSE=0.0464] 83%|████████▎ | 5/6 [00:00<00:00, 48.88it/s, MSE=0.0274]100%|██████████| 6/6 [00:00<00:00, 46.81it/s, MSE=0.0274]
01:25:46 - INFO: Epoch 299 average loss: 0.059412
01:25:47 - INFO: Epoch 309 average loss: 0.058161
01:25:49 - INFO: Epoch 319 average loss: 0.060424
01:25:50 - INFO: Epoch 329 average loss: 0.051156
01:25:51 - INFO: Epoch 339 average loss: 0.047372
01:25:53 - INFO: Epoch 349 average loss: 0.054734
01:25:54 - INFO: Epoch 359 average loss: 0.063063
01:25:55 - INFO: Epoch 369 average loss: 0.053824
01:25:57 - INFO: Epoch 379 average loss: 0.048362
01:25:58 - INFO: Epoch 389 average loss: 0.069110
01:25:59 - INFO: Starting epoch 399:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0325]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0456]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0432]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0481]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0262] 83%|████████▎ | 5/6 [00:00<00:00, 38.31it/s, MSE=0.0262] 83%|████████▎ | 5/6 [00:00<00:00, 38.31it/s, MSE=0.0185]100%|██████████| 6/6 [00:00<00:00, 37.75it/s, MSE=0.0185]
01:26:00 - INFO: Epoch 399 average loss: 0.035687
01:26:01 - INFO: Epoch 409 average loss: 0.040263
01:26:02 - INFO: Epoch 419 average loss: 0.064107
01:26:04 - INFO: Epoch 429 average loss: 0.059571
01:26:05 - INFO: Epoch 439 average loss: 0.058419
01:26:07 - INFO: Epoch 449 average loss: 0.094103
01:26:08 - INFO: Epoch 459 average loss: 0.054765
01:26:10 - INFO: Epoch 469 average loss: 0.055417
01:26:11 - INFO: Epoch 479 average loss: 0.061575
01:26:12 - INFO: Epoch 489 average loss: 0.069524
01:26:13 - INFO: Starting epoch 499:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0679]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0203]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0479]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0723]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0907] 83%|████████▎ | 5/6 [00:00<00:00, 46.92it/s, MSE=0.0907] 83%|████████▎ | 5/6 [00:00<00:00, 46.92it/s, MSE=0.0532]100%|██████████| 6/6 [00:00<00:00, 45.61it/s, MSE=0.0532]
01:26:14 - INFO: Epoch 499 average loss: 0.058710
01:26:15 - INFO: Epoch 509 average loss: 0.064145
01:26:16 - INFO: Epoch 519 average loss: 0.059893
01:26:17 - INFO: Epoch 529 average loss: 0.072986
01:26:19 - INFO: Epoch 539 average loss: 0.068545
01:26:20 - INFO: Epoch 549 average loss: 0.083162
01:26:21 - INFO: Epoch 559 average loss: 0.049064
01:26:23 - INFO: Epoch 569 average loss: 0.048119
01:26:24 - INFO: Epoch 579 average loss: 0.062919
01:26:25 - INFO: Epoch 589 average loss: 0.066617
01:26:26 - INFO: Starting epoch 599:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.065]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0498]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0686]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.106]  67%|██████▋   | 4/6 [00:00<00:00, 34.50it/s, MSE=0.106] 67%|██████▋   | 4/6 [00:00<00:00, 34.50it/s, MSE=0.0395] 67%|██████▋   | 4/6 [00:00<00:00, 34.50it/s, MSE=0.0828]100%|██████████| 6/6 [00:00<00:00, 36.25it/s, MSE=0.0828]
01:26:27 - INFO: Epoch 599 average loss: 0.068611
01:26:28 - INFO: Epoch 609 average loss: 0.062865
01:26:30 - INFO: Epoch 619 average loss: 0.044582
01:26:31 - INFO: Epoch 629 average loss: 0.040833
01:26:32 - INFO: Epoch 639 average loss: 0.062669
01:26:34 - INFO: Epoch 649 average loss: 0.068491
01:26:35 - INFO: Epoch 659 average loss: 0.035250
01:26:37 - INFO: Epoch 669 average loss: 0.042098
01:26:38 - INFO: Epoch 679 average loss: 0.068100
01:26:39 - INFO: Epoch 689 average loss: 0.056950
01:26:40 - INFO: Starting epoch 699:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0818]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0562]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0458]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0467]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0439]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.00176]100%|██████████| 6/6 [00:00<00:00, 55.42it/s, MSE=0.00176]100%|██████████| 6/6 [00:00<00:00, 55.29it/s, MSE=0.00176]
01:26:41 - INFO: Epoch 699 average loss: 0.046026
01:26:42 - INFO: Epoch 709 average loss: 0.040018
01:26:43 - INFO: Epoch 719 average loss: 0.059338
01:26:45 - INFO: Epoch 729 average loss: 0.055060
01:26:46 - INFO: Epoch 739 average loss: 0.046013
01:26:48 - INFO: Epoch 749 average loss: 0.057113
01:26:49 - INFO: Epoch 759 average loss: 0.045699
01:26:50 - INFO: Epoch 769 average loss: 0.058102
01:26:51 - INFO: Epoch 779 average loss: 0.043393
01:26:53 - INFO: Epoch 789 average loss: 0.045353
01:26:54 - INFO: Starting epoch 799:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0478]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0646]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0723]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0318]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0467] 83%|████████▎ | 5/6 [00:00<00:00, 43.23it/s, MSE=0.0467] 83%|████████▎ | 5/6 [00:00<00:00, 43.23it/s, MSE=0.0253]100%|██████████| 6/6 [00:00<00:00, 43.46it/s, MSE=0.0253]
01:26:54 - INFO: Epoch 799 average loss: 0.048076
01:26:56 - INFO: Epoch 809 average loss: 0.066588
01:26:57 - INFO: Epoch 819 average loss: 0.070345
01:26:59 - INFO: Epoch 829 average loss: 0.044361
01:27:00 - INFO: Epoch 839 average loss: 0.056818
01:27:01 - INFO: Epoch 849 average loss: 0.052200
01:27:03 - INFO: Epoch 859 average loss: 0.050492
01:27:04 - INFO: Epoch 869 average loss: 0.048079
01:27:05 - INFO: Epoch 879 average loss: 0.042209
01:27:07 - INFO: Epoch 889 average loss: 0.036346
01:27:08 - INFO: Starting epoch 899:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.035]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0548]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0359]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0654]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0497] 83%|████████▎ | 5/6 [00:00<00:00, 42.40it/s, MSE=0.0497] 83%|████████▎ | 5/6 [00:00<00:00, 42.40it/s, MSE=0.0868]100%|██████████| 6/6 [00:00<00:00, 43.41it/s, MSE=0.0868]
01:27:08 - INFO: Epoch 899 average loss: 0.054603
01:27:09 - INFO: Epoch 909 average loss: 0.038191
01:27:11 - INFO: Epoch 919 average loss: 0.066891
01:27:12 - INFO: Epoch 929 average loss: 0.041023
01:27:13 - INFO: Epoch 939 average loss: 0.050433
01:27:15 - INFO: Epoch 949 average loss: 0.048307
01:27:16 - INFO: Epoch 959 average loss: 0.083156
01:27:17 - INFO: Epoch 969 average loss: 0.064284
01:27:19 - INFO: Epoch 979 average loss: 0.085459
01:27:20 - INFO: Epoch 989 average loss: 0.063761
01:27:21 - INFO: Starting epoch 999:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.041]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.067]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0581]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0456]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0227]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0109]100%|██████████| 6/6 [00:00<00:00, 46.95it/s, MSE=0.0109]100%|██████████| 6/6 [00:00<00:00, 46.71it/s, MSE=0.0109]
01:27:21 - INFO: Epoch 999 average loss: 0.040875
01:27:22 - INFO: Total sampling 80 new convMatrix....
01:29:51 - INFO: Epoch 9 average loss: 0.185003
01:29:52 - INFO: Epoch 19 average loss: 0.242181
01:29:53 - INFO: Epoch 29 average loss: 0.225941
01:29:54 - INFO: Epoch 39 average loss: 0.087562
01:29:55 - INFO: Epoch 49 average loss: 0.199041
01:29:56 - INFO: Epoch 59 average loss: 0.226183
01:29:57 - INFO: Epoch 69 average loss: 0.188757
01:29:58 - INFO: Epoch 79 average loss: 0.173675
01:29:59 - INFO: Epoch 89 average loss: 0.146506
01:30:00 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... torch.Size([80, 16])
feature shape after [Data Augmentation]:  (162, 16)
(162, 16)
====>MLP training... Epoch: 20 total loss: 1.3679
====>MLP training... Epoch: 40 total loss: 0.1579
====>MLP training... Epoch: 60 total loss: 0.0760
====>MLP training... Epoch: 80 total loss: 0.0627
====>MLP training... Epoch: 100 total loss: 0.0651
====>MLP training... Epoch: 120 total loss: 0.0519
====>MLP training... Epoch: 140 total loss: 0.0503
====>MLP training... Epoch: 160 total loss: 0.0549
====>MLP training... Epoch: 180 total loss: 0.0541
====>MLP training... Epoch: 200 total loss: 0.0518
====>CNN training... Epoch: 0  total loss: 0.2494
====>CNN training... Epoch: 20  total loss: 0.2478
====>CNN training... Epoch: 40  total loss: 0.2470
====>CNN training... Epoch: 60  total loss: 0.2493
====>CNN training... Epoch: 80  total loss: 0.2400
====>CNN training... Epoch: 100  total loss: 0.2136
====>CNN training... Epoch: 120  total loss: 0.1869
====>CNN training... Epoch: 140  total loss: 0.1759
====>CNN training... Epoch: 160  total loss: 0.1635
====>CNN training... Epoch: 180  total loss: 0.1644
====>CNN training... Epoch: 200  total loss: 0.1487
====>RNN training... Epoch: 20 total loss: 0.2445
====>RNN training... Epoch: 40 total loss: 0.2417
====>RNN training... Epoch: 60 total loss: 0.2389
====>RNN training... Epoch: 80 total loss: 0.2362
====>RNN training... Epoch: 100 total loss: 0.2333
====>RNN training... Epoch: 120 total loss: 0.2303
====>RNN training... Epoch: 140 total loss: 0.2270
====>RNN training... Epoch: 160 total loss: 0.2233
====>RNN training... Epoch: 180 total loss: 0.2191
====>RNN training... Epoch: 200 total loss: 0.2144

test: Lang-17-fs_ddpm
fault line:  [300083, 300090, 300094, 300096, 300097, 300098, 300099, 300100, 300102, 300089]
feature shape before [Data Augmentation]:  (36, 1255)
[1024, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1026, 44, 1066, 46, 1067, 1027, 1068, 1069, 1028, 1078, 1082, 1083, 1084, 1085, 1086, 1087, 1089, 1090, 1091, 1031, 1092, 1033, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1035, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1131, 1132, 1169, 1170, 1171, 1172, 1173, 1176, 1193, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1217, 1218, 1219, 1220, 1223, 1224, 1242, 1243, 1010, 1011, 1012, 1023]
entropy 81.1254843930771 10.441257012722723
req_shape:  16
statements selected: 16
statements selected: Index([ 100046,  100050,  100054,  100058,  100062,  100066,  300087,  300088,
        300089,  400183, 1000051, 1000052, 1000085, 1200049, 1200072, 1400036],
      dtype='int64')
Dataset shape: torch.Size([36, 1, 1, 16]) torch.Size([36])
Batch Dataloader shape: torch.Size([8, 1, 1, 16]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.127]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.124]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0762]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0723]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.239] 100%|██████████| 5/5 [00:00<00:00, 45.47it/s, MSE=0.239]100%|██████████| 5/5 [00:00<00:00, 45.27it/s, MSE=0.239]
01:30:00 - INFO: Epoch 99 average loss: 0.127742
01:30:02 - INFO: Epoch 109 average loss: 0.121937
01:30:03 - INFO: Epoch 119 average loss: 0.164189
01:30:04 - INFO: Epoch 129 average loss: 0.094698
01:30:05 - INFO: Epoch 139 average loss: 0.121214
01:30:06 - INFO: Epoch 149 average loss: 0.178775
01:30:07 - INFO: Epoch 159 average loss: 0.157037
01:30:08 - INFO: Epoch 169 average loss: 0.113382
01:30:10 - INFO: Epoch 179 average loss: 0.143991
01:30:11 - INFO: Epoch 189 average loss: 0.153242
01:30:12 - INFO: Starting epoch 199:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.173]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.129]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.12]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.124] 80%|████████  | 4/5 [00:00<00:00, 38.95it/s, MSE=0.124] 80%|████████  | 4/5 [00:00<00:00, 38.95it/s, MSE=0.0584]100%|██████████| 5/5 [00:00<00:00, 38.74it/s, MSE=0.0584]
01:30:12 - INFO: Epoch 199 average loss: 0.121001
01:30:13 - INFO: Epoch 209 average loss: 0.121295
01:30:15 - INFO: Epoch 219 average loss: 0.129796
01:30:16 - INFO: Epoch 229 average loss: 0.103263
01:30:17 - INFO: Epoch 239 average loss: 0.136187
01:30:18 - INFO: Epoch 249 average loss: 0.126796
01:30:19 - INFO: Epoch 259 average loss: 0.116334
01:30:20 - INFO: Epoch 269 average loss: 0.165210
01:30:21 - INFO: Epoch 279 average loss: 0.094349
01:30:23 - INFO: Epoch 289 average loss: 0.102470
01:30:24 - INFO: Starting epoch 299:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0661]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.104]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.101]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0714]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.058] 100%|██████████| 5/5 [00:00<00:00, 40.49it/s, MSE=0.058]100%|██████████| 5/5 [00:00<00:00, 40.34it/s, MSE=0.058]
01:30:24 - INFO: Epoch 299 average loss: 0.080198
01:30:25 - INFO: Epoch 309 average loss: 0.103358
01:30:26 - INFO: Epoch 319 average loss: 0.147474
01:30:28 - INFO: Epoch 329 average loss: 0.125680
01:30:29 - INFO: Epoch 339 average loss: 0.140845
01:30:30 - INFO: Epoch 349 average loss: 0.097815
01:30:31 - INFO: Epoch 359 average loss: 0.105107
01:30:32 - INFO: Epoch 369 average loss: 0.097326
01:30:34 - INFO: Epoch 379 average loss: 0.088694
01:30:35 - INFO: Epoch 389 average loss: 0.091700
01:30:36 - INFO: Starting epoch 399:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0634]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0903]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0976]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.096]  80%|████████  | 4/5 [00:00<00:00, 37.55it/s, MSE=0.096] 80%|████████  | 4/5 [00:00<00:00, 37.55it/s, MSE=0.134]100%|██████████| 5/5 [00:00<00:00, 40.03it/s, MSE=0.134]
01:30:36 - INFO: Epoch 399 average loss: 0.096203
01:30:37 - INFO: Epoch 409 average loss: 0.118836
01:30:39 - INFO: Epoch 419 average loss: 0.114365
01:30:40 - INFO: Epoch 429 average loss: 0.096811
01:30:41 - INFO: Epoch 439 average loss: 0.154160
01:30:42 - INFO: Epoch 449 average loss: 0.080274
01:30:43 - INFO: Epoch 459 average loss: 0.090391
01:30:44 - INFO: Epoch 469 average loss: 0.082056
01:30:46 - INFO: Epoch 479 average loss: 0.115570
01:30:47 - INFO: Epoch 489 average loss: 0.080088
01:30:48 - INFO: Starting epoch 499:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.1]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.118]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.107]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0732] 80%|████████  | 4/5 [00:00<00:00, 36.70it/s, MSE=0.0732] 80%|████████  | 4/5 [00:00<00:00, 36.70it/s, MSE=0.106] 100%|██████████| 5/5 [00:00<00:00, 35.89it/s, MSE=0.106]
01:30:48 - INFO: Epoch 499 average loss: 0.101108
01:30:50 - INFO: Epoch 509 average loss: 0.101090
01:30:51 - INFO: Epoch 519 average loss: 0.051842
01:30:52 - INFO: Epoch 529 average loss: 0.088795
01:30:53 - INFO: Epoch 539 average loss: 0.104490
01:30:54 - INFO: Epoch 549 average loss: 0.123306
01:30:55 - INFO: Epoch 559 average loss: 0.097113
01:30:56 - INFO: Epoch 569 average loss: 0.118881
01:30:57 - INFO: Epoch 579 average loss: 0.114779
01:30:58 - INFO: Epoch 589 average loss: 0.141263
01:30:59 - INFO: Starting epoch 599:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.103]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.414]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.129]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.153]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0911]100%|██████████| 5/5 [00:00<00:00, 60.59it/s, MSE=0.0911]
01:30:59 - INFO: Epoch 599 average loss: 0.177950
01:31:01 - INFO: Epoch 609 average loss: 0.111453
01:31:02 - INFO: Epoch 619 average loss: 0.116792
01:31:03 - INFO: Epoch 629 average loss: 0.102368
01:31:04 - INFO: Epoch 639 average loss: 0.087272
01:31:05 - INFO: Epoch 649 average loss: 0.064807
01:31:06 - INFO: Epoch 659 average loss: 0.071547
01:31:08 - INFO: Epoch 669 average loss: 0.117902
01:31:09 - INFO: Epoch 679 average loss: 0.090999
01:31:10 - INFO: Epoch 689 average loss: 0.072085
01:31:11 - INFO: Starting epoch 699:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0582]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0986]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0872]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.102]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0837]100%|██████████| 5/5 [00:00<00:00, 49.58it/s, MSE=0.0837]100%|██████████| 5/5 [00:00<00:00, 49.50it/s, MSE=0.0837]
01:31:11 - INFO: Epoch 699 average loss: 0.085861
01:31:12 - INFO: Epoch 709 average loss: 0.106007
01:31:13 - INFO: Epoch 719 average loss: 0.067739
01:31:14 - INFO: Epoch 729 average loss: 0.107338
01:31:16 - INFO: Epoch 739 average loss: 0.073065
01:31:17 - INFO: Epoch 749 average loss: 0.080507
01:31:18 - INFO: Epoch 759 average loss: 0.155772
01:31:19 - INFO: Epoch 769 average loss: 0.146259
01:31:20 - INFO: Epoch 779 average loss: 0.103763
01:31:22 - INFO: Epoch 789 average loss: 0.092189
01:31:23 - INFO: Starting epoch 799:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.145]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.107]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0967]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0666]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.102] 100%|██████████| 5/5 [00:00<00:00, 50.98it/s, MSE=0.102]
01:31:23 - INFO: Epoch 799 average loss: 0.103413
01:31:24 - INFO: Epoch 809 average loss: 0.119340
01:31:25 - INFO: Epoch 819 average loss: 0.093221
01:31:27 - INFO: Epoch 829 average loss: 0.081621
01:31:28 - INFO: Epoch 839 average loss: 0.115791
01:31:29 - INFO: Epoch 849 average loss: 0.149166
01:31:30 - INFO: Epoch 859 average loss: 0.102958
01:31:32 - INFO: Epoch 869 average loss: 0.080086
01:31:33 - INFO: Epoch 879 average loss: 0.114805
01:31:34 - INFO: Epoch 889 average loss: 0.072314
01:31:35 - INFO: Starting epoch 899:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0889]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.155]   0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0689]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.16]    0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0559]100%|██████████| 5/5 [00:00<00:00, 55.55it/s, MSE=0.0559]
01:31:35 - INFO: Epoch 899 average loss: 0.105672
01:31:36 - INFO: Epoch 909 average loss: 0.128925
01:31:37 - INFO: Epoch 919 average loss: 0.099613
01:31:38 - INFO: Epoch 929 average loss: 0.094155
01:31:39 - INFO: Epoch 939 average loss: 0.110031
01:31:41 - INFO: Epoch 949 average loss: 0.079862
01:31:42 - INFO: Epoch 959 average loss: 0.122852
01:31:43 - INFO: Epoch 969 average loss: 0.099743
01:31:44 - INFO: Epoch 979 average loss: 0.122486
01:31:45 - INFO: Epoch 989 average loss: 0.061630
01:31:46 - INFO: Starting epoch 999:
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0836]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0279]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0635]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.0692]  0%|          | 0/5 [00:00<?, ?it/s, MSE=0.045] 100%|██████████| 5/5 [00:00<00:00, 42.32it/s, MSE=0.045]100%|██████████| 5/5 [00:00<00:00, 42.20it/s, MSE=0.045]
01:31:46 - INFO: Epoch 999 average loss: 0.057853
01:31:47 - INFO: Total sampling 34 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
01:33:01 - INFO: Epoch 9 average loss: 0.148286
01:33:02 - INFO: Epoch 19 average loss: 0.165980
01:33:04 - INFO: Epoch 29 average loss: 0.147182
01:33:05 - INFO: Epoch 39 average loss: 0.111616
01:33:06 - INFO: Epoch 49 average loss: 0.149588
01:33:08 - INFO: Epoch 59 average loss: 0.078050
01:33:09 - INFO: Epoch 69 average loss: 0.130407
01:33:10 - INFO: Epoch 79 average loss: 0.100959
01:33:12 - INFO: Epoch 89 average loss: 0.088502
01:33:13 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... torch.Size([34, 16])
feature shape after [Data Augmentation]:  (70, 16)
(70, 16)
====>MLP training... Epoch: 20 total loss: 1.2399
====>MLP training... Epoch: 40 total loss: 1.2362
====>MLP training... Epoch: 60 total loss: 1.1311
====>MLP training... Epoch: 80 total loss: 1.0401
====>MLP training... Epoch: 100 total loss: 0.8251
====>MLP training... Epoch: 120 total loss: 0.7822
====>MLP training... Epoch: 140 total loss: 0.6199
====>MLP training... Epoch: 160 total loss: 0.4678
====>MLP training... Epoch: 180 total loss: 0.4604
====>MLP training... Epoch: 200 total loss: 0.5883
====>CNN training... Epoch: 0  total loss: 0.2546
====>CNN training... Epoch: 20  total loss: 0.2490
====>CNN training... Epoch: 40  total loss: 0.2444
====>CNN training... Epoch: 60  total loss: 0.2164
====>CNN training... Epoch: 80  total loss: 0.1791
====>CNN training... Epoch: 100  total loss: 0.1461
====>CNN training... Epoch: 120  total loss: 0.1292
====>CNN training... Epoch: 140  total loss: 0.1176
====>CNN training... Epoch: 160  total loss: 0.1104
====>CNN training... Epoch: 180  total loss: 0.1235
====>CNN training... Epoch: 200  total loss: 0.0910
====>RNN training... Epoch: 20 total loss: 0.2503
====>RNN training... Epoch: 40 total loss: 0.2463
====>RNN training... Epoch: 60 total loss: 0.2430
====>RNN training... Epoch: 80 total loss: 0.2405
====>RNN training... Epoch: 100 total loss: 0.2385
====>RNN training... Epoch: 120 total loss: 0.2368
====>RNN training... Epoch: 140 total loss: 0.2353
====>RNN training... Epoch: 160 total loss: 0.2339
====>RNN training... Epoch: 180 total loss: 0.2326
====>RNN training... Epoch: 200 total loss: 0.2313

test: Lang-18-fs_ddpm
fault line:  [100495, 100496, 100497, 100499]
feature shape before [Data Augmentation]:  (48, 518)
[0, 1, 2, 3, 4, 5, 6, 7, 9, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 95, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 155, 156, 157, 158, 159, 164, 165, 166, 167, 184, 185, 186, 187, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 256, 257, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 298, 299, 300, 301, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 334, 335, 336, 337, 338, 339, 340, 341, 342, 344, 345, 346, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 406, 408]
entropy 65.90565176809665 21.27775347042366
req_shape:  16
statements selected: 128
statements selected: Index([ 100432,  100433,  100434,  100435,  100437,  100438,  100107,  100159,
        100415,  100419,
       ...
       1701454, 1701456, 1701457, 1701460, 1701461, 1701462, 1701463, 1800937,
       1800944, 1800951],
      dtype='int64', length=128)
Dataset shape: torch.Size([48, 1, 1, 128]) torch.Size([48])
Batch Dataloader shape: torch.Size([8, 1, 1, 128]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0364]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0608]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.123]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0235] 67%|██████▋   | 4/6 [00:00<00:00, 38.57it/s, MSE=0.0235] 67%|██████▋   | 4/6 [00:00<00:00, 38.57it/s, MSE=0.0899] 67%|██████▋   | 4/6 [00:00<00:00, 38.57it/s, MSE=0.0315]100%|██████████| 6/6 [00:00<00:00, 39.10it/s, MSE=0.0315]
01:33:13 - INFO: Epoch 99 average loss: 0.060765
01:33:15 - INFO: Epoch 109 average loss: 0.119274
01:33:16 - INFO: Epoch 119 average loss: 0.118520
01:33:17 - INFO: Epoch 129 average loss: 0.102762
01:33:19 - INFO: Epoch 139 average loss: 0.109088
01:33:20 - INFO: Epoch 149 average loss: 0.086376
01:33:22 - INFO: Epoch 159 average loss: 0.090834
01:33:23 - INFO: Epoch 169 average loss: 0.108327
01:33:25 - INFO: Epoch 179 average loss: 0.095157
01:33:26 - INFO: Epoch 189 average loss: 0.090805
01:33:27 - INFO: Starting epoch 199:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0435]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0609]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0735]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0539] 67%|██████▋   | 4/6 [00:00<00:00, 33.88it/s, MSE=0.0539] 67%|██████▋   | 4/6 [00:00<00:00, 33.88it/s, MSE=0.0792] 67%|██████▋   | 4/6 [00:00<00:00, 33.88it/s, MSE=0.15]  100%|██████████| 6/6 [00:00<00:00, 35.82it/s, MSE=0.15]
01:33:27 - INFO: Epoch 199 average loss: 0.076912
01:33:29 - INFO: Epoch 209 average loss: 0.069074
01:33:30 - INFO: Epoch 219 average loss: 0.084628
01:33:31 - INFO: Epoch 229 average loss: 0.061012
01:33:33 - INFO: Epoch 239 average loss: 0.086333
01:33:34 - INFO: Epoch 249 average loss: 0.061350
01:33:36 - INFO: Epoch 259 average loss: 0.077950
01:33:37 - INFO: Epoch 269 average loss: 0.074580
01:33:38 - INFO: Epoch 279 average loss: 0.062004
01:33:40 - INFO: Epoch 289 average loss: 0.071335
01:33:41 - INFO: Starting epoch 299:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0323]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0266]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0299]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0487]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0661]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.128] 100%|██████████| 6/6 [00:00<00:00, 59.45it/s, MSE=0.128]100%|██████████| 6/6 [00:00<00:00, 59.30it/s, MSE=0.128]
01:33:41 - INFO: Epoch 299 average loss: 0.055237
01:33:42 - INFO: Epoch 309 average loss: 0.058244
01:33:44 - INFO: Epoch 319 average loss: 0.088522
01:33:45 - INFO: Epoch 329 average loss: 0.049198
01:33:47 - INFO: Epoch 339 average loss: 0.061391
01:33:48 - INFO: Epoch 349 average loss: 0.058262
01:33:50 - INFO: Epoch 359 average loss: 0.069673
01:33:51 - INFO: Epoch 369 average loss: 0.060158
01:33:52 - INFO: Epoch 379 average loss: 0.070780
01:33:54 - INFO: Epoch 389 average loss: 0.070989
01:33:55 - INFO: Starting epoch 399:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0326]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.13]    0%|          | 0/6 [00:00<?, ?it/s, MSE=0.116]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0771]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0549] 83%|████████▎ | 5/6 [00:00<00:00, 41.79it/s, MSE=0.0549] 83%|████████▎ | 5/6 [00:00<00:00, 41.79it/s, MSE=0.128] 100%|██████████| 6/6 [00:00<00:00, 40.94it/s, MSE=0.128]
01:33:55 - INFO: Epoch 399 average loss: 0.089686
01:33:56 - INFO: Epoch 409 average loss: 0.069508
01:33:57 - INFO: Epoch 419 average loss: 0.073601
01:33:59 - INFO: Epoch 429 average loss: 0.074439
01:34:00 - INFO: Epoch 439 average loss: 0.074310
01:34:02 - INFO: Epoch 449 average loss: 0.064901
01:34:03 - INFO: Epoch 459 average loss: 0.072466
01:34:04 - INFO: Epoch 469 average loss: 0.061443
01:34:06 - INFO: Epoch 479 average loss: 0.068984
01:34:07 - INFO: Epoch 489 average loss: 0.049860
01:34:08 - INFO: Starting epoch 499:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0498]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0582]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.111]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0437]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0412] 83%|████████▎ | 5/6 [00:00<00:00, 46.37it/s, MSE=0.0412] 83%|████████▎ | 5/6 [00:00<00:00, 46.37it/s, MSE=0.081] 100%|██████████| 6/6 [00:00<00:00, 46.93it/s, MSE=0.081]
01:34:08 - INFO: Epoch 499 average loss: 0.064221
01:34:10 - INFO: Epoch 509 average loss: 0.066209
01:34:11 - INFO: Epoch 519 average loss: 0.064103
01:34:13 - INFO: Epoch 529 average loss: 0.076513
01:34:14 - INFO: Epoch 539 average loss: 0.051374
01:34:15 - INFO: Epoch 549 average loss: 0.068913
01:34:17 - INFO: Epoch 559 average loss: 0.064901
01:34:18 - INFO: Epoch 569 average loss: 0.061940
01:34:20 - INFO: Epoch 579 average loss: 0.044900
01:34:21 - INFO: Epoch 589 average loss: 0.058545
01:34:22 - INFO: Starting epoch 599:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0311]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0621]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0191]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.047]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0666] 83%|████████▎ | 5/6 [00:00<00:00, 42.54it/s, MSE=0.0666] 83%|████████▎ | 5/6 [00:00<00:00, 42.54it/s, MSE=0.0535]100%|██████████| 6/6 [00:00<00:00, 41.71it/s, MSE=0.0535]
01:34:22 - INFO: Epoch 599 average loss: 0.046567
01:34:24 - INFO: Epoch 609 average loss: 0.038718
01:34:25 - INFO: Epoch 619 average loss: 0.049932
01:34:27 - INFO: Epoch 629 average loss: 0.051393
01:34:28 - INFO: Epoch 639 average loss: 0.058172
01:34:29 - INFO: Epoch 649 average loss: 0.055291
01:34:31 - INFO: Epoch 659 average loss: 0.052123
01:34:32 - INFO: Epoch 669 average loss: 0.059705
01:34:34 - INFO: Epoch 679 average loss: 0.062728
01:34:35 - INFO: Epoch 689 average loss: 0.071672
01:34:36 - INFO: Starting epoch 699:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0636]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0603]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.084]   0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0309]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0755]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.022] 100%|██████████| 6/6 [00:00<00:00, 55.15it/s, MSE=0.022]100%|██████████| 6/6 [00:00<00:00, 54.96it/s, MSE=0.022]
01:34:36 - INFO: Epoch 699 average loss: 0.056052
01:34:38 - INFO: Epoch 709 average loss: 0.070303
01:34:39 - INFO: Epoch 719 average loss: 0.059625
01:34:41 - INFO: Epoch 729 average loss: 0.056717
01:34:42 - INFO: Epoch 739 average loss: 0.040223
01:34:44 - INFO: Epoch 749 average loss: 0.058186
01:34:45 - INFO: Epoch 759 average loss: 0.056035
01:34:46 - INFO: Epoch 769 average loss: 0.046961
01:34:48 - INFO: Epoch 779 average loss: 0.074082
01:34:49 - INFO: Epoch 789 average loss: 0.059170
01:34:50 - INFO: Starting epoch 799:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0185]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0604]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0672]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0671]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0494] 83%|████████▎ | 5/6 [00:00<00:00, 40.97it/s, MSE=0.0494] 83%|████████▎ | 5/6 [00:00<00:00, 40.97it/s, MSE=0.0627]100%|██████████| 6/6 [00:00<00:00, 40.79it/s, MSE=0.0627]
01:34:50 - INFO: Epoch 799 average loss: 0.054216
01:34:52 - INFO: Epoch 809 average loss: 0.056325
01:34:53 - INFO: Epoch 819 average loss: 0.044635
01:34:55 - INFO: Epoch 829 average loss: 0.052967
01:34:56 - INFO: Epoch 839 average loss: 0.056260
01:34:58 - INFO: Epoch 849 average loss: 0.044385
01:34:59 - INFO: Epoch 859 average loss: 0.053855
01:35:00 - INFO: Epoch 869 average loss: 0.051734
01:35:02 - INFO: Epoch 879 average loss: 0.047118
01:35:03 - INFO: Epoch 889 average loss: 0.050645
01:35:04 - INFO: Starting epoch 899:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.028]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0731]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0275]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0751]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0391] 83%|████████▎ | 5/6 [00:00<00:00, 41.76it/s, MSE=0.0391] 83%|████████▎ | 5/6 [00:00<00:00, 41.76it/s, MSE=0.0485]100%|██████████| 6/6 [00:00<00:00, 43.03it/s, MSE=0.0485]
01:35:05 - INFO: Epoch 899 average loss: 0.048558
01:35:06 - INFO: Epoch 909 average loss: 0.060550
01:35:07 - INFO: Epoch 919 average loss: 0.062654
01:35:09 - INFO: Epoch 929 average loss: 0.052967
01:35:10 - INFO: Epoch 939 average loss: 0.044778
01:35:12 - INFO: Epoch 949 average loss: 0.070393
01:35:13 - INFO: Epoch 959 average loss: 0.048459
01:35:14 - INFO: Epoch 969 average loss: 0.064922
01:35:15 - INFO: Epoch 979 average loss: 0.038132
01:35:17 - INFO: Epoch 989 average loss: 0.049688
01:35:18 - INFO: Starting epoch 999:
  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0375]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0323]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0204]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0553]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0498]  0%|          | 0/6 [00:00<?, ?it/s, MSE=0.0757]100%|██████████| 6/6 [00:00<00:00, 49.56it/s, MSE=0.0757]100%|██████████| 6/6 [00:00<00:00, 49.45it/s, MSE=0.0757]
01:35:18 - INFO: Epoch 999 average loss: 0.045157
01:35:18 - INFO: Total sampling 46 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
01:36:51 - INFO: Epoch 9 average loss: 0.311744
01:36:52 - INFO: Epoch 19 average loss: 0.203849
01:36:52 - INFO: Epoch 29 average loss: 0.156612
01:36:53 - INFO: Epoch 39 average loss: 0.121661
01:36:54 - INFO: Epoch 49 average loss: 0.142259
01:36:54 - INFO: Epoch 59 average loss: 0.089068
01:36:55 - INFO: Epoch 69 average loss: 0.106023
01:36:56 - INFO: Epoch 79 average loss: 0.256824
01:36:57 - INFO: Epoch 89 average loss: 0.199517
01:36:57 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... torch.Size([46, 128])
feature shape after [Data Augmentation]:  (94, 128)
(94, 128)
====>MLP training... Epoch: 20 total loss: 0.8781
====>MLP training... Epoch: 40 total loss: 0.0136
====>MLP training... Epoch: 60 total loss: 0.0056
====>MLP training... Epoch: 80 total loss: 0.0017
====>MLP training... Epoch: 100 total loss: 0.0019
====>MLP training... Epoch: 120 total loss: 0.0010
====>MLP training... Epoch: 140 total loss: 0.0017
====>MLP training... Epoch: 160 total loss: 0.0007
====>MLP training... Epoch: 180 total loss: 0.0007
====>MLP training... Epoch: 200 total loss: 0.0006
====>CNN training... Epoch: 0  total loss: 0.2517
====>CNN training... Epoch: 20  total loss: 0.2487
====>CNN training... Epoch: 40  total loss: 0.2380
====>CNN training... Epoch: 60  total loss: 0.1860
====>CNN training... Epoch: 80  total loss: 0.1462
====>CNN training... Epoch: 100  total loss: 0.1282
====>CNN training... Epoch: 120  total loss: 0.1101
====>CNN training... Epoch: 140  total loss: 0.0926
====>CNN training... Epoch: 160  total loss: 0.0791
====>CNN training... Epoch: 180  total loss: 0.0743
====>CNN training... Epoch: 200  total loss: 0.0676
====>RNN training... Epoch: 20 total loss: 0.2217
====>RNN training... Epoch: 40 total loss: 0.1800
====>RNN training... Epoch: 60 total loss: 0.1330
====>RNN training... Epoch: 80 total loss: 0.0864
====>RNN training... Epoch: 100 total loss: 0.0520
====>RNN training... Epoch: 120 total loss: 0.0329
====>RNN training... Epoch: 140 total loss: 0.0234
====>RNN training... Epoch: 160 total loss: 0.0183
====>RNN training... Epoch: 180 total loss: 0.0153
====>RNN training... Epoch: 200 total loss: 0.0133

test: Lang-19-fs_ddpm
fault line:  [300040, 300054, 300080, 300050, 300079]
feature shape before [Data Augmentation]:  (23, 63)
[8, 9, 11, 12, 13, 16, 18, 20, 21, 22, 23, 24, 30, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 62]
entropy 10.09912179013116 1.9633388027817613
req_shape:  16
statements selected: 16
statements selected: Index([200032, 200054, 200058, 200059, 200060, 200076, 200079, 200082, 200083,
       200086, 200088, 200089, 200105, 300031, 300038, 300040],
      dtype='int64')
Dataset shape: torch.Size([23, 1, 1, 16]) torch.Size([23])
Batch Dataloader shape: torch.Size([8, 1, 1, 16]) torch.Size([8])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.339]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.108]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.355]100%|██████████| 3/3 [00:00<00:00, 46.18it/s, MSE=0.355]
01:36:57 - INFO: Epoch 99 average loss: 0.267479
01:36:58 - INFO: Epoch 109 average loss: 0.136784
01:36:59 - INFO: Epoch 119 average loss: 0.135669
01:36:59 - INFO: Epoch 129 average loss: 0.111630
01:37:00 - INFO: Epoch 139 average loss: 0.146801
01:37:01 - INFO: Epoch 149 average loss: 0.108589
01:37:01 - INFO: Epoch 159 average loss: 0.177661
01:37:02 - INFO: Epoch 169 average loss: 0.134724
01:37:03 - INFO: Epoch 179 average loss: 0.112381
01:37:03 - INFO: Epoch 189 average loss: 0.155605
01:37:04 - INFO: Starting epoch 199:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.19]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0648]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0659]100%|██████████| 3/3 [00:00<00:00, 44.45it/s, MSE=0.0659]
01:37:04 - INFO: Epoch 199 average loss: 0.106928
01:37:05 - INFO: Epoch 209 average loss: 0.121533
01:37:06 - INFO: Epoch 219 average loss: 0.187750
01:37:06 - INFO: Epoch 229 average loss: 0.074102
01:37:07 - INFO: Epoch 239 average loss: 0.081725
01:37:08 - INFO: Epoch 249 average loss: 0.090510
01:37:08 - INFO: Epoch 259 average loss: 0.281776
01:37:09 - INFO: Epoch 269 average loss: 0.143654
01:37:10 - INFO: Epoch 279 average loss: 0.152503
01:37:10 - INFO: Epoch 289 average loss: 0.156619
01:37:11 - INFO: Starting epoch 299:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0882]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.194]   0%|          | 0/3 [00:00<?, ?it/s, MSE=0.166]100%|██████████| 3/3 [00:00<00:00, 39.17it/s, MSE=0.166]
01:37:11 - INFO: Epoch 299 average loss: 0.149316
01:37:12 - INFO: Epoch 309 average loss: 0.146338
01:37:12 - INFO: Epoch 319 average loss: 0.121660
01:37:13 - INFO: Epoch 329 average loss: 0.113475
01:37:14 - INFO: Epoch 339 average loss: 0.085841
01:37:15 - INFO: Epoch 349 average loss: 0.146534
01:37:15 - INFO: Epoch 359 average loss: 0.112306
01:37:16 - INFO: Epoch 369 average loss: 0.103044
01:37:17 - INFO: Epoch 379 average loss: 0.090066
01:37:17 - INFO: Epoch 389 average loss: 0.076171
01:37:18 - INFO: Starting epoch 399:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0963]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0689]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.166] 100%|██████████| 3/3 [00:00<00:00, 63.81it/s, MSE=0.166]
01:37:18 - INFO: Epoch 399 average loss: 0.110567
01:37:19 - INFO: Epoch 409 average loss: 0.057755
01:37:20 - INFO: Epoch 419 average loss: 0.120586
01:37:20 - INFO: Epoch 429 average loss: 0.118513
01:37:21 - INFO: Epoch 439 average loss: 0.080688
01:37:21 - INFO: Epoch 449 average loss: 0.100950
01:37:22 - INFO: Epoch 459 average loss: 0.104286
01:37:23 - INFO: Epoch 469 average loss: 0.083867
01:37:24 - INFO: Epoch 479 average loss: 0.095721
01:37:24 - INFO: Epoch 489 average loss: 0.136203
01:37:25 - INFO: Starting epoch 499:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.157]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0806]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.265] 100%|██████████| 3/3 [00:00<00:00, 43.26it/s, MSE=0.265]
01:37:25 - INFO: Epoch 499 average loss: 0.167728
01:37:26 - INFO: Epoch 509 average loss: 0.091800
01:37:26 - INFO: Epoch 519 average loss: 0.121527
01:37:27 - INFO: Epoch 529 average loss: 0.105821
01:37:28 - INFO: Epoch 539 average loss: 0.113336
01:37:28 - INFO: Epoch 549 average loss: 0.053872
01:37:29 - INFO: Epoch 559 average loss: 0.088471
01:37:30 - INFO: Epoch 569 average loss: 0.094788
01:37:30 - INFO: Epoch 579 average loss: 0.145031
01:37:31 - INFO: Epoch 589 average loss: 0.087851
01:37:32 - INFO: Starting epoch 599:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.101]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.11]   0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0385]100%|██████████| 3/3 [00:00<00:00, 45.37it/s, MSE=0.0385]
01:37:32 - INFO: Epoch 599 average loss: 0.083193
01:37:33 - INFO: Epoch 609 average loss: 0.116289
01:37:33 - INFO: Epoch 619 average loss: 0.118494
01:37:34 - INFO: Epoch 629 average loss: 0.102743
01:37:35 - INFO: Epoch 639 average loss: 0.098062
01:37:35 - INFO: Epoch 649 average loss: 0.139284
01:37:36 - INFO: Epoch 659 average loss: 0.108369
01:37:37 - INFO: Epoch 669 average loss: 0.079622
01:37:37 - INFO: Epoch 679 average loss: 0.099294
01:37:38 - INFO: Epoch 689 average loss: 0.077938
01:37:39 - INFO: Starting epoch 699:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0797]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0888]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.161] 100%|██████████| 3/3 [00:00<00:00, 45.30it/s, MSE=0.161]
01:37:39 - INFO: Epoch 699 average loss: 0.109837
01:37:40 - INFO: Epoch 709 average loss: 0.086949
01:37:40 - INFO: Epoch 719 average loss: 0.123080
01:37:41 - INFO: Epoch 729 average loss: 0.113937
01:37:42 - INFO: Epoch 739 average loss: 0.154161
01:37:42 - INFO: Epoch 749 average loss: 0.130632
01:37:43 - INFO: Epoch 759 average loss: 0.124564
01:37:44 - INFO: Epoch 769 average loss: 0.111940
01:37:44 - INFO: Epoch 779 average loss: 0.175104
01:37:45 - INFO: Epoch 789 average loss: 0.095199
01:37:46 - INFO: Starting epoch 799:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0292]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0757]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0571]100%|██████████| 3/3 [00:00<00:00, 38.91it/s, MSE=0.0571]
01:37:46 - INFO: Epoch 799 average loss: 0.054002
01:37:47 - INFO: Epoch 809 average loss: 0.076821
01:37:47 - INFO: Epoch 819 average loss: 0.087745
01:37:48 - INFO: Epoch 829 average loss: 0.080092
01:37:49 - INFO: Epoch 839 average loss: 0.083584
01:37:49 - INFO: Epoch 849 average loss: 0.100504
01:37:50 - INFO: Epoch 859 average loss: 0.112073
01:37:51 - INFO: Epoch 869 average loss: 0.055077
01:37:51 - INFO: Epoch 879 average loss: 0.070423
01:37:52 - INFO: Epoch 889 average loss: 0.082625
01:37:52 - INFO: Starting epoch 899:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0808]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0759]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.467] 100%|██████████| 3/3 [00:00<00:00, 38.72it/s, MSE=0.467]
01:37:53 - INFO: Epoch 899 average loss: 0.207786
01:37:53 - INFO: Epoch 909 average loss: 0.088496
01:37:54 - INFO: Epoch 919 average loss: 0.072416
01:37:55 - INFO: Epoch 929 average loss: 0.057043
01:37:56 - INFO: Epoch 939 average loss: 0.073138
01:37:56 - INFO: Epoch 949 average loss: 0.090690
01:37:57 - INFO: Epoch 959 average loss: 0.114527
01:37:58 - INFO: Epoch 969 average loss: 0.088495
01:37:59 - INFO: Epoch 979 average loss: 0.122030
01:37:59 - INFO: Epoch 989 average loss: 0.106221
01:38:00 - INFO: Starting epoch 999:
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.141]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.0443]  0%|          | 0/3 [00:00<?, ?it/s, MSE=0.122] 100%|██████████| 3/3 [00:00<00:00, 45.34it/s, MSE=0.122]
01:38:00 - INFO: Epoch 999 average loss: 0.102479
01:38:01 - INFO: Total sampling 19 new convMatrix....
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
01:38:53 - INFO: Epoch 9 average loss: 0.098809
01:38:55 - INFO: Epoch 19 average loss: 0.094737
01:38:57 - INFO: Epoch 29 average loss: 0.071554
01:38:59 - INFO: Epoch 39 average loss: 0.057007
01:39:00 - INFO: Epoch 49 average loss: 0.064476
01:39:02 - INFO: Epoch 59 average loss: 0.043539
01:39:04 - INFO: Epoch 69 average loss: 0.053560
01:39:06 - INFO: Epoch 79 average loss: 0.064015
01:39:08 - INFO: Epoch 89 average loss: 0.045795
01:39:09 - INFO: Starting epoch 99:
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... torch.Size([19, 16])
feature shape after [Data Augmentation]:  (42, 16)
(42, 16)
====>MLP training... Epoch: 20 total loss: 1.4790
====>MLP training... Epoch: 40 total loss: 1.3606
====>MLP training... Epoch: 60 total loss: 0.6274
====>MLP training... Epoch: 80 total loss: 0.8509
====>MLP training... Epoch: 100 total loss: 0.3935
====>MLP training... Epoch: 120 total loss: 0.4937
====>MLP training... Epoch: 140 total loss: 0.3046
====>MLP training... Epoch: 160 total loss: 0.2874
====>MLP training... Epoch: 180 total loss: 0.3797
====>MLP training... Epoch: 200 total loss: 0.3172
====>CNN training... Epoch: 0  total loss: 0.2532
====>CNN training... Epoch: 20  total loss: 0.2519
====>CNN training... Epoch: 40  total loss: 0.2510
====>CNN training... Epoch: 60  total loss: 0.2501
====>CNN training... Epoch: 80  total loss: 0.2490
====>CNN training... Epoch: 100  total loss: 0.2480
====>CNN training... Epoch: 120  total loss: 0.2482
====>CNN training... Epoch: 140  total loss: 0.2471
====>CNN training... Epoch: 160  total loss: 0.2453
====>CNN training... Epoch: 180  total loss: 0.2433
====>CNN training... Epoch: 200  total loss: 0.2414
====>RNN training... Epoch: 20 total loss: 0.2528
====>RNN training... Epoch: 40 total loss: 0.2502
====>RNN training... Epoch: 60 total loss: 0.2479
====>RNN training... Epoch: 80 total loss: 0.2458
====>RNN training... Epoch: 100 total loss: 0.2438
====>RNN training... Epoch: 120 total loss: 0.2418
====>RNN training... Epoch: 140 total loss: 0.2398
====>RNN training... Epoch: 160 total loss: 0.2376
====>RNN training... Epoch: 180 total loss: 0.2353
====>RNN training... Epoch: 200 total loss: 0.2329

test: Lang-20-fs_ddpm
fault line:  [103298, 103383]
feature shape before [Data Augmentation]:  (235, 1283)
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 30, 31, 32, 590, 591, 592, 593, 594, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 611, 612, 613, 614, 615, 616, 617, 618, 620, 622]
entropy 1.9045621357586715 0.5111048173792878
req_shape:  16
statements selected: 16
statements selected: Index([100147, 100704, 100705, 100706, 100708, 100709, 100710, 100711, 100717,
       100719, 100720, 100722, 100724, 100733, 100737, 100739],
      dtype='int64')
Dataset shape: torch.Size([235, 1, 1, 16]) torch.Size([235])
Batch Dataloader shape: torch.Size([32, 1, 1, 16]) torch.Size([32])

start train
model save to:  models/DDPM_conditional/ckpt.pt
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.051]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0552]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.047]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0441]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0459] 62%|██████▎   | 5/8 [00:00<00:00, 45.18it/s, MSE=0.0459] 62%|██████▎   | 5/8 [00:00<00:00, 45.18it/s, MSE=0.0544] 62%|██████▎   | 5/8 [00:00<00:00, 45.18it/s, MSE=0.0596] 62%|██████▎   | 5/8 [00:00<00:00, 45.18it/s, MSE=0.103] 100%|██████████| 8/8 [00:00<00:00, 42.97it/s, MSE=0.103]
01:39:10 - INFO: Epoch 99 average loss: 0.057528
01:39:12 - INFO: Epoch 109 average loss: 0.038919
01:39:14 - INFO: Epoch 119 average loss: 0.053677
01:39:16 - INFO: Epoch 129 average loss: 0.057915
01:39:18 - INFO: Epoch 139 average loss: 0.049263
01:39:20 - INFO: Epoch 149 average loss: 0.044475
01:39:21 - INFO: Epoch 159 average loss: 0.050835
01:39:23 - INFO: Epoch 169 average loss: 0.047453
01:39:25 - INFO: Epoch 179 average loss: 0.049606
01:39:27 - INFO: Epoch 189 average loss: 0.046276
01:39:28 - INFO: Starting epoch 199:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0498]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.032]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0717]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0705]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0294] 62%|██████▎   | 5/8 [00:00<00:00, 46.14it/s, MSE=0.0294] 62%|██████▎   | 5/8 [00:00<00:00, 46.14it/s, MSE=0.0382] 62%|██████▎   | 5/8 [00:00<00:00, 46.14it/s, MSE=0.0362] 62%|██████▎   | 5/8 [00:00<00:00, 46.14it/s, MSE=0.0176]100%|██████████| 8/8 [00:00<00:00, 47.90it/s, MSE=0.0176]
01:39:29 - INFO: Epoch 199 average loss: 0.043194
01:39:31 - INFO: Epoch 209 average loss: 0.047052
01:39:33 - INFO: Epoch 219 average loss: 0.045136
01:39:35 - INFO: Epoch 229 average loss: 0.053481
01:39:36 - INFO: Epoch 239 average loss: 0.046498
01:39:38 - INFO: Epoch 249 average loss: 0.043537
01:39:40 - INFO: Epoch 259 average loss: 0.049037
01:39:42 - INFO: Epoch 269 average loss: 0.047585
01:39:44 - INFO: Epoch 279 average loss: 0.040224
01:39:46 - INFO: Epoch 289 average loss: 0.054154
01:39:47 - INFO: Starting epoch 299:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0656]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.047]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0502]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0497] 50%|█████     | 4/8 [00:00<00:00, 35.15it/s, MSE=0.0497] 50%|█████     | 4/8 [00:00<00:00, 35.15it/s, MSE=0.0698] 50%|█████     | 4/8 [00:00<00:00, 35.15it/s, MSE=0.0412] 50%|█████     | 4/8 [00:00<00:00, 35.15it/s, MSE=0.0502] 50%|█████     | 4/8 [00:00<00:00, 35.15it/s, MSE=0.0413]100%|██████████| 8/8 [00:00<00:00, 34.66it/s, MSE=0.0413]100%|██████████| 8/8 [00:00<00:00, 34.65it/s, MSE=0.0413]
01:39:48 - INFO: Epoch 299 average loss: 0.051865
01:39:49 - INFO: Epoch 309 average loss: 0.046734
01:39:51 - INFO: Epoch 319 average loss: 0.045307
01:39:53 - INFO: Epoch 329 average loss: 0.040401
01:39:55 - INFO: Epoch 339 average loss: 0.045071
01:39:57 - INFO: Epoch 349 average loss: 0.047059
01:39:59 - INFO: Epoch 359 average loss: 0.047569
01:40:01 - INFO: Epoch 369 average loss: 0.050083
01:40:03 - INFO: Epoch 379 average loss: 0.048054
01:40:05 - INFO: Epoch 389 average loss: 0.046161
01:40:07 - INFO: Starting epoch 399:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0353]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0413]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0315]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0308]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0412] 62%|██████▎   | 5/8 [00:00<00:00, 44.06it/s, MSE=0.0412] 62%|██████▎   | 5/8 [00:00<00:00, 44.06it/s, MSE=0.0489] 62%|██████▎   | 5/8 [00:00<00:00, 44.06it/s, MSE=0.0444] 62%|██████▎   | 5/8 [00:00<00:00, 44.06it/s, MSE=0.0472]100%|██████████| 8/8 [00:00<00:00, 42.06it/s, MSE=0.0472]
01:40:07 - INFO: Epoch 399 average loss: 0.040078
01:40:09 - INFO: Epoch 409 average loss: 0.043434
01:40:11 - INFO: Epoch 419 average loss: 0.046326
01:40:12 - INFO: Epoch 429 average loss: 0.040942
01:40:14 - INFO: Epoch 439 average loss: 0.042076
01:40:16 - INFO: Epoch 449 average loss: 0.044368
01:40:18 - INFO: Epoch 459 average loss: 0.035995
01:40:19 - INFO: Epoch 469 average loss: 0.037548
01:40:21 - INFO: Epoch 479 average loss: 0.052289
01:40:23 - INFO: Epoch 489 average loss: 0.040992
01:40:25 - INFO: Starting epoch 499:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.047]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0521]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0304]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0493]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0606] 62%|██████▎   | 5/8 [00:00<00:00, 42.99it/s, MSE=0.0606] 62%|██████▎   | 5/8 [00:00<00:00, 42.99it/s, MSE=0.0428] 62%|██████▎   | 5/8 [00:00<00:00, 42.99it/s, MSE=0.0359] 62%|██████▎   | 5/8 [00:00<00:00, 42.99it/s, MSE=0.0255]100%|██████████| 8/8 [00:00<00:00, 42.66it/s, MSE=0.0255]
01:40:25 - INFO: Epoch 499 average loss: 0.042950
01:40:27 - INFO: Epoch 509 average loss: 0.039308
01:40:29 - INFO: Epoch 519 average loss: 0.042343
01:40:31 - INFO: Epoch 529 average loss: 0.038582
01:40:33 - INFO: Epoch 539 average loss: 0.045402
01:40:35 - INFO: Epoch 549 average loss: 0.040055
01:40:37 - INFO: Epoch 559 average loss: 0.035499
01:40:39 - INFO: Epoch 569 average loss: 0.046724
01:40:40 - INFO: Epoch 579 average loss: 0.043445
01:40:42 - INFO: Epoch 589 average loss: 0.044740
01:40:44 - INFO: Starting epoch 599:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0476]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.033]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0637]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0614]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.028]  62%|██████▎   | 5/8 [00:00<00:00, 47.09it/s, MSE=0.028] 62%|██████▎   | 5/8 [00:00<00:00, 47.09it/s, MSE=0.0584] 62%|██████▎   | 5/8 [00:00<00:00, 47.09it/s, MSE=0.0356] 62%|██████▎   | 5/8 [00:00<00:00, 47.09it/s, MSE=0.0266]100%|██████████| 8/8 [00:00<00:00, 48.31it/s, MSE=0.0266]
01:40:44 - INFO: Epoch 599 average loss: 0.044276
01:40:46 - INFO: Epoch 609 average loss: 0.044198
01:40:48 - INFO: Epoch 619 average loss: 0.040920
01:40:50 - INFO: Epoch 629 average loss: 0.047050
01:40:52 - INFO: Epoch 639 average loss: 0.044457
01:40:54 - INFO: Epoch 649 average loss: 0.042515
01:40:56 - INFO: Epoch 659 average loss: 0.046819
01:40:58 - INFO: Epoch 669 average loss: 0.046593
01:40:59 - INFO: Epoch 679 average loss: 0.043036
01:41:01 - INFO: Epoch 689 average loss: 0.047603
01:41:03 - INFO: Starting epoch 699:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0523]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0518]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0539]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0551]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0324] 62%|██████▎   | 5/8 [00:00<00:00, 43.96it/s, MSE=0.0324] 62%|██████▎   | 5/8 [00:00<00:00, 43.96it/s, MSE=0.0475] 62%|██████▎   | 5/8 [00:00<00:00, 43.96it/s, MSE=0.0531] 62%|██████▎   | 5/8 [00:00<00:00, 43.96it/s, MSE=0.0503]100%|██████████| 8/8 [00:00<00:00, 42.48it/s, MSE=0.0503]
01:41:03 - INFO: Epoch 699 average loss: 0.049546
01:41:05 - INFO: Epoch 709 average loss: 0.039424
01:41:07 - INFO: Epoch 719 average loss: 0.039906
01:41:09 - INFO: Epoch 729 average loss: 0.044446
01:41:11 - INFO: Epoch 739 average loss: 0.053135
01:41:13 - INFO: Epoch 749 average loss: 0.040385
01:41:15 - INFO: Epoch 759 average loss: 0.038251
01:41:17 - INFO: Epoch 769 average loss: 0.054698
01:41:19 - INFO: Epoch 779 average loss: 0.042883
01:41:21 - INFO: Epoch 789 average loss: 0.043099
01:41:22 - INFO: Starting epoch 799:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0505]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0531]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0409]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0364] 50%|█████     | 4/8 [00:00<00:00, 39.17it/s, MSE=0.0364] 50%|█████     | 4/8 [00:00<00:00, 39.17it/s, MSE=0.0471] 50%|█████     | 4/8 [00:00<00:00, 39.17it/s, MSE=0.0465] 50%|█████     | 4/8 [00:00<00:00, 39.17it/s, MSE=0.0694] 50%|█████     | 4/8 [00:00<00:00, 39.17it/s, MSE=0.0661]100%|██████████| 8/8 [00:00<00:00, 41.83it/s, MSE=0.0661]
01:41:23 - INFO: Epoch 799 average loss: 0.051244
01:41:24 - INFO: Epoch 809 average loss: 0.043828
01:41:26 - INFO: Epoch 819 average loss: 0.041152
01:41:28 - INFO: Epoch 829 average loss: 0.035382
01:41:30 - INFO: Epoch 839 average loss: 0.040942
01:41:32 - INFO: Epoch 849 average loss: 0.042012
01:41:34 - INFO: Epoch 859 average loss: 0.043964
01:41:36 - INFO: Epoch 869 average loss: 0.038020
01:41:38 - INFO: Epoch 879 average loss: 0.042804
01:41:40 - INFO: Epoch 889 average loss: 0.038916
01:41:41 - INFO: Starting epoch 899:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0434]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0562]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0379]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0433] 50%|█████     | 4/8 [00:00<00:00, 38.25it/s, MSE=0.0433] 50%|█████     | 4/8 [00:00<00:00, 38.25it/s, MSE=0.0472] 50%|█████     | 4/8 [00:00<00:00, 38.25it/s, MSE=0.0425] 50%|█████     | 4/8 [00:00<00:00, 38.25it/s, MSE=0.0426] 50%|█████     | 4/8 [00:00<00:00, 38.25it/s, MSE=0.0296]100%|██████████| 8/8 [00:00<00:00, 39.00it/s, MSE=0.0296]100%|██████████| 8/8 [00:00<00:00, 38.82it/s, MSE=0.0296]
01:41:41 - INFO: Epoch 899 average loss: 0.042827
01:41:43 - INFO: Epoch 909 average loss: 0.037276
01:41:45 - INFO: Epoch 919 average loss: 0.043474
01:41:47 - INFO: Epoch 929 average loss: 0.046799
01:41:49 - INFO: Epoch 939 average loss: 0.044352
01:41:51 - INFO: Epoch 949 average loss: 0.046001
01:41:53 - INFO: Epoch 959 average loss: 0.039450
01:41:55 - INFO: Epoch 969 average loss: 0.043094
01:41:57 - INFO: Epoch 979 average loss: 0.041017
01:41:58 - INFO: Epoch 989 average loss: 0.040838
01:42:00 - INFO: Starting epoch 999:
  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0299]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0434]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0595]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.061]   0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0429]  0%|          | 0/8 [00:00<?, ?it/s, MSE=0.0451] 75%|███████▌  | 6/8 [00:00<00:00, 55.79it/s, MSE=0.0451] 75%|███████▌  | 6/8 [00:00<00:00, 55.79it/s, MSE=0.0527] 75%|███████▌  | 6/8 [00:00<00:00, 55.79it/s, MSE=0.0398]100%|██████████| 8/8 [00:00<00:00, 53.95it/s, MSE=0.0398]
01:42:00 - INFO: Epoch 999 average loss: 0.046787
01:42:01 - INFO: Total sampling 231 new convMatrix....
load from:/home/fushihao/diffusion/Code_FL/data_process/data_systhesis/models/DDPM_conditional/ckpt.pt 

Now sampling 0st new convMatrix.... Now sampling 1st new convMatrix.... Now sampling 2st new convMatrix.... Now sampling 3st new convMatrix.... Now sampling 4st new convMatrix.... Now sampling 5st new convMatrix.... Now sampling 6st new convMatrix.... Now sampling 7st new convMatrix.... Now sampling 8st new convMatrix.... Now sampling 9st new convMatrix.... Now sampling 10st new convMatrix.... Now sampling 11st new convMatrix.... Now sampling 12st new convMatrix.... Now sampling 13st new convMatrix.... Now sampling 14st new convMatrix.... Now sampling 15st new convMatrix.... Now sampling 16st new convMatrix.... Now sampling 17st new convMatrix.... Now sampling 18st new convMatrix.... Now sampling 19st new convMatrix.... Now sampling 20st new convMatrix.... Now sampling 21st new convMatrix.... Now sampling 22st new convMatrix.... Now sampling 23st new convMatrix.... Now sampling 24st new convMatrix.... Now sampling 25st new convMatrix.... Now sampling 26st new convMatrix.... Now sampling 27st new convMatrix.... Now sampling 28st new convMatrix.... Now sampling 29st new convMatrix.... Now sampling 30st new convMatrix.... Now sampling 31st new convMatrix.... Now sampling 32st new convMatrix.... Now sampling 33st new convMatrix.... Now sampling 34st new convMatrix.... Now sampling 35st new convMatrix.... Now sampling 36st new convMatrix.... Now sampling 37st new convMatrix.... Now sampling 38st new convMatrix.... Now sampling 39st new convMatrix.... Now sampling 40st new convMatrix.... Now sampling 41st new convMatrix.... Now sampling 42st new convMatrix.... Now sampling 43st new convMatrix.... Now sampling 44st new convMatrix.... Now sampling 45st new convMatrix.... Now sampling 46st new convMatrix.... Now sampling 47st new convMatrix.... Now sampling 48st new convMatrix.... Now sampling 49st new convMatrix.... Now sampling 50st new convMatrix.... Now sampling 51st new convMatrix.... Now sampling 52st new convMatrix.... Now sampling 53st new convMatrix.... Now sampling 54st new convMatrix.... Now sampling 55st new convMatrix.... Now sampling 56st new convMatrix.... Now sampling 57st new convMatrix.... Now sampling 58st new convMatrix.... Now sampling 59st new convMatrix.... Now sampling 60st new convMatrix.... Now sampling 61st new convMatrix.... Now sampling 62st new convMatrix.... Now sampling 63st new convMatrix.... Now sampling 64st new convMatrix.... Now sampling 65st new convMatrix.... Now sampling 66st new convMatrix.... Now sampling 67st new convMatrix.... Now sampling 68st new convMatrix.... Now sampling 69st new convMatrix.... Now sampling 70st new convMatrix.... Now sampling 71st new convMatrix.... Now sampling 72st new convMatrix.... Now sampling 73st new convMatrix.... Now sampling 74st new convMatrix.... Now sampling 75st new convMatrix.... Now sampling 76st new convMatrix.... Now sampling 77st new convMatrix.... Now sampling 78st new convMatrix.... Now sampling 79st new convMatrix.... Now sampling 80st new convMatrix.... Now sampling 81st new convMatrix.... Now sampling 82st new convMatrix.... Now sampling 83st new convMatrix.... Now sampling 84st new convMatrix.... Now sampling 85st new convMatrix.... Now sampling 86st new convMatrix.... Now sampling 87st new convMatrix.... Now sampling 88st new convMatrix.... Now sampling 89st new convMatrix.... Now sampling 90st new convMatrix.... Now sampling 91st new convMatrix.... Now sampling 92st new convMatrix.... Now sampling 93st new convMatrix.... Now sampling 94st new convMatrix.... Now sampling 95st new convMatrix.... Now sampling 96st new convMatrix.... Now sampling 97st new convMatrix.... Now sampling 98st new convMatrix.... Now sampling 99st new convMatrix.... Now sampling 100st new convMatrix.... Now sampling 101st new convMatrix.... Now sampling 102st new convMatrix.... Now sampling 103st new convMatrix.... Now sampling 104st new convMatrix.... Now sampling 105st new convMatrix.... Now sampling 106st new convMatrix.... Now sampling 107st new convMatrix.... Now sampling 108st new convMatrix.... Now sampling 109st new convMatrix.... Now sampling 110st new convMatrix.... Now sampling 111st new convMatrix.... Now sampling 112st new convMatrix.... Now sampling 113st new convMatrix.... Now sampling 114st new convMatrix.... Now sampling 115st new convMatrix.... Now sampling 116st new convMatrix.... Now sampling 117st new convMatrix.... Now sampling 118st new convMatrix.... Now sampling 119st new convMatrix.... Now sampling 120st new convMatrix.... Now sampling 121st new convMatrix.... Now sampling 122st new convMatrix.... Now sampling 123st new convMatrix.... Now sampling 124st new convMatrix.... Now sampling 125st new convMatrix.... Now sampling 126st new convMatrix.... Now sampling 127st new convMatrix.... Now sampling 128st new convMatrix.... Now sampling 129st new convMatrix.... Now sampling 130st new convMatrix.... Now sampling 131st new convMatrix.... Now sampling 132st new convMatrix.... Now sampling 133st new convMatrix.... Now sampling 134st new convMatrix.... Now sampling 135st new convMatrix.... Now sampling 136st new convMatrix.... Now sampling 137st new convMatrix.... Now sampling 138st new convMatrix.... Now sampling 139st new convMatrix.... Now sampling 140st new convMatrix.... Now sampling 141st new convMatrix.... Now sampling 142st new convMatrix.... Now sampling 143st new convMatrix.... Now sampling 144st new convMatrix.... Now sampling 145st new convMatrix.... Now sampling 146st new convMatrix.... Now sampling 147st new convMatrix.... Now sampling 148st new convMatrix.... Now sampling 149st new convMatrix.... Now sampling 150st new convMatrix.... Now sampling 151st new convMatrix.... Now sampling 152st new convMatrix.... Now sampling 153st new convMatrix.... Now sampling 154st new convMatrix.... Now sampling 155st new convMatrix.... Now sampling 156st new convMatrix.... Now sampling 157st new convMatrix.... Now sampling 158st new convMatrix.... Now sampling 159st new convMatrix.... Now sampling 160st new convMatrix.... Now sampling 161st new convMatrix.... Now sampling 162st new convMatrix.... Now sampling 163st new convMatrix.... Now sampling 164st new convMatrix.... Now sampling 165st new convMatrix.... Now sampling 166st new convMatrix.... Now sampling 167st new convMatrix.... Now sampling 168st new convMatrix.... Now sampling 169st new convMatrix.... Now sampling 170st new convMatrix.... Now sampling 171st new convMatrix.... Now sampling 172st new convMatrix.... Now sampling 173st new convMatrix.... Now sampling 174st new convMatrix.... Now sampling 175st new convMatrix.... Now sampling 176st new convMatrix.... Now sampling 177st new convMatrix.... Now sampling 178st new convMatrix.... Now sampling 179st new convMatrix.... Now sampling 180st new convMatrix.... Now sampling 181st new convMatrix.... Now sampling 182st new convMatrix.... Now sampling 183st new convMatrix.... Now sampling 184st new convMatrix.... Now sampling 185st new convMatrix.... Now sampling 186st new convMatrix.... Now sampling 187st new convMatrix.... Now sampling 188st new convMatrix.... Now sampling 189st new convMatrix.... Now sampling 190st new convMatrix.... Now sampling 191st new convMatrix.... Now sampling 192st new convMatrix.... Now sampling 193st new convMatrix.... Now sampling 194st new convMatrix.... Now sampling 195st new convMatrix.... Now sampling 196st new convMatrix.... Now sampling 197st new convMatrix.... Now sampling 198st new convMatrix.... Now sampling 199st new convMatrix.... Now sampling 200st new convMatrix.... Now sampling 201st new convMatrix.... Now sampling 202st new convMatrix.... Now sampling 203st new convMatrix.... Now sampling 204st new convMatrix.... Now sampling 205st new convMatrix.... Now sampling 206st new convMatrix.... Now sampling 207st new convMatrix.... Now sampling 208st new convMatrix.... Now sampling 209st new convMatrix.... /home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
Now sampling 210st new convMatrix.... Now sampling 211st new convMatrix.... Now sampling 212st new convMatrix.... Now sampling 213st new convMatrix.... Now sampling 214st new convMatrix.... Now sampling 215st new convMatrix.... Now sampling 216st new convMatrix.... Now sampling 217st new convMatrix.... Now sampling 218st new convMatrix.... Now sampling 219st new convMatrix.... Now sampling 220st new convMatrix.... Now sampling 221st new convMatrix.... Now sampling 222st new convMatrix.... Now sampling 223st new convMatrix.... Now sampling 224st new convMatrix.... Now sampling 225st new convMatrix.... Now sampling 226st new convMatrix.... Now sampling 227st new convMatrix.... Now sampling 228st new convMatrix.... Now sampling 229st new convMatrix.... Now sampling 230st new convMatrix.... torch.Size([231, 16])
feature shape after [Data Augmentation]:  (466, 16)
(466, 16)
====>MLP training... Epoch: 20 total loss: 1.9666
====>MLP training... Epoch: 40 total loss: 1.8849
====>MLP training... Epoch: 60 total loss: 1.6624
====>MLP training... Epoch: 80 total loss: 1.4527
====>MLP training... Epoch: 100 total loss: 1.4394
====>MLP training... Epoch: 120 total loss: 1.4090
====>MLP training... Epoch: 140 total loss: 1.2875
====>MLP training... Epoch: 160 total loss: 1.2525
====>MLP training... Epoch: 180 total loss: 1.2466
====>MLP training... Epoch: 200 total loss: 1.1806
====>CNN training... Epoch: 0  total loss: 0.2500
====>CNN training... Epoch: 20  total loss: 0.2497
====>CNN training... Epoch: 40  total loss: 0.2497
====>CNN training... Epoch: 60  total loss: 0.2494
====>CNN training... Epoch: 80  total loss: 0.2493
====>CNN training... Epoch: 100  total loss: 0.2489
====>CNN training... Epoch: 120  total loss: 0.2479
====>CNN training... Epoch: 140  total loss: 0.2465
====>CNN training... Epoch: 160  total loss: 0.2449
====>CNN training... Epoch: 180  total loss: 0.2417
====>CNN training... Epoch: 200  total loss: 0.2359
====>RNN training... Epoch: 20 total loss: 0.2485
====>RNN training... Epoch: 40 total loss: 0.2477
====>RNN training... Epoch: 60 total loss: 0.2471
====>RNN training... Epoch: 80 total loss: 0.2466
====>RNN training... Epoch: 100 total loss: 0.2462
====>RNN training... Epoch: 120 total loss: 0.2458
====>RNN training... Epoch: 140 total loss: 0.2455
====>RNN training... Epoch: 160 total loss: 0.2451
====>RNN training... Epoch: 180 total loss: 0.2447
====>RNN training... Epoch: 200 total loss: 0.2444

test: Lang-21-fs_ddpm
fault line:  [200265]
feature shape before [Data Augmentation]:  (109, 310)
[32, 34, 14, 15]
entropy 0.10434397973289405 0.1861750100009486
req_shape:  16
statements selected: 16
Traceback (most recent call last):
  File "/home/zhangxiaohong/yangjunzhe/temp_F/Code_FL/run.py", line 67, in <module>
    main()
  File "/home/zhangxiaohong/yangjunzhe/temp_F/Code_FL/run.py", line 62, in main
    pl.run()
  File "/home/zhangxiaohong/yangjunzhe/temp_F/Code_FL/pipeline/Pipeline.py", line 69, in run
    self._run_task()
  File "/home/zhangxiaohong/yangjunzhe/temp_F/Code_FL/pipeline/Pipeline.py", line 147, in _run_task
    self.data_obj.process(True)
  File "/home/zhangxiaohong/yangjunzhe/temp_F/Code_FL/data_process/dimensional_reduciton/DynamicSlice.py", line 97, in process
    print("statements selected:", self.feature_df.columns[inter_index])
                                  ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/home/zhangxiaohong/yangjunzhe/.conda/envs/pytorch_F/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 5416, in __getitem__
    result = getitem(key)
             ^^^^^^^^^^^^
IndexError: arrays used as indices must be of integer (or boolean) type
